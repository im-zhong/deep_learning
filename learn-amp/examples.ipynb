{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/24\n",
    "# zhangzhong\n",
    "# https://docs.pytorch.org/docs/stable/notes/amp_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/docs/stable/notes/amp_examples.html#typical-mixed-precision-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70155d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Unscaled Gradients\n",
    "# https://docs.pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients\n",
    "\n",
    "# Gradient Clipping\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n",
    "        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Multiple GPUs\n",
    "# https://docs.pytorch.org/docs/stable/notes/amp_examples.html#distributeddataparallel-one-gpu-per-process\n",
    "# torch.nn.parallel.DistributedDataParallelâ€™s documentation recommends one GPU per process for best performance. \n",
    "# In this case, DistributedDataParallel does not spawn threads internally, so usages of autocast and GradScaler are not affected.\n",
    "\n",
    "model = MyModel()\n",
    "dp_model = nn.DataParallel(model)\n",
    "\n",
    "# Sets autocast in the main thread\n",
    "with autocast(device_type='cuda', dtype=torch.float16):\n",
    "    # dp_model's internal threads will autocast.\n",
    "    output = dp_model(input)\n",
    "    # loss_fn also autocast\n",
    "    loss = loss_fn(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e134b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
