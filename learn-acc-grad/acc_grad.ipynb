{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbda9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/29\n",
    "# zhangzhong\n",
    "# https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c43d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how stochastic gradient descent works in 5 steps:\n",
    "\n",
    "# predictions = model(inputs)               # Forward pass\n",
    "# loss = loss_function(predictions, labels) # Compute loss function\n",
    "# loss.backward()                           # Backward pass\n",
    "# optimizer.step()                          # Optimizer step\n",
    "# predictions = model(inputs)               # Forward pass with new parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e2b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulating gradients:\n",
    "# before calling optimizer.step() to perform a step of gradient descent, \n",
    "# we will sum the gradients of several backward operations in the parameter.grad tensors\n",
    "# This is straightforward to do in PyTorch as the gradient tensors are not reset unless we call model.zero_grad() or optimizer.zero_grad()\n",
    "# We’ll also need to divide by the number of accumulation steps if our loss is averaged over the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889c0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.zero_grad()                                   # Reset gradients tensors\n",
    "# for i, (inputs, labels) in enumerate(training_set):\n",
    "#     predictions = model(inputs)                     # Forward pass\n",
    "#     loss = loss_function(predictions, labels)       # Compute loss function\n",
    "#     loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n",
    "#      PyTorch 会对计算图进行反向传播，把梯度累加到每个叶子张量（例如模型的权重）的 .grad 属性中。\n",
    "#     loss.backward()                                 # Backward pass\n",
    "#     if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "#         optimizer.step()                            # Now we can do an optimizer step\n",
    "#         model.zero_grad()                           # Reset gradients tensors\n",
    "#         if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n",
    "#             evaluate_model()                        # ...have no gradients accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0514129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you train a model for which not even a single sample can fit on a GPU?\n",
    "# gradient checkpoint! 这个有点超纲了，，暂时不看了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e069aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataParallel\n",
    "# 这个不看了，毕竟我用的是DDP\n",
    "# parallel_model = torch.nn.DataParallel(model) # Encapsulate the model\n",
    "\n",
    "# predictions = parallel_model(inputs)          # Forward pass on multi-GPUs\n",
    "# loss = loss_function(predictions, labels)     # Compute loss function\n",
    "# loss.mean().backward()                        # Average GPU-losses + backward pass\n",
    "# optimizer.step()                              # Optimizer step\n",
    "# predictions = parallel_model(inputs)          # Forward pass with new parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDP\n",
    "# But be careful: while the code looks similar, training your model in a distributed setting will change your workflow \n",
    "# because you will actually have to start an independent python training script on each node (these scripts are all identical).\n",
    "# As we will see, once started, these training scripts will be synchronized together by PyTorch distributed backend.\n",
    "#\n",
    "# In practice, this means that each training script will have:\n",
    "# - its own optimizer and performs a complete optimization step with each iteration, no parameter broadcast (step 2 in DataParallel) is needed,\n",
    "# - an independent Python interpreter: this will also avoid the GIL-freeze that can come from driving several parallel execution threads in a single Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff79199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated by ChatGPT\n",
    "# refs:\n",
    "# 1. https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync\n",
    "# 2. https://discuss.pytorch.org/t/whats-no-sync-exactly-do-in-ddp/170259\n",
    "# context manager no_sync() 必须也要包括forward过程，因为DPP会在forward过程放置gradient hook，我记得文档中提到过，我要找一下\n",
    "# https://docs.pytorch.org/docs/main/notes/ddp.html\n",
    "# 在这里提到了autograd hook是在construction阶段放的，并不是在forward阶段放的\n",
    "# 不过既然no_sync的文档专门提到了这一点，还是根据文档的说明来吧\n",
    "# The forward pass should be included inside the context manager, or else gradients will still be synchronized.\n",
    "# 到时候可以测试一下？看看训练速度是不是有提升\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import contextlib\n",
    "\n",
    "# 简单的 MLP 模型用于 MNIST 分类\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 单个进程的训练逻辑（每个 GPU 运行一个）\n",
    "def train(rank, world_size, accumulation_steps=4):\n",
    "    # 设置当前进程的设备\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    # 初始化进程组\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # 创建模型并放到对应 GPU 上\n",
    "    model = SimpleModel().to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    # 准备数据（使用 DistributedSampler 确保每个进程加载不同数据）\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
    "\n",
    "    # 设置优化器、损失函数、AMP 缩放器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(dataloader):\n",
    "        inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "\n",
    "        # 是否是累积的最后一步（是否执行梯度同步与 step）\n",
    "        # 因为step从0开始，所以累积步数是 step + 1\n",
    "        is_final_accum = (step + 1) % accumulation_steps == 0\n",
    "\n",
    "        # 如果不是最后一步，禁用 DDP 的梯度同步\n",
    "        sync_context = model.no_sync() if not is_final_accum else contextlib.nullcontext()\n",
    "        with sync_context:\n",
    "            with autocast():  # 自动混合精度的上下文\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss = loss / accumulation_steps  # 均分 loss 实现梯度缩放\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "        # 还有一个事情，如果要记录checkpoint，那么只有accumulate steps的倍数才有意义！\n",
    "        # 包括记录日志，记录loss等，\n",
    "        # 这样设计就会修改训练的逻辑，不如step保持现在的含义不变\n",
    "        # 我们将这个acc的过程包装起来，是的对外表现的就像是一个step一样，这样是最合理的！\n",
    "\n",
    "        # 累积满了才执行优化器更新\n",
    "        if is_final_accum:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 仅 rank 0 打印日志\n",
    "            if rank == 0 and step % 100 == 0:\n",
    "                print(f\"[Rank {rank}] Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # 训练结束，销毁进程组\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# 使用 spawn 启动多进程（每个 GPU 一个进程）\n",
    "def main():\n",
    "    world_size = torch.cuda.device_count()\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
