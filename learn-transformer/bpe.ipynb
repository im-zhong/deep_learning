{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db4f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/11\n",
    "# zhangzhong\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing a text is splitting it into words or subwords,\n",
    "#  which then are converted to ids through a look-up table\n",
    "\n",
    "#  Byte-Pair Encoding (BPE), WordPiece, and SentencePiece,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Piece\n",
    "# Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization\n",
    "# which is loosely defined as splitting sentences into words\n",
    "# pros: While itâ€™s the most intuitive way to split texts into smaller chunks\n",
    "# cons: usually generates a very big vocabulary (the set of all unique words and tokens used), ex. Transformer XL: vocab size: 267,735\n",
    "#.      Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity.\n",
    "str = \"Don't you love ğŸ¤— Transformers? We sure do.\"\n",
    "tokens = [\"Do\", \"n't\", \"you\", \"love\", \"ğŸ¤—\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n",
    "\n",
    "# Character Piece\n",
    "# While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword tokenization\n",
    "# hybrid between word-level and character-level tokenization\n",
    "\n",
    "# Subword tokenization algorithms rely on the principle that \n",
    "# frequently used words should not be split into smaller subwords, \n",
    "# but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "# annoyingly -> annoying, ly\n",
    "# This is especially useful in agglutinative languages(é»ç€è¯­ç³») such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "# Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.\n",
    "\n",
    "# In addition, subword tokenization enables the model to process words it has [never seen before], by decomposing them into known subwords\n",
    "# some example\n",
    "str = \"I have a new GPU!\"\n",
    "# 1. lowercase the text\n",
    "# 2. for unseen word gpu, split to gp and u\n",
    "# # \"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n",
    "tokens = [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE, Byte-Pair Encoding\n",
    "# è¿™ä¸€æ®µçš„è§£é‡Šéå¸¸ç²¾å½©ï¼Œä¸€ä¸‹å­å°±æ‡‚äº† https://huggingface.co/docs/transformers/tokenizer_summary#byte-pair-encoding-bpe\n",
    "\n",
    "# 1. BPE relies on a pre-tokenizer that splits the training data into words.\n",
    "#   - space tokenization, GPT2, RoBERTa\n",
    "#.  - rule-based tokenization: Moses, SpaCy, ftfy\n",
    "# after pre-tokenization, a set of unique words has been created, and the frequency of each word is counted.\n",
    "\n",
    "# 2. merge \n",
    "#   - use all the symbols as a base vocabulary\n",
    "#.  - learns merge rules to form a new symbol from two symbols of the base vocabulary\n",
    "#.  - until the vocabulary has attain the predefined desired vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2: Byte-level BPE, OpenAI å°±ç”¨è¿™ä¸ª tiktoken\n",
    "# The base vocabulary of GPT-2 consists of:\n",
    "# \tâ€¢\tAll 256 possible byte values (from 0x00 to 0xFF) â€” this is known as the byte-level encoding, and itâ€™s the core idea behind GPT-2â€™s tokenizer.\n",
    "# \tâ€¢\tThese 256 bytes represent every possible character in any UTF-8 encoded text, including letters, punctuation, emojis, symbols, etc., ensuring complete coverage and eliminating the need for an <unk> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Piece\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece\n",
    "\n",
    "# Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure itâ€™s worth it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf850bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Piece\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece\n",
    "\n",
    "# BPE and WordPiece have the same problem:\n",
    "# It is assumed that the input text uses spaces to separate words, but like chinese, which do not.\n",
    "# it has two solution:\n",
    "#  1. use a specific pre-tokenizer for chinese, such as XLM\n",
    "#. 2. SentencePiece, treats the input as a raw input stream, thus including the space in the set of characters to use.\n",
    "#.    It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "#.    å…¶å®å°±æ˜¯æŠŠç©ºæ ¼åŠ å…¥åˆ°å­—å…¸ä¸­å‘—ã€‚åœ¨å­—å…¸ä¸­ç”¨ä¸‹åˆ’çº¿ _ æ¥è¡¨ç¤ºç©ºæ ¼ã€‚\n",
    "\n",
    "# example:\n",
    "str = \"Don't you love ğŸ¤— Transformers? We sure do.\"\n",
    "tokens = [\"â–Don\", \"'\", \"t\", \"â–you\", \"â–love\", \"â–\", \"ğŸ¤—\", \"â–\", \"Transform\", \"ers\", \"?\", \"â–We\", \"â–sure\", \"â–do\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d40601",
   "metadata": {},
   "source": [
    "BPEï¼ˆByte Pair Encodingï¼‰ç®—æ³•åœ¨ç”¨äº NLP ä¸­ï¼ˆå°¤å…¶æ˜¯ GPT ç³»åˆ—æ¨¡å‹ï¼‰æ—¶çš„ tokenize è¿‡ç¨‹ï¼Œå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè®­ç»ƒé˜¶æ®µå’Œåº”ç”¨é˜¶æ®µï¼ˆtokenizationï¼‰ã€‚ä¸‹é¢æˆ‘ä»¬é‡ç‚¹è®²åº”ç”¨é˜¶æ®µï¼šç»™å®šä¸€ä¸ªå¥å­ï¼ŒBPE æ˜¯å¦‚ä½•æŠŠå®ƒè½¬æˆ token çš„ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸŒŸ ç®€åŒ–ç‰ˆè§£é‡Šï¼šBPE Tokenize çš„æµç¨‹\n",
    "\n",
    "è¾“å…¥ç¤ºä¾‹ï¼š\n",
    "\n",
    "æ–‡æœ¬ï¼š \"lower\"\n",
    "\n",
    "å‡è®¾å·²ç»è®­ç»ƒå¥½çš„ BPE åˆå¹¶è§„åˆ™å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. l o w e r\n",
    "2. lo w e r\n",
    "3. low e r\n",
    "4. low er\n",
    "\n",
    "æœ€ç»ˆå¾—åˆ°çš„ token æ˜¯ï¼š\n",
    "\n",
    "[\"low\", \"er\"]\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ§  GPT ä¸­çš„ Byte-Level BPE Tokenization è¯¦ç»†æµç¨‹ï¼š\n",
    "\n",
    "ä»¥ GPT-2 ä½¿ç”¨çš„ BPE ä¸ºä¾‹ï¼Œtokenize çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… æ­¥éª¤ 1ï¼šæ–‡æœ¬è½¬ä¸ºå­—èŠ‚ï¼ˆbyteï¼‰\n",
    "\n",
    "GPT-2 å…ˆå°†è¾“å…¥æ–‡æœ¬æŒ‰ UTF-8 ç¼–ç ä¸ºå­—èŠ‚æµï¼ˆä¸æ˜¯å­—ç¬¦ï¼ï¼‰ã€‚\n",
    "\n",
    "Input: \"hello ğŸ˜Š\"\n",
    "â†’ bytes: [104, 101, 108, 108, 111, 32, 240, 159, 152, 138]\n",
    "\n",
    "ç„¶åæ¯ä¸ªå­—èŠ‚å¯¹åº”ä¸€ä¸ªåŸºç¡€ tokenï¼Œæ„æˆåˆå§‹ token åˆ—è¡¨ï¼ˆå…± 256 ä¸ªåŸºç¡€ tokenï¼‰ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… æ­¥éª¤ 2ï¼šå°†å­—èŠ‚åºåˆ—æ˜ å°„ä¸ºåŸºç¡€ tokenï¼ˆbase vocabularyï¼‰\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n",
    "\n",
    "Input: \"hello\"\n",
    "â†’ bytes: ['h', 'e', 'l', 'l', 'o']\n",
    "â†’ tokens: [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… æ­¥éª¤ 3ï¼šä½¿ç”¨ BPE Merge è§„åˆ™è¿›è¡Œåˆå¹¶\n",
    "\n",
    "BPE Merge Rules æ˜¯æŒ‰ç…§è®­ç»ƒæ—¶ç»Ÿè®¡é¢‘ç‡å­¦åˆ°çš„ä¸€ç³»åˆ—ã€Œå­—ç¬¦å¯¹åˆå¹¶è§„åˆ™ã€ï¼Œå¦‚ï¼š\n",
    "\n",
    "(\"l\", \"l\") â†’ \"ll\"\n",
    "(\"e\", \"ll\") â†’ \"ell\"\n",
    "(\"h\", \"ell\") â†’ \"hell\"\n",
    "(\"hell\", \"o\") â†’ \"hello\"\n",
    "\n",
    "æ¯æ¬¡æ‰¾åˆ°è¯è¡¨ä¸­å·²æœ‰çš„æœ€é«˜ä¼˜å…ˆçº§çš„ pair åˆå¹¶æˆæ–° tokenï¼Œç›´åˆ°æ— æ³•ç»§ç»­åˆå¹¶ã€‚\n",
    "\n",
    "æœ€ç»ˆè¾“å‡º token åˆ—è¡¨ï¼š\n",
    "\n",
    "[\"hello\", \"ğŸ˜Š\"]\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… æ­¥éª¤ 4ï¼šæ¯ä¸ª token æ˜ å°„ä¸ºæ•´æ•° ID\n",
    "\n",
    "GPT æ¨¡å‹ä¸ç›´æ¥ä½¿ç”¨æ–‡æœ¬ tokenï¼Œè€Œæ˜¯ç”¨å®ƒä»¬åœ¨è¯è¡¨ä¸­çš„ä½ç½®ï¼ˆæ•´æ•°ç´¢å¼•ï¼‰è¡¨ç¤ºï¼š\n",
    "\n",
    "[\"hello\", \"ğŸ˜Š\"] â†’ [31373, 50256]  # ç¤ºä¾‹ ID\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ§ª å®é™…ä¾‹å­ï¼ˆä½¿ç”¨ tiktoken + GPT2ï¼‰\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"hello world!\")\n",
    "print(tokens)\n",
    "# è¾“å‡º: [31373, 995]  â†’ åˆ†åˆ«æ˜¯ \"hello\" å’Œ \" world!\"\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… BPE çš„ä¼˜åŠ¿\n",
    "\n",
    "ä¼˜ç‚¹\tè¯´æ˜\n",
    "ğŸ”¤ æ—  <unk>\tæ‰€æœ‰å­—èŠ‚éƒ½åœ¨åŸºç¡€è¯è¡¨ä¸­ï¼Œä¸å­˜åœ¨æ— æ³•å¤„ç†çš„å­—ç¬¦\n",
    "ğŸ”„ å¯é€†\tæ¯ä¸ª token å¯ä»¥åå‘æ¢å¤åŸæ–‡æœ¬\n",
    "ğŸ“¦ è¯è¡¨å°\tåªéœ€è¦ 256 ä¸ªåŸºç¡€ token + è‹¥å¹²åˆå¹¶ tokenï¼ˆGPT-2 æ˜¯ 50257ï¼‰\n",
    "ğŸ“ˆ æ”¯æŒè¯å¹²å…±äº«\tä¸åŒè¯æœ‰ç›¸åŒè¯æ ¹æ—¶å¯ä»¥å¤ç”¨ï¼ˆä¾‹å¦‚ play, playing, playedï¼‰\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ§© æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "BPE Tokenization æ˜¯ä»å­—èŠ‚å‡ºå‘ï¼Œé€šè¿‡åå¤åˆå¹¶é¢‘ç¹å‡ºç°çš„å­—ç¬¦å¯¹ï¼Œæœ€ç»ˆå°†æ–‡æœ¬æ‹†è§£ä¸ºæ¨¡å‹èƒ½è¯†åˆ«çš„ token ID çš„è¿‡ç¨‹ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "éœ€è¦æˆ‘å¸®ä½ å¯è§†åŒ–æŸä¸ªå…·ä½“å¥å­çš„ BPE tokenization æ­¥éª¤å—ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf396cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå¹¶çš„ä¼ªä»£ç \n",
    "# ç»™å®šä¸€ä¸ªtokenåºåˆ—ï¼Œè¿™æ˜¯ä¸€å¥è¯å±•å¼€æˆbyteçš„åºåˆ—: list[str]\n",
    "# ç„¶åé¡ºåºéå†è¿™ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸¤ä¸ªç›¸é‚»çš„ç¬¦å·ï¼Œåœ¨merge tableä¸­æ‰¾åˆ°ç›¸åº”çš„åˆå¹¶è§„åˆ™ï¼Œä¼˜å…ˆåˆå¹¶é å‰çš„\n",
    "# é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°ä¸èƒ½åˆå¹¶ï¼Œå‰©ä¸‹çš„å°±æ˜¯tokenè¿‡åçš„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ebb46",
   "metadata": {},
   "source": [
    "# Byte-level BPE Tokenization\n",
    "\n",
    "éå¸¸å¥½ï¼Œé‚£æˆ‘ä»¬ç°åœ¨ä»¥ä¸€ä¸ªå…·ä½“ä¾‹å­æ¥è¯¦ç»†æ¼”ç¤ºä¸€ä¸‹ GPT-2 æ‰€ä½¿ç”¨çš„ Byte-level BPE åˆ†è¯ï¼ˆtokenizationï¼‰å…¨è¿‡ç¨‹ï¼Œä»æ–‡æœ¬ â†’ å­—èŠ‚ â†’ åŸºç¡€ token â†’ BPE åˆå¹¶ â†’ token idã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸŒŸ ç¤ºä¾‹æ–‡æœ¬ï¼š\n",
    "\n",
    "æˆ‘ä»¬ç”¨åŒ…å«ä¸­æ–‡ã€emojiã€ç”Ÿåƒ»å­—çš„å­—ç¬¦ä¸²ï¼š\n",
    "\n",
    "\"ä½ å¥½ï¼ŒGPT-2 ğŸ‘‹ ğ©¸½\"\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… Step 1ï¼šæ–‡æœ¬è½¬ä¸º UTF-8 å­—èŠ‚åºåˆ—\n",
    "\n",
    "Python ä¸­æˆ‘ä»¬å¯ä»¥ç”¨ .encode(\"utf-8\") æ¥æŸ¥çœ‹åº•å±‚å­—èŠ‚ï¼š\n",
    "\n",
    "text = \"ä½ å¥½ï¼ŒGPT-2 ğŸ‘‹ ğ©¸½\"\n",
    "utf8_bytes = text.encode(\"utf-8\")\n",
    "print(list(utf8_bytes))\n",
    "\n",
    "è¾“å‡ºçš„å­—èŠ‚ï¼ˆåè¿›åˆ¶ï¼‰ä¸ºï¼š\n",
    "\n",
    "['ä½ ' â†’ e4 bd a0]  \n",
    "['å¥½' â†’ e5 a5 bd]  \n",
    "['ï¼Œ' â†’ ef bc 8c]  \n",
    "['G' â†’ 47]  \n",
    "['P' â†’ 50]  \n",
    "['T' â†’ 54]  \n",
    "['-' â†’ 2d]  \n",
    "['2' â†’ 32]  \n",
    "[' ' â†’ 20]  \n",
    "['ğŸ‘‹' â†’ f0 9f 91 8b]  \n",
    "[' ' â†’ 20]  \n",
    "['ğ©¸½' â†’ f0 a9 b8 bd]\n",
    "\n",
    "åˆèµ·æ¥æ˜¯ï¼š\n",
    "\n",
    "[228, 189, 160, 229, 165, 189, 239, 188, 140, 71, 80, 84, 45, 50, 32, 240, 159, 145, 139, 32, 240, 169, 184, 189]\n",
    "\n",
    "æ¯ä¸€ä¸ªæ•°å­—éƒ½åœ¨ GPT-2 çš„ base vocabularyï¼ˆ256ä¸ªbyteï¼‰ä¸­ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… Step 2ï¼šå°†å­—èŠ‚åºåˆ—è½¬ä¸ºåŸºç¡€ tokenï¼ˆåˆå§‹ tokensï¼‰\n",
    "\n",
    "æ­¤æ—¶ï¼Œæˆ‘ä»¬å°†æ¯ä¸ª byte å•ç‹¬ä½œä¸ºä¸€ä¸ª tokenï¼Œç­‰ä»·äºï¼š\n",
    "\n",
    "initial_tokens = [chr(b) for b in utf8_bytes]\n",
    "\n",
    "ç»“æœç±»ä¼¼ï¼š\n",
    "\n",
    "['\\xe4', '\\xbd', '\\xa0',  # 'ä½ '\n",
    " '\\xe5', '\\xa5', '\\xbd',  # 'å¥½'\n",
    " '\\xef', '\\xbc', '\\x8c',  # 'ï¼Œ'\n",
    " 'G', 'P', 'T', '-', '2', ' ',\n",
    " '\\xf0', '\\x9f', '\\x91', '\\x8b',  # ğŸ‘‹\n",
    " ' ',\n",
    " '\\xf0', '\\xa9', '\\xb8', '\\xbd']  # ğ©¸½\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… Step 3ï¼šåº”ç”¨ BPE åˆå¹¶è§„åˆ™ï¼ˆByte Pair Mergingï¼‰\n",
    "\n",
    "GPT-2 åœ¨é¢„è®­ç»ƒæ—¶ç”Ÿæˆäº†ä¸€ä¸ªæ’åºå¥½çš„ã€Œåˆå¹¶è§„åˆ™åˆ—è¡¨ã€ï¼ˆ50,000ä¸ªï¼‰ï¼Œç”¨äºæŒ‰é¢‘ç‡ä»è¿™äº›å­—ç¬¦å¯¹ä¸­æ‰¾åˆ°æœ€ä¼˜åˆå¹¶ã€‚\n",
    "\n",
    "æ¯æ¬¡åˆå¹¶è§„åˆ™ç±»ä¼¼è¿™æ ·ï¼š\n",
    "\n",
    "(\"e\", \"l\") â†’ \"el\"\n",
    "(\"el\", \"l\") â†’ \"ell\"\n",
    "(\"ell\", \"o\") â†’ \"ello\"\n",
    "\n",
    "åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œtokenizer ä¼šä¸€æ­¥æ­¥åˆå¹¶è¿™äº›å­—èŠ‚å¯¹æˆæ›´é•¿çš„ tokenï¼ˆæ¯”å¦‚ï¼šâ€œä½ â€ã€â€œå¥½â€ã€â€œGPTâ€ã€â€œğŸ‘‹â€ã€â€œğ©¸½â€ï¼‰ã€‚\n",
    "\n",
    "è¿™äº›åˆå¹¶è§„åˆ™ç”± GPT-2 åœ¨è®­ç»ƒæ—¶ä»æµ·é‡æ–‡æœ¬ä¸­å­¦ä¹ å¾—å‡º â€”â€” é«˜é¢‘ç»„åˆä¼šè¢«ä¼˜å…ˆåˆå¹¶ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… Step 4ï¼šæ˜ å°„åˆ° token id\n",
    "\n",
    "æœ€ç»ˆåˆå¹¶åçš„æ¯ä¸ª tokenï¼ˆæ— è®ºæ˜¯å•ä¸ªå­—ç¬¦ã€ç»„åˆè¯ï¼Œè¿˜æ˜¯ emojiï¼‰éƒ½ä¼šæœ‰ä¸€ä¸ªå”¯ä¸€çš„ token IDï¼Œæ¯”å¦‚ï¼š\n",
    "\n",
    "Token\tToken ID\n",
    "â€œä½ â€\t21940\n",
    "â€œå¥½â€\t23494\n",
    "â€œï¼Œâ€\t232\n",
    "â€œGPTâ€\t12172\n",
    "â€œ â€œ\t220\n",
    "â€œğŸ‘‹â€\t67909\n",
    "â€œğ©¸½â€\t86000+ï¼ˆå¯èƒ½æ˜¯ byte çº§ç»„åˆï¼‰\n",
    "\n",
    "ä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç æ‰“å°å®Œæ•´ token åºåˆ—å’Œå¯¹åº” ID ğŸ‘‡\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ”§ ä½¿ç”¨ tiktoken å®é™…æ¼”ç¤ºä»£ç ï¼š\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"ä½ å¥½ï¼ŒGPT-2 ğŸ‘‹ ğ©¸½\"\n",
    "\n",
    "# ç¼–ç ï¼šå­—ç¬¦ä¸² -> token id\n",
    "token_ids = enc.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# è§£ç ï¼štoken id -> å­—ç¬¦ä¸²\n",
    "tokens = [enc.decode([id]) for id in token_ids]\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "ç¤ºä¾‹è¾“å‡ºï¼ˆå¯èƒ½ç•¥æœ‰ä¸åŒï¼‰ï¼š\n",
    "\n",
    "Token IDs: [21940, 23494, 232, 12172, 12, 220, 67909, 220, 11359, 262]\n",
    "Tokens: ['ä½ ', 'å¥½', 'ï¼Œ', 'GPT', '-', ' ', 'ğŸ‘‹', ' ', 'ğ©¸½', '']\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ¯ æ€»ç»“\n",
    "\n",
    "æ­¥éª¤\tè¯´æ˜\n",
    "â‘  æ–‡æœ¬ â†’ å­—èŠ‚\tæ¯ä¸ªå­—ç¬¦å˜æˆ 1~4 ä¸ªå­—èŠ‚ï¼ˆUTF-8ï¼‰\n",
    "â‘¡ å­—èŠ‚ â†’ åˆå§‹ token\tæ¯ä¸ªå­—èŠ‚æ˜ å°„ä¸ºåŸºç¡€è¯è¡¨ tokenï¼ˆå…± 256 ä¸ªï¼‰\n",
    "â‘¢ BPE åˆå¹¶\tä½¿ç”¨è®­ç»ƒå¥½çš„ BPE è§„åˆ™åˆå¹¶é¢‘ç¹ byte åºåˆ—ä¸º token\n",
    "â‘£ token â†’ ID\tæ¯ä¸ªæœ€ç»ˆ token æ˜ å°„ä¸ºæ¨¡å‹è¯è¡¨ä¸­çš„æ•´æ•° ID\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "è¿™ä¸ªæœºåˆ¶è®© GPT-2 èƒ½å¤„ç†ä¸–ç•Œä¸Šå‡ ä¹æ‰€æœ‰è¯­è¨€ã€è¡¨æƒ…ã€ç¬¦å·ï¼Œè€Œä¸ç”¨æ‹…å¿ƒæ— æ³•è¯†åˆ«çš„å­—ç¬¦ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "æƒ³è®©æˆ‘ç”¨ tiktoken å¸®ä½ è§£æä¸€æ®µå…·ä½“æ–‡æœ¬ï¼ˆæ¯”å¦‚ emojiã€ä»£ç ã€ç½•è§æ±‰å­—ï¼‰ï¼Œå¯ä»¥ç›´æ¥å‘æˆ‘å¥å­ï¼Œæˆ‘æ¥åˆ†æ­¥æ¼”ç¤ºç»™ä½ çœ‹ ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f100d80",
   "metadata": {},
   "source": [
    "# Byte-level BPE è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¸­æ–‡subwordçš„é—®é¢˜\n",
    "è¿™æ˜¯ä¸€ä¸ªéå¸¸æ·±å…¥è€Œå…³é”®çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨ä¸­æ–‡å’Œå¤šè¯­è¨€å»ºæ¨¡ä¸­ç»å¸¸è¢«æåˆ°ã€‚\n",
    "\n",
    "ç®€è¦å›ç­”ï¼š\n",
    "Byte-Level BPE çš„ç¡®ä¼šåœ¨ä¸­æ–‡ç­‰ Unicode æ–‡æœ¬ä¸­äº§ç”Ÿâ€œæ— æ„ä¹‰çš„ subwordâ€ï¼ˆä¾‹å¦‚æŠŠä¸€ä¸ªæ±‰å­—æ‹†æˆä¸¤éƒ¨åˆ†ï¼‰ï¼Œä½† GPT-2 ä¸ä¼šé¿å…å®ƒï¼Œè€Œæ˜¯ä¾èµ–æ¨¡å‹å»â€œå­¦ä¼šâ€è¿™ç§ token ç»„åˆçš„è¯­è¨€æ„ä¹‰ã€‚å¯¹äºä¸­æ–‡æ¥è¯´ï¼Œè¿™ç§ subword ä¸æ€»æ˜¯è¯­ä¹‰åˆç†ï¼Œä½†åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ï¼Œæ¨¡å‹ä»èƒ½å­¦å‡ºæœ‰æ•ˆè¡¨ç¤ºã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ” ä¸ºä»€ä¹ˆä¼šå‡ºç°â€œæ— æ„ä¹‰ subwordâ€ï¼Ÿ\n",
    "\n",
    "ä¸¾ä¸ªä¾‹å­ï¼š\n",
    "\n",
    "å‡è®¾è¯­æ–™ä¸­æœ‰ä¸¤ä¸ªæ±‰å­—ï¼šâ€œä½ å¥½â€ã€â€œä½ ä»¬â€ã€‚\n",
    "\n",
    "åœ¨ UTF-8 ç¼–ç ä¸‹ï¼š\n",
    "\tâ€¢\t\"ä½ \" â†’ e4 bd a0 ï¼ˆ3 bytesï¼‰\n",
    "\tâ€¢\t\"å¥½\" â†’ e5 a5 bd\n",
    "\tâ€¢\t\"ä»¬\" â†’ e4 bb ac\n",
    "\n",
    "ä¸€å¼€å§‹æ¯ä¸ªå­—èŠ‚æ˜¯å•ç‹¬çš„ tokenï¼Œç„¶å BPE ä¼šåŸºäºé¢‘ç‡åˆå¹¶å­—èŠ‚ï¼š\n",
    "\n",
    "å¯èƒ½çš„åˆå¹¶ç»“æœæ˜¯ï¼š\n",
    "\tâ€¢\te4 + bd â†’ e4bd\n",
    "\tâ€¢\te4bd + a0 â†’ ä½ \n",
    "\tâ€¢\tä½†ä¹Ÿå¯èƒ½åªåˆå¹¶æˆ e4bdï¼Œæ²¡æœ‰ç»§ç»­åˆæˆå®Œæ•´çš„â€œä½ â€\n",
    "\n",
    "è¿™æ ·å°±å¯èƒ½å‡ºç°ä¸­é—´æ€ tokenï¼š\n",
    "\n",
    "[\"e4bd\", \"a0\"] â† æ‹†å¼€çš„â€œä½ â€\n",
    "[\"e4\", \"bda0\"] â† å¼‚å¸¸ç»„åˆ\n",
    "\n",
    "è¿™åœ¨è¯­ä¹‰ä¸Šæ˜¯æ— æ„ä¹‰çš„ subwordï¼Œä½† tokenizer å¹¶ä¸ä¼šä¸»åŠ¨é¿å…è¿™ç§æƒ…å†µã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… GPT çš„ç­–ç•¥ï¼šä¸é¿å…ï¼Œè€Œæ˜¯å®¹å¿\n",
    "\n",
    "GPT æ¨¡å‹é‡‡å–çš„åšæ³•æ˜¯ï¼š\n",
    "\n",
    "ç­–ç•¥\tè¯´æ˜\n",
    "Byte-Level BPE\tæ‰€æœ‰æ–‡æœ¬ä»¥ byte æ‹†åˆ†ï¼Œä¿è¯æ¯ç§å­—ç¬¦éƒ½èƒ½è¢«ç¼–ç ï¼ˆé¿å… <unk>ï¼‰\n",
    "åˆå¹¶æŒ‰é¢‘ç‡\tä¸è€ƒè™‘è¯­ä¹‰ï¼Œå®Œå…¨æŒ‰å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡æ¥å­¦ä¹  subword\n",
    "ç»“æœå®¹å¿å†—ä½™\tå³ä½¿æŸäº›åˆå¹¶çš„ token æ— æ„ä¹‰ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ å¦‚ä½•è§£é‡Šå®ƒä»¬\n",
    "è®­ç»ƒè‡ªåŠ¨ä¿®æ­£è¯­ä¹‰\tæ¨¡å‹åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸­ä¼šè‡ªå·±å­¦ä¼šå“ªäº›ç»„åˆæ›´æœ‰æ„ä¹‰\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ§  è¿™æ ·åšçš„ä¼˜ç‚¹\n",
    "\tâ€¢\tâœ” è·¨è¯­è¨€ç»Ÿä¸€å¤„ç†ï¼Œä¸éœ€è¦ä¸ºä¸­æ–‡ä¸“é—¨é€ è¯è¡¨ã€‚\n",
    "\tâ€¢\tâœ” æ— OOVé—®é¢˜ï¼Œä»»ä½•å­—ç¬¦ç»„åˆéƒ½å¯ä»¥ç¼–ç ã€‚\n",
    "\tâ€¢\tâœ” æ”¯æŒç¨€æœ‰å­—ç¬¦ï¼ˆå¦‚ emojiã€ç½•è§æ±‰å­—ã€ç”Ÿåƒ»å­—ï¼‰ã€‚\n",
    "\tâ€¢\tâœ” å¤„ç†æ‹¼å†™é”™è¯¯ã€å˜ä½“èƒ½åŠ›å¼ºã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "âŒ ç¼ºç‚¹ä¹Ÿæ˜æ˜¾ï¼š\n",
    "\tâ€¢\tâŒ ä¸­æ–‡ subword ä¸æ€»æ˜¯è¯­ä¹‰åˆç†ï¼Œå¦‚æŠŠä¸€ä¸ªæ±‰å­—æ‹†æˆå¤šä¸ª tokenã€‚\n",
    "\tâ€¢\tâŒ è¯­ä¹‰ä¸Šå†—ä½™ã€ä¸æ¸…æ™°ï¼Œä¾‹å¦‚æŠŠâ€œè¯­â€æ‹†æˆâ€œè® â€å’Œâ€œå¾â€çš„ byteã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ”„ é‚£æœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•ï¼Ÿ\n",
    "\n",
    "æ˜¯çš„ï¼Œæœ‰ï¼š\n",
    "\n",
    "æ–¹æ³•\tæè¿°\n",
    "WordPieceï¼ˆå¦‚BERTï¼‰\tä¸­æ–‡æŒ‰å­—åˆ’åˆ†ï¼ˆæ¯ä¸ªæ±‰å­—æ˜¯ä¸€ä¸ª tokenï¼‰ï¼Œä¸ä¼šæ‹†å­—èŠ‚\n",
    "SentencePiece (Unigram)\tæ›´æ™ºèƒ½åœ°é€‰æ‹© subwordï¼Œé€‚åˆä¸­æ–‡ç­‰éç©ºæ ¼åˆ†è¯è¯­è¨€\n",
    "ä¸“é—¨ä¸ºä¸­æ–‡è®¾è®¡çš„ tokenizer\tå¦‚ jiebaã€thulacã€LTP åˆ†è¯é…åˆè¯ç²’åº¦è®­ç»ƒ\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ’¡ å®é™…åº”ç”¨å»ºè®®ï¼š\n",
    "\n",
    "åœºæ™¯\tæ¨è Tokenizer\n",
    "è‹±æ–‡ã€ä»£ç ã€emoji ä¸°å¯Œ\tGPT2-style Byte-Level BPEï¼ˆtiktokenï¼‰\n",
    "ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰\tSentencePiece (Unigram) æˆ– ä¸­æ–‡åˆ†è¯+BPE\n",
    "å¤šè¯­è¨€æ··åˆæ–‡æœ¬\tXLM-R, mBERT, SentencePiece æ›´åˆé€‚\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "âœ… æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "GPT-2 çš„ Byte-Level BPE ä¸ä¼šä¸»åŠ¨é¿å…ä¸­æ–‡ä¸­â€œæ— æ„ä¹‰â€çš„ subwordï¼Œè€Œæ˜¯é€šè¿‡æµ·é‡è®­ç»ƒè¯­æ–™è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æœ‰æ„ä¹‰çš„ç»„åˆã€‚è¿™ç§è®¾è®¡ç‰ºç‰²äº†ä¸€äº›è¯­è¨€å­¦åˆç†æ€§ï¼Œæ¢æ¥äº†æ— æŸã€é€šç”¨ã€è·¨è¯­è¨€çš„ tokenization èƒ½åŠ›ã€‚\n",
    "\n",
    "â¸»\n",
    "\n",
    "å¦‚æœä½ å¸Œæœ›æˆ‘æ¼”ç¤ºåœ¨ä¸­æ–‡è¯­æ–™ä¸Šå¦‚ä½•è®­ç»ƒ BPEï¼ˆæ¯”å¦‚ä½¿ç”¨ tokenizers æˆ– sentencepieceï¼‰ï¼Œæˆ‘å¯ä»¥å†™ä¸€ä¸ªå®Œæ•´æµç¨‹ç»™ä½ çœ‹ï¼ŒåŒ…æ‹¬å¦‚ä½•è®© tokenizer æ›´åˆç†åœ°å¤„ç†ä¸­æ–‡ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56363a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bdc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
