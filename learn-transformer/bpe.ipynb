{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db4f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/11\n",
    "# zhangzhong\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba50ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing a text is splitting it into words or subwords,\n",
    "#  which then are converted to ids through a look-up table\n",
    "\n",
    "#  Byte-Pair Encoding (BPE), WordPiece, and SentencePiece,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Piece\n",
    "# Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization\n",
    "# which is loosely defined as splitting sentences into words\n",
    "# pros: While itâ€™s the most intuitive way to split texts into smaller chunks\n",
    "# cons: usually generates a very big vocabulary (the set of all unique words and tokens used), ex. Transformer XL: vocab size: 267,735\n",
    "#.      Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity.\n",
    "str = \"Don't you love ğŸ¤— Transformers? We sure do.\"\n",
    "tokens = [\"Do\", \"n't\", \"you\", \"love\", \"ğŸ¤—\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n",
    "\n",
    "# Character Piece\n",
    "# While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword tokenization\n",
    "# hybrid between word-level and character-level tokenization\n",
    "\n",
    "# Subword tokenization algorithms rely on the principle that \n",
    "# frequently used words should not be split into smaller subwords, \n",
    "# but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "# annoyingly -> annoying, ly\n",
    "# This is especially useful in agglutinative languages(é»ç€è¯­ç³») such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "# Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.\n",
    "\n",
    "# In addition, subword tokenization enables the model to process words it has [never seen before], by decomposing them into known subwords\n",
    "# some example\n",
    "str = \"I have a new GPU!\"\n",
    "# 1. lowercase the text\n",
    "# 2. for unseen word gpu, split to gp and u\n",
    "# # \"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n",
    "tokens = [\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE, Byte-Pair Encoding\n",
    "# è¿™ä¸€æ®µçš„è§£é‡Šéå¸¸ç²¾å½©ï¼Œä¸€ä¸‹å­å°±æ‡‚äº† https://huggingface.co/docs/transformers/tokenizer_summary#byte-pair-encoding-bpe\n",
    "\n",
    "# 1. BPE relies on a pre-tokenizer that splits the training data into words.\n",
    "#   - space tokenization, GPT2, RoBERTa\n",
    "#.  - rule-based tokenization: Moses, SpaCy, ftfy\n",
    "# after pre-tokenization, a set of unique words has been created, and the frequency of each word is counted.\n",
    "\n",
    "# 2. merge \n",
    "#   - use all the symbols as a base vocabulary\n",
    "#.  - learns merge rules to form a new symbol from two symbols of the base vocabulary\n",
    "#.  - until the vocabulary has attain the predefined desired vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2: Byte-level BPE, OpenAI å°±ç”¨è¿™ä¸ª tiktoken\n",
    "# The base vocabulary of GPT-2 consists of:\n",
    "# \tâ€¢\tAll 256 possible byte values (from 0x00 to 0xFF) â€” this is known as the byte-level encoding, and itâ€™s the core idea behind GPT-2â€™s tokenizer.\n",
    "# \tâ€¢\tThese 256 bytes represent every possible character in any UTF-8 encoded text, including letters, punctuation, emojis, symbols, etc., ensuring complete coverage and eliminating the need for an <unk> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f802443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Piece\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece\n",
    "\n",
    "# Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure itâ€™s worth it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf850bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Piece\n",
    "# https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece\n",
    "\n",
    "# BPE and WordPiece have the same problem:\n",
    "# It is assumed that the input text uses spaces to separate words, but like chinese, which do not.\n",
    "# it has two solution:\n",
    "#  1. use a specific pre-tokenizer for chinese, such as XLM\n",
    "#. 2. SentencePiece, treats the input as a raw input stream, thus including the space in the set of characters to use.\n",
    "#.    It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "#.    å…¶å®å°±æ˜¯æŠŠç©ºæ ¼åŠ å…¥åˆ°å­—å…¸ä¸­å‘—ã€‚åœ¨å­—å…¸ä¸­ç”¨ä¸‹åˆ’çº¿ _ æ¥è¡¨ç¤ºç©ºæ ¼ã€‚\n",
    "\n",
    "# example:\n",
    "str = \"Don't you love ğŸ¤— Transformers? We sure do.\"\n",
    "tokens = [\"â–Don\", \"'\", \"t\", \"â–you\", \"â–love\", \"â–\", \"ğŸ¤—\", \"â–\", \"Transform\", \"ers\", \"?\", \"â–We\", \"â–sure\", \"â–do\", \".\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
