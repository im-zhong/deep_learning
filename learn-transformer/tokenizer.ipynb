{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20417672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/11\n",
    "# zhangzhong\n",
    "# 1. [ ] https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "# 2. [ ] https://huggingface.co/docs/transformers/tokenizer_summary\n",
    "# 3. [ ] read all the tokenizer papers, and impl it，三篇论文都看，但是实现我们大概只会实现tiktoken也就是BPE，其他的没有用到，就先算了，节约时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ac103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers convert text into an array of numbers known as tensors, the inputs to a text model. \n",
    "# Split text into smaller words or subwords (tokens) according to some rules, and convert them into numbers (input ids)\n",
    "# A Transformers tokenizer also returns an attention mask to indicate which tokens should be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad64d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained tokenizer is saved in a tokenizer.model file with all its associated vocabulary files.\n",
    "# Pass a string of text to the tokenizer to return the input ids and attention mask, \n",
    "# and set the framework tensor type to return with the return_tensors parameter.\n",
    "# Whichever tokenizer you use, make sure the tokenizer vocabulary is the same as the pretrained models tokenizer vocabulary.\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35dde37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/fast_tokenizers?tokenizer-classes=AutoTokenizer#tokenizer-classes\n",
    "# There are two main tokenizer classes that build on top of the base class.\n",
    "# - PreTrainedTokenizer is a Python implementation, for example LlamaTokenizer.\n",
    "# - PreTrainedTokenizerFast is a fast Rust-based implementation from the Tokenizers library, for example LlamaTokenizerFast.\n",
    "# There are two ways you can load a tokenizer, with AutoTokenizer or a model-specific tokenizer.\n",
    "# AutoTokenizer: By default, AutoTokenizer tries to load a fast tokenizer if it’s available, otherwise, it loads the Python implementation.\n",
    "from transformers import AutoTokenizer, GemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087f03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also load your own tokenizer by passing its vocab file to the vocab_file parameter.\n",
    "# from transformers import GemmaTokenizerFast\n",
    "# tokenizer = GemmaTokenizerFast(vocab_file=\"my_vocab_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250eb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> 32000\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/fast_tokenizers?tokenizer-classes=model-specific+tokenizer#multimodal-tokenizers\n",
    "# Multimodal tokenizers\n",
    "# TODO： 目前还是不理解这个东西，先把text的tokenizer整明白了再说吧\n",
    "vision_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n",
    ")\n",
    "print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to train a fast tokenizer and reuse it in Transformers\n",
    "\n",
    "# 1. To train a Byte-Pair Encoding (BPE) tokenizer, \n",
    "# create a Tokenizer and BpeTrainer class and define the unknown token and special tokens.\n",
    "\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer \n",
    "\n",
    "# # define the unknown token\n",
    "# tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# # define the special tokens\n",
    "# trainer = BpeTrainer(\n",
    "#     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "# )\n",
    "\n",
    "# # Split the tokens on Whitespace to create tokens that don’t overlap with each other.\n",
    "# from tokenizers.pre_tokenizers import Whitespace \n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# # Call train on the text files and trainer to start training.\n",
    "# files = [...]\n",
    "# tokenizer.train(files, trainer)\n",
    "\n",
    "# # Use save to save the tokenizers configuration and vocabulary to a JSON file.\n",
    "# tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# # Now you can load and reuse the tokenizer object in Transformers by passing it to the tokenizer_object parameter in PreTrainedTokenizerFast.\n",
    "# from transformers import PreTrainedTokenizerFast\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# # To load a saved tokenizer from its JSON file, pass the file path to the tokenizer_file parameter in PreTrainedTokenizerFast.\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf62a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tictoken, BPE\n",
    "# https://github.com/huggingface/tokenizers/issues/437\n",
    "# https://github.com/huggingface/tokenizers/pull/1433\n",
    "# 竟然有一个pr修复了这个rust中数值溢出的问题，那我要试一下了，应该只有几百G的数据才会触发这个问题\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the subfolder parameter to from_pretrained() to specify where the tokenizer.model tiktoken file is located.\n",
    "# need protobuf library\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", subfolder=\"original\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e079a82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Transformers model expects the input to be a PyTorch, TensorFlow, or NumPy tensor\n",
    "# A tokenizers job is to preprocess text into those tensors.\n",
    "tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0cda3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'Ġare', 'Ġvery', 'Ġhappy', 'Ġto', 'Ġshow', 'Ġyou', 'Ġthe', 'ĠðŁ', '¤', 'Ĺ', 'ĠTransformers', 'Ġlibrary', '.']\n",
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "We are very happy to show you the 🤗 Transformers library.\n"
     ]
    }
   ],
   "source": [
    "# The tokenization process of converting text into input ids is completed in two steps.\n",
    "# tokenizer playground: 还有这种东西，牛逼 https://xenova-the-tokenizer-playground.static.hf.space/index.html\n",
    "\n",
    "# 1. tokenize\n",
    "tokens = tokenizer.tokenize(\"We are very happy to show you the 🤗 Transformers library.\")\n",
    "print(tokens)\n",
    "\n",
    "# 2. convert tokens to ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# 3. decode ids to text\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd823ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "We are very happy to show you the 🤗 Transformers library.\n",
      "We are very happy to show you the 🤗 Transformers library.\n"
     ]
    }
   ],
   "source": [
    "# speicial tokens\n",
    "model_inputs = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n",
    "print(model_inputs[\"input_ids\"])\n",
    "print(ids)\n",
    "# gpt2好像没有specical tokens，我看hugging face的教程里main是有的，参考那个东西吧\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))\n",
    "# 以为tokenizer会帮你处理special tokens，所以用的时候就采取第一种方法就行了，简单又正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf59711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1537, 644, 546, 1218, 12607, 30], [3987, 470, 892, 339, 4206, 546, 1218, 12607, 11, 25149, 13], [2061, 546, 22216, 82, 444, 30]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# batch tokenization\n",
    "# It is faster and more efficient to preprocess batches of text instead of a single sentence at a time\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aafedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1537, 644, 546, 1218, 12607, 30, 50256, 50256, 50256, 50256, 50256], [3987, 470, 892, 339, 4206, 546, 1218, 12607, 11, 25149, 13], [2061, 546, 22216, 82, 444, 30, 50256, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# padding and truncation\n",
    "# In the output above, the input_ids have different lengths\n",
    "# This is an issue because Transformers expects them to have the same lengths so it can pack them into a batch.\n",
    "\n",
    "# Padding adds a special padding token to ensure all sequences have the same length.\n",
    "# Set padding=True to pad the sequences to the longest sequence length in the batch.\n",
    "\n",
    "# Asking to pad but the tokenizer does not have a padding token.\n",
    "# Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \n",
    "# or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the pad token to the end of sentence token\n",
    "encoded_inputs = tokenizer(batch_sentences, padding=True)\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce9f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[1537, 644, 546, 1218, 12607, 30],\n",
      "               [3987, 470, 892, 339, 4206, 546, 1218, 12607],\n",
      "               [2061, 546, 22216, 82, 444, 30]]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "# Models are only able to process sequences up to a certain length\n",
    "# Truncation removes tokens from a sequence to ensure it doesn’t exceed the maximum length\n",
    "encoded_inputs = tokenizer(batch_sentences, max_length=8, truncation=True)\n",
    "# 牛逼！这个好用啊\n",
    "pprint(encoded_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
