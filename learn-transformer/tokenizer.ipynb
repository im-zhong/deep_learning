{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20417672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/11\n",
    "# zhangzhong\n",
    "# 1. [ ] https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "# 2. [ ] https://huggingface.co/docs/transformers/tokenizer_summary\n",
    "# 3. [ ] read all the tokenizer papers, and impl itï¼Œä¸‰ç¯‡è®ºæ–‡éƒ½çœ‹ï¼Œä½†æ˜¯å®ç°æˆ‘ä»¬å¤§æ¦‚åªä¼šå®ç°tiktokenä¹Ÿå°±æ˜¯BPEï¼Œå…¶ä»–çš„æ²¡æœ‰ç”¨åˆ°ï¼Œå°±å…ˆç®—äº†ï¼ŒèŠ‚çº¦æ—¶é—´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ac103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers convert text into an array of numbers known as tensors, the inputs to a text model. \n",
    "# Split text into smaller words or subwords (tokens) according to some rules, and convert them into numbers (input ids)\n",
    "# A Transformers tokenizer also returns an attention mask to indicate which tokens should be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad64d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pretrained tokenizer is saved in a tokenizer.model file with all its associated vocabulary files.\n",
    "# Pass a string of text to the tokenizer to return the input ids and attention mask, \n",
    "# and set the framework tensor type to return with the return_tensors parameter.\n",
    "# Whichever tokenizer you use, make sure the tokenizer vocabulary is the same as the pretrained models tokenizer vocabulary.\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35dde37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/fast_tokenizers?tokenizer-classes=AutoTokenizer#tokenizer-classes\n",
    "# There are two main tokenizer classes that build on top of the base class.\n",
    "# - PreTrainedTokenizer is a Python implementation, for example LlamaTokenizer.\n",
    "# - PreTrainedTokenizerFast is a fast Rust-based implementation from the Tokenizers library, for example LlamaTokenizerFast.\n",
    "# There are two ways you can load a tokenizer, with AutoTokenizer or a model-specific tokenizer.\n",
    "# AutoTokenizer: By default, AutoTokenizer tries to load a fast tokenizer if itâ€™s available, otherwise, it loads the Python implementation.\n",
    "from transformers import AutoTokenizer, GemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087f03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could also load your own tokenizer by passing its vocab file to the vocab_file parameter.\n",
    "# from transformers import GemmaTokenizerFast\n",
    "# tokenizer = GemmaTokenizerFast(vocab_file=\"my_vocab_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250eb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> 32000\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/fast_tokenizers?tokenizer-classes=model-specific+tokenizer#multimodal-tokenizers\n",
    "# Multimodal tokenizers\n",
    "# TODOï¼š ç›®å‰è¿˜æ˜¯ä¸ç†è§£è¿™ä¸ªä¸œè¥¿ï¼Œå…ˆæŠŠtextçš„tokenizeræ•´æ˜ç™½äº†å†è¯´å§\n",
    "vision_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n",
    ")\n",
    "print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to train a fast tokenizer and reuse it in Transformers\n",
    "\n",
    "# 1. To train a Byte-Pair Encoding (BPE) tokenizer, \n",
    "# create a Tokenizer and BpeTrainer class and define the unknown token and special tokens.\n",
    "\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer \n",
    "\n",
    "# # define the unknown token\n",
    "# tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# # define the special tokens\n",
    "# trainer = BpeTrainer(\n",
    "#     special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "# )\n",
    "\n",
    "# # Split the tokens on Whitespace to create tokens that donâ€™t overlap with each other.\n",
    "# from tokenizers.pre_tokenizers import Whitespace \n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# # Call train on the text files and trainer to start training.\n",
    "# files = [...]\n",
    "# tokenizer.train(files, trainer)\n",
    "\n",
    "# # Use save to save the tokenizers configuration and vocabulary to a JSON file.\n",
    "# tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "# # Now you can load and reuse the tokenizer object in Transformers by passing it to the tokenizer_object parameter in PreTrainedTokenizerFast.\n",
    "# from transformers import PreTrainedTokenizerFast\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "\n",
    "# # To load a saved tokenizer from its JSON file, pass the file path to the tokenizer_file parameter in PreTrainedTokenizerFast.\n",
    "# fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf62a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tictoken, BPE\n",
    "# https://github.com/huggingface/tokenizers/issues/437\n",
    "# https://github.com/huggingface/tokenizers/pull/1433\n",
    "# ç«Ÿç„¶æœ‰ä¸€ä¸ªprä¿®å¤äº†è¿™ä¸ªrustä¸­æ•°å€¼æº¢å‡ºçš„é—®é¢˜ï¼Œé‚£æˆ‘è¦è¯•ä¸€ä¸‹äº†ï¼Œåº”è¯¥åªæœ‰å‡ ç™¾Gçš„æ•°æ®æ‰ä¼šè§¦å‘è¿™ä¸ªé—®é¢˜\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the subfolder parameter to from_pretrained() to specify where the tokenizer.model tiktoken file is located.\n",
    "# need protobuf library\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", subfolder=\"original\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e079a82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Transformers model expects the input to be a PyTorch, TensorFlow, or NumPy tensor\n",
    "# A tokenizers job is to preprocess text into those tensors.\n",
    "tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0cda3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'Ä are', 'Ä very', 'Ä happy', 'Ä to', 'Ä show', 'Ä you', 'Ä the', 'Ä Ã°Å', 'Â¤', 'Ä¹', 'Ä Transformers', 'Ä library', '.']\n",
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "We are very happy to show you the ğŸ¤— Transformers library.\n"
     ]
    }
   ],
   "source": [
    "# The tokenization process of converting text into input ids is completed in two steps.\n",
    "# tokenizer playground: è¿˜æœ‰è¿™ç§ä¸œè¥¿ï¼Œç‰›é€¼ https://xenova-the-tokenizer-playground.static.hf.space/index.html\n",
    "\n",
    "# 1. tokenize\n",
    "tokens = tokenizer.tokenize(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "print(tokens)\n",
    "\n",
    "# 2. convert tokens to ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "# 3. decode ids to text\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd823ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "[1135, 389, 845, 3772, 284, 905, 345, 262, 12520, 97, 245, 39185, 5888, 13]\n",
      "We are very happy to show you the ğŸ¤— Transformers library.\n",
      "We are very happy to show you the ğŸ¤— Transformers library.\n"
     ]
    }
   ],
   "source": [
    "# speicial tokens\n",
    "model_inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "print(model_inputs[\"input_ids\"])\n",
    "print(ids)\n",
    "# gpt2å¥½åƒæ²¡æœ‰specical tokensï¼Œæˆ‘çœ‹hugging faceçš„æ•™ç¨‹é‡Œmainæ˜¯æœ‰çš„ï¼Œå‚è€ƒé‚£ä¸ªä¸œè¥¿å§\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))\n",
    "# ä»¥ä¸ºtokenizerä¼šå¸®ä½ å¤„ç†special tokensï¼Œæ‰€ä»¥ç”¨çš„æ—¶å€™å°±é‡‡å–ç¬¬ä¸€ç§æ–¹æ³•å°±è¡Œäº†ï¼Œç®€å•åˆæ­£ç¡®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf59711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1537, 644, 546, 1218, 12607, 30], [3987, 470, 892, 339, 4206, 546, 1218, 12607, 11, 25149, 13], [2061, 546, 22216, 82, 444, 30]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# batch tokenization\n",
    "# It is faster and more efficient to preprocess batches of text instead of a single sentence at a time\n",
    "\n",
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aafedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1537, 644, 546, 1218, 12607, 30, 50256, 50256, 50256, 50256, 50256], [3987, 470, 892, 339, 4206, 546, 1218, 12607, 11, 25149, 13], [2061, 546, 22216, 82, 444, 30, 50256, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# padding and truncation\n",
    "# In the output above, the input_ids have different lengths\n",
    "# This is an issue because Transformers expects them to have the same lengths so it can pack them into a batch.\n",
    "\n",
    "# Padding adds a special padding token to ensure all sequences have the same length.\n",
    "# Set padding=True to pad the sequences to the longest sequence length in the batch.\n",
    "\n",
    "# Asking to pad but the tokenizer does not have a padding token.\n",
    "# Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \n",
    "# or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set the pad token to the end of sentence token\n",
    "encoded_inputs = tokenizer(batch_sentences, padding=True)\n",
    "print(encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce9f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[1537, 644, 546, 1218, 12607, 30],\n",
      "               [3987, 470, 892, 339, 4206, 546, 1218, 12607],\n",
      "               [2061, 546, 22216, 82, 444, 30]]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "# Models are only able to process sequences up to a certain length\n",
    "# Truncation removes tokens from a sequence to ensure it doesnâ€™t exceed the maximum length\n",
    "encoded_inputs = tokenizer(batch_sentences, max_length=8, truncation=True)\n",
    "# ç‰›é€¼ï¼è¿™ä¸ªå¥½ç”¨å•Š\n",
    "pprint(encoded_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
