{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6502a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/2/6\n",
    "# zhangzhong\n",
    "# REINFORCE with baseline, monet caro actor aritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afcaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    # TODO: discrete state mapping to embedding\n",
    "    def __init__(self, goal: int = 4):\n",
    "        self.goal: int = goal\n",
    "        self.state: int = 0\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "    def step(self, action: str) -> tuple[float, int]:\n",
    "        if action == \"LEFT\":\n",
    "            self.state = max(0, self.state-1)\n",
    "        elif action == \"RIGHT\":\n",
    "            self.state = min(self.goal, self.state+1)\n",
    "        # return (10 if self.is_done() else -1, self.state)\n",
    "        return (-1 if action == \"LEFT\" else 1, self.state)\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.state == self.goal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f15cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # BUG: out of index\n",
    "        # self.state_embedding = nn.Embedding(num_embeddings=env.goal, embedding_dim=8)\n",
    "        self.state_embedding = nn.Embedding(num_embeddings=env.goal + 1, embedding_dim=8)\n",
    "        self.env = env\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=2)\n",
    "        )\n",
    "\n",
    "    def curr_state(self) -> Tensor:\n",
    "        return torch.tensor(data=[self.env.state])\n",
    "\n",
    "    def get_action(self, state: Tensor) -> tuple[str, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            embeddings: Tensor = self.state_embedding(state)\n",
    "            logits: Tensor = self.policy_net(embeddings)\n",
    "            index = logits.argmax()\n",
    "            print(index)\n",
    "            \n",
    "            probs = Categorical(logits=logits)\n",
    "            print(probs)\n",
    "            actions = probs.sample()\n",
    "            # print(actions)\n",
    "            return self.env.actions[actions.int()], probs.log_prob(value=actions)\n",
    "            # log_probs = probs.log_prob(actions)\n",
    "\n",
    "    # state\n",
    "    # https://docs.pytorch.org/docs/stable/distributions.html#categorical\n",
    "    def sample_action(self, state: Tensor) -> tuple[str, Tensor]:\n",
    "\n",
    "        # 我们在sample的时候就能计算出log_probs来，要不也顺便返回一下吧\n",
    "\n",
    "        embeddings: Tensor = self.state_embedding(state)\n",
    "        logits = self.policy_net(embeddings)    \n",
    "        probs = Categorical(logits=logits)   \n",
    "        actions = probs.sample()\n",
    "        # print(actions)\n",
    "        return self.env.actions[actions.int()], probs.log_prob(value=actions)\n",
    "        # log_probs = probs.log_prob(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebd1213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LEFT', tensor([-0.4201], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "agent.sample_action(state=torch.tensor(data=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b9403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.state_embedding = nn.Embedding(num_embeddings=env.goal + 1, embedding_dim=8)\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: Tensor) -> Tensor:\n",
    "        embeddings = self.state_embedding(state)\n",
    "        return self.value_net(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e318dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample episode\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def sample_episode(env: Environment, agent: Agent) -> list[tuple[Tensor, str, float, Tensor]]:\n",
    "    env.reset()\n",
    "\n",
    "    # episode 要收集什么？ (st: tensor, at: str, rt: float)\n",
    "    # ()\n",
    "    episode = []\n",
    "    curr_state: Tensor = agent.curr_state()\n",
    "\n",
    "    for t in range(100):\n",
    "        # initial state = ?\n",
    "\n",
    "\n",
    "        if env.is_done():\n",
    "            break\n",
    "        # \n",
    "        action, log_prob = agent.sample_action(state=curr_state)\n",
    "        # \n",
    "        reward, next_state = env.step(action=action)\n",
    "        \n",
    "        episode.append((curr_state, action, reward, log_prob))\n",
    "        curr_state = torch.tensor(data=[next_state])\n",
    "\n",
    "        # BUG: 如果env已经done了，上面还会采样状态，就是不对的\n",
    "        # 应该在最开始就判断env是不是已经结束\n",
    "        # if env.is_done():\n",
    "        #     break\n",
    "\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b984b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.4928], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.4928], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.4928], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.4928], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.9440], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.5847], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.9440], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.5847], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.4928], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.4201], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-1.0700], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.9440], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.5847], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.9440], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', 1, tensor([-0.8148], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'RIGHT', 1, tensor([-0.8638], grad_fn=<SqueezeBackward1>))]\n"
     ]
    }
   ],
   "source": [
    "# collect on episode\n",
    "\n",
    "episode = sample_episode(env, agent)\n",
    "print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2544931",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate rewards-to-go\n",
    "\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def calculate_rewards_to_go(rewards: list[float], gamma: float = 1.0) -> list[float]:\n",
    "    reversed_rewards: Iterator[float] = reversed(rewards)\n",
    "    rewards_to_go: list[float] = []\n",
    "    curr_reward = 0\n",
    "    for reward in reversed_rewards:\n",
    "        curr_reward = reward + gamma*curr_reward\n",
    "        rewards_to_go.append(curr_reward)\n",
    "    return list(reversed(rewards_to_go))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfefc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 1.0]\n",
      "[2.71, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "## test rewards to go\n",
    "rewards = [1.0, 1.0, 1.0]\n",
    "print(calculate_rewards_to_go(rewards, gamma=1.0))\n",
    "print(calculate_rewards_to_go(rewards, gamma=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b7496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with rewards sampled from a episode\n",
    "\n",
    "def make_rewards_to_go(episode: list, gamma: float) -> list[float]:\n",
    "    rewards: list[float] = [reward for _, _, reward, _ in episode]\n",
    "    rewards_to_go = calculate_rewards_to_go(rewards=rewards, gamma=gamma)\n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154b277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect one batch of data, how?\n",
    "# 这里有两种实现的方法\n",
    "# 一种是严格的按照公式来计算，每个episode都计算完成之后，然后再计算N个episode\n",
    "# 还有一种就是每个step看成是独立的，就收集batch size个step，然后就计算就行\n",
    "\n",
    "# 从整体实现上来将，肯定是第二种简单，就按照这种来吧，从数学上是一样的\n",
    "# 咱们先不写reward to go了\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# 还有一个问题，episode的每个step对应的reward应该是rewards to go\n",
    "# 可以写一个函数来计算一下\n",
    "\n",
    "def collect_training_data() -> tuple[list[Tensor], list[str], list[Tensor], list[float]]:\n",
    "    batch_log_probs = []\n",
    "    batch_rewards_to_go = []\n",
    "    batch_states = []\n",
    "    batch_actions = []\n",
    "    while True:\n",
    "        episode = sample_episode(env=env, agent=agent)\n",
    "        rewards_to_go = make_rewards_to_go(episode=episode, gamma=gamma)\n",
    "        for (curr_state, curr_action, _, log_prob), reward in zip(episode, rewards_to_go):\n",
    "            # print(curr_state, curr_action, log_prob, reward)\n",
    "            \n",
    "            batch_states.append(curr_state)\n",
    "            batch_actions.append(curr_action)\n",
    "            # TODO(policy-gradient / PPO-ready):\n",
    "            # 当前实现中在采样阶段直接保存 log_prob（带计算图），\n",
    "            # 并在同一轮中立刻用于 policy_loss.backward()。\n",
    "            #\n",
    "            # 这种方式在「一次采样 -> 一次更新」的 Monte Carlo REINFORCE / Actor-Critic 中是数学正确的，\n",
    "            # 但有以下限制：\n",
    "            # - 不能在同一批数据上做多次 policy update（如 PPO 的多 epoch 更新）\n",
    "            # - 不能在参数更新后复用旧 trajectory\n",
    "            #\n",
    "            # 如果未来实现 PPO / 多次更新：\n",
    "            # - 应改为存 (state, action)\n",
    "            # - 在 update 阶段重新计算 log_prob\n",
    "            # 这是 Spinning Up / PPO 的标准做法。\n",
    "            batch_log_probs.append(log_prob)\n",
    "            batch_rewards_to_go.append(reward)\n",
    "            if (len(batch_log_probs) >= batch_size):\n",
    "                break\n",
    "\n",
    "        if (len(batch_log_probs) >= batch_size):\n",
    "            break\n",
    "\n",
    "    return batch_states, batch_actions, batch_log_probs, batch_rewards_to_go\n",
    "\n",
    "# 我们的一个batch的数据需要什么？\n",
    "# optimizer.zero_grad()\n",
    "# compute_loss -> loss\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db71d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.8638], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.8638], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.5474], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.5474], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.5474], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.5474], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.8638], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.8148], grad_fn=<SqueezeBackward1>), tensor([-0.5474], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>), tensor([-0.5847], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.4928], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-0.4201], grad_fn=<SqueezeBackward1>), tensor([-1.0700], grad_fn=<SqueezeBackward1>), tensor([-0.9440], grad_fn=<SqueezeBackward1>)] [-1.147460783168619, -0.14895028602890803, -1.1605558444736446, -0.16217762068044905, 0.8462852316359101, -0.15526744279201, 0.853265209301, 1.8719850599, 2.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0, -1.1407558444736443, -0.1421776206804488, -1.1537147683640898, -0.15526744279201, 0.853265209301, 1.8719850599, 2.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0, -3.6113410577219187, -2.6377182401231503, -1.6542608486092427, -2.68107156425176, -3.7182541053048084, -4.765913237681625, -3.8039527653349747, -2.8322755205403785, -3.8709853742832108, -4.920187246750718, -3.9597850977279974, -5.009883937098987, -4.0503878152515025, -3.081199813385356, -4.122424053924602, -3.153963690832931, -2.1757208998312434, -1.1875968685164078, -0.1894917863802098, 0.8186951652725154, -0.18313619669442882, 0.8251149528339103, 1.8435504574079902, 2.872273189301, 3.9113870599, 2.94079501, 1.960399, 0.9701, 1.99, 1.0, -22.3286257845939, -21.54406644908475, -20.75158227180278, -19.95109320384119, -19.142518387718372, -18.325776149210476, -17.500783989101492, -16.66745857484999, -17.84591775237373, -19.036280557953262, -20.23866723025582, -19.432997202278607, -18.619189093210714, -17.797160700212842, -18.987031010316002, -20.188920212440408, -19.382747689333744, -20.588634029630043, -19.786499019828327, -20.996463656392248, -20.198448137769947, -19.39237185633328, -20.59835541043766, -19.796318596401676, -21.006382420607753, -20.208467091522984, -21.4226940318414, -20.628983870546868, -19.827256434895826, -19.017430742319018, -18.199424992241433, -17.37315655781963, -16.538541977595585, -15.69549694706625, -14.84393631016793, -13.983774050674677, -13.114923283509775, -12.23729624596947, -11.35080428885805, -10.455357867533385, -11.571068553064025, -10.677847023296994, -9.775603053835347, -8.864245508924594, -7.9436823322470635, -7.013820537623297, -6.0745661996194915, -5.125824444060092, -6.187701458646558, -5.240102483481372, -6.3031338216983555, -7.376902850200359, -6.441316010303393, -5.496278798286256, -6.561897776046723, -7.638280581865377, -6.7053339210761385, -5.762963556642564, -4.811074299638953, -3.849569999635306, -4.898555555187178, -3.9379349042294725, -2.9676110143732046, -1.987485873104247, -3.017662498085098, -4.058244947560706, -3.0891363106673797, -2.110238697643818, -1.1214532299432505, -2.142882050447728, -1.1544263135835635, -0.15598617533693282, 0.852539216831381, -0.14895028602890803]\n"
     ]
    }
   ],
   "source": [
    "batch_states, batch_actions, batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "print(batch_log_probs, batch_rewards_to_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45c364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value_loss(values: Tensor, batch_rewards_to_go: list[float]) -> Tensor:\n",
    "    rewards = torch.tensor(batch_rewards_to_go)\n",
    "    # assert values.squeeze(-1).shape == rewards.shape\n",
    "\n",
    "    loss = torch.mean((values.squeeze(dim=-1)-rewards).pow(2))\n",
    "    return loss\n",
    "\n",
    "def compute_loss(batch_log_probs: list[Tensor], batch_rewards_to_go: list[float], batch_vt: list[float] = [0]) -> Tensor:\n",
    "    grads = [log_probs*(rewards - vt) for log_probs, rewards, vt in zip(batch_log_probs, batch_rewards_to_go, batch_vt)]\n",
    "\n",
    "    # BUG!!! 这一步是构建一个新的tensor，然后把旧的值给复制过来，所以不会保留计算图，requireds_grad=False\n",
    "    # grads_tensor = torch.tensor(grads)\n",
    "    grads_tensor = torch.stack(grads)\n",
    "    assert grads_tensor.shape == (batch_size, 1)\n",
    "    loss = -grads_tensor.mean()\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63768e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env: Environment, agent: Agent, value_net: ValueNet, optimizer4agent, optimizer4value):\n",
    "    # 1. collect training data of this epoch\n",
    "    batch_states, batch_actions, batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "\n",
    "    # 2. zero policy net grads\n",
    "    optimizer4agent.zero_grad()\n",
    "    # 应该在这里吧\n",
    "    optimizer4value.zero_grad()\n",
    "\n",
    "    # 3. compute vt by using value net\n",
    "    values = value_net.forward(state=torch.stack(batch_states).squeeze(dim=-1))\n",
    "    assert values.shape == (batch_size, 1)\n",
    "    # this must use detach, to isolate loss of policy net and value net\n",
    "    # 这确保 policy loss 不会把梯度传进 value_net\n",
    "    # TODO(actor-critic):\n",
    "    # 当前 actor 和 critic 使用的是完全独立的网络参数，因此：\n",
    "    # - policy_loss.backward() 只会更新 actor\n",
    "    # - value_loss.backward() 只会更新 critic\n",
    "    # 这是在数学和工程上都成立的。\n",
    "    #\n",
    "    # ⚠️ 注意：如果未来引入 shared embedding / shared trunk（actor-critic 共享部分参数），\n",
    "    # 必须严格控制梯度流向：\n",
    "    # - policy loss 中的 advantage 必须对 value.detach()\n",
    "    # - actor 与 critic 的 backward / optimizer.step 需要明确隔离\n",
    "    # 否则 critic 会被 policy loss 的梯度“错误地拖动”，导致训练不稳定。\n",
    "    batch_vt: list[float] = values.detach().squeeze(dim=-1).tolist()\n",
    "\n",
    "    # 4. policy compute loss\n",
    "    # 大问题！我这样计算出来的loss是没有梯度的！为什么？\n",
    "    policy_loss = compute_loss(batch_log_probs, batch_rewards_to_go, batch_vt=batch_vt)\n",
    "\n",
    "    # 5. policy net backward propogation\n",
    "    policy_loss.backward()\n",
    "    # 1) actor must have grads\n",
    "    assert any(p.grad is not None for p in agent.parameters())\n",
    "    # 2) critic must NOT get grads from policy loss\n",
    "    assert all(p.grad is None for p in value_net.parameters())\n",
    "\n",
    "    # 6. optimize policy net\n",
    "    optimizer4agent.step()\n",
    "\n",
    "    ## 7. compute vt loss and optimize\n",
    "    # BUG！不对，这个optimizer 不应该在这里\n",
    "    # 经过测试，这个optimizer在哪里都不影响\n",
    "    # optimizer4value.zero_grad()\n",
    "    value_loss = compute_value_loss(values, batch_rewards_to_go)\n",
    "    value_loss.backward()\n",
    "    assert any(p.grad is not None for p in value_net.parameters())\n",
    "    optimizer4value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b3a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e032e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "LEFT tensor([-0.6802])\n",
      "tensor(0)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.9141])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.6536])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "LEFT tensor([-0.7967])\n",
      "tensor([[-0.0881]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0705]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0484]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0648]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 200 # 经过测试，200个epoch可以较好的拟合value net\n",
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "value_net = ValueNet(env=env)\n",
    "optimizer4agent = Adam(agent.parameters(), lr=1e-3)\n",
    "optimizer4value = Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "# 输出一下policy net, 看看动作选的对不对\n",
    "for i in range(env.goal):\n",
    "    action, logp= agent.get_action(state=torch.tensor(data=[i]))\n",
    "    print(action, logp)\n",
    "\n",
    "# 输出一下value net的数值, 看看训练的对不对\n",
    "for i in range(env.goal):\n",
    "    output = value_net.forward(state=torch.tensor(data=[i]))\n",
    "    print(output.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "725c657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 47.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train_one_epoch(env=env, agent=agent, value_net=value_net, optimizer4agent=optimizer4agent, optimizer4value=optimizer4value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e806ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.0375])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.0366])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.1620])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.1496])\n",
      "tensor([[3.8382]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[2.9628]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.9706]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0019]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 输出一下policy net, 看看动作选的对不对\n",
    "for i in range(env.goal):\n",
    "    action, logp= agent.get_action(state=torch.tensor(data=[i]))\n",
    "    print(action, logp)\n",
    "\n",
    "# 输出一下value net的数值, 看看训练的对不对\n",
    "for i in range(env.goal):\n",
    "    output = value_net.forward(state=torch.tensor(data=[i]))\n",
    "    print(output.float())\n",
    "# 直接根据Vt的公式进行计算，也就是sum of discouted rewards, 应该是 4 3 2 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
