{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "c6502a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/2/6\n",
    "# zhangzhong\n",
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "3afcaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    # TODO: discrete state mapping to embedding\n",
    "    def __init__(self, goal: int = 4):\n",
    "        self.goal: int = goal\n",
    "        self.state: int = 0\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "    def step(self, action: str) -> tuple[float, int]:\n",
    "        if action == \"LEFT\":\n",
    "            self.state = max(0, self.state-1)\n",
    "        elif action == \"RIGHT\":\n",
    "            self.state = min(self.goal, self.state+1)\n",
    "        # return (10 if self.is_done() else -1, self.state)\n",
    "        return (-1 if action == \"LEFT\" else 1, self.state)\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.state == self.goal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # BUG: out of index\n",
    "        # self.state_embedding = nn.Embedding(num_embeddings=env.goal, embedding_dim=8)\n",
    "        self.state_embedding = nn.Embedding(num_embeddings=env.goal + 1, embedding_dim=8)\n",
    "        self.env = env\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=2)\n",
    "        )\n",
    "\n",
    "    def curr_state(self) -> Tensor:\n",
    "        return torch.tensor(data=[self.env.state])\n",
    "\n",
    "    def get_action(self, state: Tensor) -> tuple[str, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            embeddings: Tensor = self.state_embedding(state)\n",
    "            logits: Tensor = self.policy_net(embeddings)\n",
    "            index = logits.argmax()\n",
    "            print(index)\n",
    "            \n",
    "            probs = Categorical(logits=logits)\n",
    "            print(probs)\n",
    "            actions = probs.sample()\n",
    "            # print(actions)\n",
    "            return self.env.actions[actions.int()], probs.log_prob(value=actions)\n",
    "            # log_probs = probs.log_prob(actions)\n",
    "\n",
    "    # state\n",
    "    # https://docs.pytorch.org/docs/stable/distributions.html#categorical\n",
    "    def sample_action(self, state: Tensor) -> tuple[str, Tensor]:\n",
    "\n",
    "        # 我们在sample的时候就能计算出log_probs来，要不也顺便返回一下吧\n",
    "\n",
    "        embeddings: Tensor = self.state_embedding(state)\n",
    "        logits = self.policy_net(embeddings)    \n",
    "        probs = Categorical(logits=logits)   \n",
    "        actions = probs.sample()\n",
    "        # print(actions)\n",
    "        return self.env.actions[actions.int()], probs.log_prob(value=actions)\n",
    "        # log_probs = probs.log_prob(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "0ebd1213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LEFT', tensor([-0.6850], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "agent.sample_action(state=torch.tensor(data=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e318dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample episode\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def sample_episode(env: Environment, agent: Agent) -> list:\n",
    "    env.reset()\n",
    "\n",
    "    # episode 要收集什么？ (st: tensor, at: str, rt: float)\n",
    "    # ()\n",
    "    episode = []\n",
    "    curr_state: Tensor = agent.curr_state()\n",
    "\n",
    "    for t in range(100):\n",
    "        # initial state = ?\n",
    "\n",
    "\n",
    "        if env.is_done():\n",
    "            break\n",
    "        # \n",
    "        action, log_prob = agent.sample_action(state=curr_state)\n",
    "        # \n",
    "        reward, next_state = env.step(action=action)\n",
    "        \n",
    "        episode.append((curr_state, action, reward, log_prob))\n",
    "        curr_state = torch.tensor(data=[next_state])\n",
    "\n",
    "        # BUG: 如果env已经done了，上面还会采样状态，就是不对的\n",
    "        # 应该在最开始就判断env是不是已经结束\n",
    "        # if env.is_done():\n",
    "        #     break\n",
    "\n",
    "\n",
    "    return episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "b984b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.6236], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.6236], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.6236], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.7679], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.4209], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.7679], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.4209], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.6236], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.6236], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.6850], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', 1, tensor([-0.7014], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', 1, tensor([-0.7679], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', 1, tensor([-1.0685], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'LEFT', -1, tensor([-0.7470], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', 1, tensor([-1.0685], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'RIGHT', 1, tensor([-0.6421], grad_fn=<SqueezeBackward1>))]\n"
     ]
    }
   ],
   "source": [
    "# collect on episode\n",
    "\n",
    "episode = sample_episode(env, agent)\n",
    "print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "e2544931",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate rewards-to-go\n",
    "\n",
    "def calculate_rewards_to_go(rewards: list[float], gamma: float = 1.0):\n",
    "    reversed_rewards = reversed(rewards)\n",
    "    rewards_to_go = []\n",
    "    curr_reward = 0\n",
    "    for reward in reversed_rewards:\n",
    "        curr_reward = reward + gamma*curr_reward\n",
    "        rewards_to_go.append(curr_reward)\n",
    "    return list(reversed(rewards_to_go))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "cfefc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 1.0]\n",
      "[2.71, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "## test rewards to go\n",
    "rewards = [1.0, 1.0, 1.0]\n",
    "print(calculate_rewards_to_go(rewards, gamma=1.0))\n",
    "print(calculate_rewards_to_go(rewards, gamma=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "d9b7496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with rewards sampled from a episode\n",
    "\n",
    "def make_rewards_to_go(episode: list, gamma: float) -> list:\n",
    "    rewards: list[float] = [reward for _, _, reward, _ in episode]\n",
    "    rewards_to_go = calculate_rewards_to_go(rewards=rewards, gamma=gamma)\n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "154b277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect one batch of data, how?\n",
    "# 这里有两种实现的方法\n",
    "# 一种是严格的按照公式来计算，每个episode都计算完成之后，然后再计算N个episode\n",
    "# 还有一种就是每个step看成是独立的，就收集batch size个step，然后就计算就行\n",
    "\n",
    "# 从整体实现上来将，肯定是第二种简单，就按照这种来吧，从数学上是一样的\n",
    "# 咱们先不写reward to go了\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# 还有一个问题，episode的每个step对应的reward应该是rewards to go\n",
    "# 可以写一个函数来计算一下\n",
    "\n",
    "def collect_training_data():\n",
    "    batch_log_probs = []\n",
    "    batch_rewards_to_go = []\n",
    "    while True:\n",
    "        episode = sample_episode(env=env, agent=agent)\n",
    "        rewards_to_go = make_rewards_to_go(episode=episode, gamma=gamma)\n",
    "        for (curr_state, curr_action, _, log_prob), reward in zip(episode, rewards_to_go):\n",
    "            # print(curr_state, curr_action, log_prob, reward)\n",
    "            \n",
    "\n",
    "            batch_log_probs.append(log_prob)\n",
    "            batch_rewards_to_go.append(reward)\n",
    "            if (len(batch_log_probs) >= batch_size):\n",
    "                break\n",
    "\n",
    "        if (len(batch_log_probs) >= batch_size):\n",
    "            break\n",
    "\n",
    "    return batch_log_probs, batch_rewards_to_go\n",
    "\n",
    "# 我们的一个batch的数据需要什么？\n",
    "# optimizer.zero_grad()\n",
    "# compute_loss -> loss\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "db71d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.7470], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.7470], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.7470], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.7470], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.7470], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.6421], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.6421], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-0.4209], grad_fn=<SqueezeBackward1>), tensor([-0.7679], grad_fn=<SqueezeBackward1>), tensor([-1.0685], grad_fn=<SqueezeBackward1>), tensor([-0.6421], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>), tensor([-0.7014], grad_fn=<SqueezeBackward1>), tensor([-0.6236], grad_fn=<SqueezeBackward1>), tensor([-0.6850], grad_fn=<SqueezeBackward1>)] [-16.196523099349342, -15.350023332676106, -16.515175083511217, -17.692096043950723, -16.8607030746977, -18.04111421686636, -17.213246683703396, -16.37701685222565, -15.532340254773384, -14.679131570478166, -15.837506636846632, -14.987380441259225, -14.128667112383056, -15.281481931700057, -16.445941345151574, -15.601960954698558, -14.749455509796524, -15.908540918986388, -15.059132241400391, -14.201143678182213, -13.334488563820416, -14.479281377596381, -13.615435734945839, -14.763066398935191, -13.902087271651707, -13.032411385506775, -14.174152914653307, -13.307225166316472, -14.451742592238862, -15.607820800241274, -14.755374545698258, -15.914519743129553, -15.065171457706619, -16.22744591687537, -15.381258501894315, -14.526523739287187, -13.66315529220928, -14.811267971928567, -13.950775729220775, -13.08159164567755, -12.203627924926819, -11.316795883764463, -10.421005943196427, -9.51616761939033, -8.602189514535688, -9.699181327813827, -8.787051846276594, -7.865708935632922, -6.935059530942345, -8.01521164741651, -7.086072371127788, -6.147547849624028, -7.219745302650534, -6.282571012778317, -5.335930315937694, -6.399929612058277, -7.474676375816442, -8.560279167491355, -7.636645623728642, -6.703682448210749, -5.76129540223308, -4.809389295184929, -5.868069995136293, -6.937444439531609, -5.9974186257895035, -5.04789760180758, -4.088785456371292, -3.1199853094659513, -4.1616013226928805, -3.1935366895887682, -4.2358956460492605, -3.2685814606558186, -2.2914964249048673, -1.3045418433382496, -0.3076180235739895, 0.699375733763647, 1.716541145215805, 2.7439809547634395, 1.7615969240034746, 2.789491842427752, 3.82776953780581, 2.8563328664705154, 1.8750837035055712, 0.8839229328339102, 1.9029524574079901, 0.9120731893010001, 1.9313870599, 0.94079501, 1.960399, 0.9701, 1.99, 1.0, 1.792431813771092, 0.8004361755263555, -0.201579620680449, 0.8064852316359101, 1.82473255720799, 2.853265209301, 1.8719850599, 2.90099501, 3.9403989999999998, 2.9701, 1.99, 1.0, -3.1985209541742043, -2.2207282365396, -3.2532608449894953, -2.2760210555449447, -1.2889101571161055, -0.29182844153141974, 0.7153248065339195, 1.7326513197312319, 2.7602538583143756, 1.7780342003175509, 2.80609515183591, 1.8243385372079897, 2.8528672093009995, 3.8917850598999997, 2.9209950099999995, 1.9403989999999998, 2.9701, 1.99, 1.0, -9.650007215578237, -8.737381025836603, -7.815536389733943, -8.904582211852468, -7.984426476618653]\n"
     ]
    }
   ],
   "source": [
    "batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "print(batch_log_probs, batch_rewards_to_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a45c364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(batch_log_probs: list[Tensor], batch_rewards_to_go: list[Tensor]) -> Tensor:\n",
    "    grads = [log_probs*rewards for log_probs, rewards in zip(batch_log_probs, batch_rewards_to_go)]\n",
    "\n",
    "    # BUG!!! 这一步是构建一个新的tensor，然后把旧的值给复制过来，所以不会保留计算图，requireds_grad=False\n",
    "    # grads_tensor = torch.tensor(grads)\n",
    "    grads_tensor = torch.stack(grads)\n",
    "    assert grads_tensor.shape == (batch_size, 1)\n",
    "    loss = -grads_tensor.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "63768e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env: Environment, agent: Agent, optimizer):\n",
    "    # 1. collect training data of this epoch\n",
    "    batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "\n",
    "    # 2. zero policy net grads\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 3. compute loss\n",
    "    # 大问题！我这样计算出来的loss是没有梯度的！为什么？\n",
    "    loss = compute_loss(batch_log_probs, batch_rewards_to_go)\n",
    "\n",
    "    # 4. backward propogation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. optimize policy net\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b3a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e032e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.6771])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.6878])\n",
      "tensor(0)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "LEFT tensor([-0.6198])\n",
      "tensor(0)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.8395])\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "optimizer = Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "# 输出一下policy net, 看看动作选的对不对\n",
    "for i in range(env.goal):\n",
    "    action, logp= agent.get_action(state=torch.tensor(data=[i]))\n",
    "    print(action, logp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "725c657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 47.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train_one_epoch(env=env, agent=agent, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "e806ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.0789])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.1760])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "LEFT tensor([-1.5047])\n",
      "tensor(1)\n",
      "Categorical(logits: torch.Size([1, 2]))\n",
      "RIGHT tensor([-0.4848])\n"
     ]
    }
   ],
   "source": [
    "# 输出一下policy net, 看看动作选的对不对\n",
    "for i in range(env.goal):\n",
    "    action, logp= agent.get_action(state=torch.tensor(data=[i]))\n",
    "    print(action, logp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
