{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "c6502a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/2/6\n",
    "# zhangzhong\n",
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3afcaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from torch.distributions import Categorical\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    # TODO: discrete state mapping to embedding\n",
    "    def __init__(self, goal: int = 4):\n",
    "        self.goal: int = goal\n",
    "        self.state: int = 0\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "    def step(self, action: str) -> tuple[float, int]:\n",
    "        if action == \"LEFT\":\n",
    "            self.state = max(0, self.state-1)\n",
    "        elif action == \"RIGHT\":\n",
    "            self.state = min(self.goal, self.state+1)\n",
    "        return (10 if self.is_done() else -1, self.state)\n",
    "    \n",
    "    \n",
    "    def curr_state(self) -> Tensor:\n",
    "        return torch.tensor(data=[self.state])\n",
    "\n",
    "    def is_done(self) -> bool:\n",
    "        return self.state == self.goal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8f15cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env: Environment) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # BUG: out of index\n",
    "        # self.state_embedding = nn.Embedding(num_embeddings=env.goal, embedding_dim=8)\n",
    "        self.state_embedding = nn.Embedding(num_embeddings=env.goal + 1, embedding_dim=8)\n",
    "        self.env = env\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(in_features=8, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=2)\n",
    "        )\n",
    "\n",
    "    # state\n",
    "    # https://docs.pytorch.org/docs/stable/distributions.html#categorical\n",
    "    def sample_action(self, state: Tensor) -> tuple[str, Tensor]:\n",
    "\n",
    "        # 我们在sample的时候就能计算出log_probs来，要不也顺便返回一下吧\n",
    "\n",
    "        embeddings: Tensor = self.state_embedding(state)\n",
    "        logits = self.policy_net(embeddings)\n",
    "        probs = Categorical(logits=logits)\n",
    "        actions = probs.sample()\n",
    "        # print(actions)\n",
    "        return self.env.actions[actions.item()], probs.log_prob(value=actions)\n",
    "        # log_probs = probs.log_prob(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "0ebd1213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RIGHT', tensor([-0.5402], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "agent.sample_action(state=torch.tensor(data=[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e318dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample episode\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def sample_episode(env: Environment, agent: Agent) -> list:\n",
    "    env.reset()\n",
    "\n",
    "    # episode 要收集什么？ (st: tensor, at: str, rt: float)\n",
    "    # ()\n",
    "    episode = []\n",
    "    curr_state: Tensor = env.curr_state()\n",
    "\n",
    "    for t in range(100):\n",
    "        # initial state = ?\n",
    "\n",
    "\n",
    "        if env.is_done():\n",
    "            break\n",
    "        # \n",
    "        action, log_prob = agent.sample_action(state=curr_state)\n",
    "        # \n",
    "        reward, next_state = env.step(action=action)\n",
    "        \n",
    "        episode.append((curr_state, action, reward, log_prob))\n",
    "        curr_state = torch.tensor(data=[next_state])\n",
    "\n",
    "        # BUG: 如果env已经done了，上面还会采样状态，就是不对的\n",
    "        # 应该在最开始就判断env是不是已经结束\n",
    "        # if env.is_done():\n",
    "        #     break\n",
    "\n",
    "\n",
    "    return episode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b984b173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([0]), 'RIGHT', -1, tensor([-0.5402], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', -1, tensor([-0.6645], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', -1, tensor([-0.7219], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'LEFT', -1, tensor([-0.7341], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'LEFT', -1, tensor([-0.6652], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.7227], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'LEFT', -1, tensor([-0.8738], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', -1, tensor([-0.5402], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.7227], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', -1, tensor([-0.5402], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'LEFT', -1, tensor([-0.7227], grad_fn=<SqueezeBackward1>)), (tensor([0]), 'RIGHT', -1, tensor([-0.5402], grad_fn=<SqueezeBackward1>)), (tensor([1]), 'RIGHT', -1, tensor([-0.6645], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', -1, tensor([-0.7219], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'LEFT', -1, tensor([-0.7341], grad_fn=<SqueezeBackward1>)), (tensor([2]), 'RIGHT', -1, tensor([-0.7219], grad_fn=<SqueezeBackward1>)), (tensor([3]), 'RIGHT', 10, tensor([-0.6538], grad_fn=<SqueezeBackward1>))]\n"
     ]
    }
   ],
   "source": [
    "# collect on episode\n",
    "\n",
    "episode = sample_episode(env, agent)\n",
    "print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e2544931",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate rewards-to-go\n",
    "\n",
    "def calculate_rewards_to_go(rewards: list[float], gamma: float = 1.0):\n",
    "    reversed_rewards = reversed(rewards)\n",
    "    rewards_to_go = []\n",
    "    curr_reward = 0\n",
    "    for reward in reversed_rewards:\n",
    "        curr_reward = reward + gamma*curr_reward\n",
    "        rewards_to_go.append(curr_reward)\n",
    "    return list(reversed(rewards_to_go))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cfefc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 2.0, 1.0]\n",
      "[2.71, 1.9, 1.0]\n"
     ]
    }
   ],
   "source": [
    "## test rewards to go\n",
    "rewards = [1.0, 1.0, 1.0]\n",
    "print(calculate_rewards_to_go(rewards, gamma=1.0))\n",
    "print(calculate_rewards_to_go(rewards, gamma=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d9b7496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with rewards sampled from a episode\n",
    "\n",
    "def make_rewards_to_go(episode: list, gamma: float) -> list:\n",
    "    rewards: list[float] = [reward for _, _, reward, _ in episode]\n",
    "    rewards_to_go = calculate_rewards_to_go(rewards=rewards, gamma=gamma)\n",
    "    return rewards_to_go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "154b277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect one batch of data, how?\n",
    "# 这里有两种实现的方法\n",
    "# 一种是严格的按照公式来计算，每个episode都计算完成之后，然后再计算N个episode\n",
    "# 还有一种就是每个step看成是独立的，就收集batch size个step，然后就计算就行\n",
    "\n",
    "# 从整体实现上来将，肯定是第二种简单，就按照这种来吧，从数学上是一样的\n",
    "# 咱们先不写reward to go了\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "# 还有一个问题，episode的每个step对应的reward应该是rewards to go\n",
    "# 可以写一个函数来计算一下\n",
    "\n",
    "def collect_training_data():\n",
    "    batch_log_probs = []\n",
    "    batch_rewards_to_go = []\n",
    "    while True:\n",
    "        episode = sample_episode(env=env, agent=agent)\n",
    "        rewards_to_go = make_rewards_to_go(episode=episode, gamma=gamma)\n",
    "        for (curr_state, curr_action, _, log_prob), reward in zip(episode, rewards_to_go):\n",
    "            # print(curr_state, curr_action, log_prob, reward)\n",
    "            \n",
    "\n",
    "            batch_log_probs.append(log_prob)\n",
    "            batch_rewards_to_go.append(reward)\n",
    "            if (len(batch_log_probs) >= batch_size):\n",
    "                break\n",
    "\n",
    "        if (len(batch_log_probs) >= batch_size):\n",
    "            break\n",
    "\n",
    "    return batch_log_probs, batch_rewards_to_go\n",
    "\n",
    "# 我们的一个batch的数据需要什么？\n",
    "# optimizer.zero_grad()\n",
    "# compute_loss -> loss\n",
    "# loss.backward()\n",
    "# optimizer.step()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "db71d7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.6652], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.7341], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.6645], grad_fn=<SqueezeBackward1>), tensor([-0.7219], grad_fn=<SqueezeBackward1>), tensor([-0.6538], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.8738], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>), tensor([-0.7227], grad_fn=<SqueezeBackward1>), tensor([-0.5402], grad_fn=<SqueezeBackward1>)] [-12.702428799197893, -11.820635150704943, -10.92993449566156, -10.030236864304605, -9.12145137808546, -8.203486240490363, -7.276248727768044, -6.33964517956368, -5.393580989458263, -4.437960595412386, -3.4726874701135215, -2.497664111225779, -1.5127920315411911, -0.5179717490315061, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, 6.73289, 7.811, 8.9, 10.0, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, -6.33964517956368, -5.393580989458263, -4.437960595412386, -3.4726874701135215, -2.497664111225779, -1.5127920315411911, -0.5179717490315061, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, -9.12145137808546, -8.203486240490363, -7.276248727768044, -6.33964517956368, -5.393580989458263, -4.437960595412386, -3.4726874701135215, -2.497664111225779, -1.5127920315411911, -0.5179717490315061, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, -8.203486240490363, -7.276248727768044, -6.33964517956368, -5.393580989458263, -4.437960595412386, -3.4726874701135215, -2.497664111225779, -1.5127920315411911, -0.5179717490315061, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, 4.6089054890000005, 5.665561100000001, 6.73289, 7.811, 8.9, 10.0, -1.5127920315411911, -0.5179717490315061, 0.4868972232004989, 1.5019163870712111, 2.5271882697689003, 3.56281643411, 4.6089054890000005, 5.665561100000001, 6.73289]\n"
     ]
    }
   ],
   "source": [
    "batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "print(batch_log_probs, batch_rewards_to_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a45c364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(batch_log_probs: list[Tensor], batch_rewards_to_go: list[Tensor]) -> Tensor:\n",
    "    grads = [log_probs*rewards for log_probs, rewards in zip(batch_log_probs, batch_rewards_to_go)]\n",
    "\n",
    "    # BUG!!! 这一步是构建一个新的tensor，然后把旧的值给复制过来，所以不会保留计算图，requireds_grad=False\n",
    "    # grads_tensor = torch.tensor(grads)\n",
    "    grads_tensor = torch.stack(grads)\n",
    "    assert grads_tensor.shape == (batch_size, 1)\n",
    "    loss = -grads_tensor.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "63768e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(env: Environment, agent: Agent, optimizer):\n",
    "    # 1. collect training data of this epoch\n",
    "    batch_log_probs, batch_rewards_to_go = collect_training_data()\n",
    "\n",
    "    # 2. zero policy net grads\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 3. compute loss\n",
    "    # 大问题！我这样计算出来的loss是没有梯度的！为什么？\n",
    "    loss = compute_loss(batch_log_probs, batch_rewards_to_go)\n",
    "\n",
    "    # 4. backward propogation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. optimize policy net\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e032e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 38.63it/s]\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "env = Environment()\n",
    "agent = Agent(env=env)\n",
    "optimizer = Adam(agent.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train_one_epoch(env=env, agent=agent, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e806ba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT tensor([-0.1109], grad_fn=<SqueezeBackward1>)\n",
      "RIGHT tensor([-0.2101], grad_fn=<SqueezeBackward1>)\n",
      "RIGHT tensor([-0.0176], grad_fn=<SqueezeBackward1>)\n",
      "RIGHT tensor([-0.1070], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# 输出一下policy net, 看看动作选的对不对\n",
    "for i in range(env.goal):\n",
    "    action, logp= agent.sample_action(state=torch.tensor(data=[i]))\n",
    "    print(action, logp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
