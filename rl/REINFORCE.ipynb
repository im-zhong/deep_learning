{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08618442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory: [('LEFT', 0), ('RIGHT', 1), ('LEFT', 0)]\n",
      "Returns G_t: [0.9, 1.0, 0.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo gradient estimate: 0.04999999999999999\n",
      "Updated theta: 0.004999999999999999\n",
      "New π(RIGHT): 0.5012499973958399\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# -----------------------------\n",
    "# Policy: πθ(RIGHT) = sigmoid(θ)\n",
    "# -----------------------------\n",
    "theta = 0.0          # policy parameter\n",
    "alpha = 0.1          # learning rate\n",
    "gamma = 0.9          # discount factor\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sample_action(theta):\n",
    "    \"\"\"Sample action from πθ\"\"\"\n",
    "    p_right = sigmoid(theta)\n",
    "    return \"RIGHT\" if random.random() < p_right else \"LEFT\"\n",
    "\n",
    "def reward_function(action):\n",
    "    \"\"\"Toy reward: RIGHT good, LEFT bad\"\"\"\n",
    "    return 1 if action == \"RIGHT\" else 0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step A: Sample one trajectory\n",
    "# -----------------------------\n",
    "trajectory = []\n",
    "T = 3   # episode length\n",
    "\n",
    "for t in range(T):\n",
    "    a_t = sample_action(theta)\n",
    "    r_t1 = reward_function(a_t)\n",
    "\n",
    "    trajectory.append((a_t, r_t1))\n",
    "\n",
    "print(\"Trajectory:\", trajectory)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step B: Compute returns G_t\n",
    "# -----------------------------\n",
    "rewards = [r for (_, r) in trajectory]\n",
    "\n",
    "returns = []\n",
    "G = 0\n",
    "for r in reversed(rewards):\n",
    "    G = r + gamma * G\n",
    "    returns.insert(0, G)\n",
    "\n",
    "print(\"Returns G_t:\", returns)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step C: Compute policy gradient\n",
    "# g = Σ ∇ log πθ(a_t) * G_t\n",
    "# -----------------------------\n",
    "grad = 0\n",
    "p_right = sigmoid(theta)\n",
    "\n",
    "for t, ((a_t, r_t1), G_t) in enumerate(zip(trajectory, returns)):\n",
    "\n",
    "    if a_t == \"RIGHT\":\n",
    "        # ∇θ log πθ(RIGHT) = 1 - πθ(RIGHT)\n",
    "        grad_logp = 1 - p_right\n",
    "    else:\n",
    "        # ∇θ log πθ(LEFT) = -πθ(RIGHT)\n",
    "        grad_logp = -p_right\n",
    "\n",
    "    grad += grad_logp * G_t\n",
    "\n",
    "print(\"Monte Carlo gradient estimate:\", grad)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step D: Update policy parameter\n",
    "# θ ← θ + α * grad\n",
    "# -----------------------------\n",
    "theta = theta + alpha * grad\n",
    "\n",
    "print(\"Updated theta:\", theta)\n",
    "print(\"New π(RIGHT):\", sigmoid(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75164b",
   "metadata": {},
   "source": [
    "好的。下面我给你一份完整、严谨、标准符号的 REINFORCE（Monte Carlo Policy Gradient） 推导与算法总结。我会把每一个数学对象的含义、梯度估计、蒙特卡洛平均、以及最终更新式全部写清楚，并给出权威来源（Williams 1992 + Sutton & Barto + OpenAI Spinning Up）。\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ REINFORCE：完整严谨版本（Monte Carlo Policy Gradient）\n",
    "\n",
    "⸻\n",
    "\n",
    "1. 强化学习 setting（MDP）\n",
    "\n",
    "我们考虑一个马尔可夫决策过程：\n",
    "\n",
    "(\\mathcal S,\\mathcal A,P,R,\\gamma)\n",
    "\t•\t状态：S_t\\in\\mathcal S\n",
    "\t•\t动作：A_t\\in\\mathcal A\n",
    "\t•\t转移：P(S_{t+1}\\mid S_t,A_t)\n",
    "\t•\t奖励：R_{t+1}\n",
    "\t•\t折扣因子：\\gamma\\in(0,1)\n",
    "\n",
    "⸻\n",
    "\n",
    "2. 参数化随机策略（stochastic policy）\n",
    "\n",
    "REINFORCE 直接学习策略：\n",
    "\n",
    "\\pi_\\theta(a\\mid s)\n",
    "\n",
    "其中 \\theta 是策略网络参数（policy parameters）。\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Trajectory（轨迹）\n",
    "\n",
    "策略与环境交互产生一条 episode：\n",
    "\n",
    "\\tau=(S_0,A_0,R_1,S_1,A_1,R_2,\\dots,S_T)\n",
    "\n",
    "轨迹的分布依赖于策略：\n",
    "\n",
    "\\tau\\sim\\pi_\\theta\n",
    "\n",
    "OpenAI Spinning Up 明确用 trajectories 来定义 policy optimization。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Return（折扣累计回报）\n",
    "\n",
    "从时间步 t 开始的 return 定义为：\n",
    "\n",
    "G_t=\\sum_{k=0}^{T-t-1}\\gamma^k R_{t+1+k}\n",
    "\n",
    "这就是 Sutton & Barto 的标准 return 定义。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Policy Objective（策略优化目标）\n",
    "\n",
    "我们希望最大化期望回报：\n",
    "\n",
    "J(\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}[G_0]\n",
    "\n",
    "目标：\n",
    "\n",
    "\\theta^\\*=\\arg\\max_\\theta J(\\theta)\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 6. Policy Gradient Theorem（关键梯度形式）\n",
    "\n",
    "核心结果：\n",
    "\n",
    "\\nabla_\\theta J(\\theta)\n",
    "=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\n",
    "\\Bigg[\n",
    "\\sum_{t=0}^{T-1}\n",
    "\\nabla_\\theta \\log \\pi_\\theta(A_t\\mid S_t)\\;G_t\n",
    "\\Bigg]\n",
    "\n",
    "这个形式的意义是：\n",
    "\t•\t我们不需要对环境动力学求导\n",
    "\t•\t只需要对策略概率的 log 求导\n",
    "\n",
    "OpenAI Spinning Up 给出同样的 Vanilla Policy Gradient 形式。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 7. Log-Derivative Trick（数学恒等式）\n",
    "\n",
    "推导中用到：\n",
    "\n",
    "\\nabla_\\theta \\pi_\\theta(a\\mid s)\n",
    "=\\pi_\\theta(a\\mid s)\\nabla_\\theta\\log\\pi_\\theta(a\\mid s)\n",
    "\n",
    "这是 score-function / likelihood-ratio trick 的核心。\n",
    "\n",
    "Williams 1992 就是基于这一思想提出 REINFORCE。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 8. Monte Carlo Estimator（采样估计）\n",
    "\n",
    "期望无法解析计算，因此用 N 条轨迹采样：\n",
    "\n",
    "\\tau_1,\\dots,\\tau_N\\sim\\pi_\\theta\n",
    "\n",
    "梯度的 Monte Carlo 估计为：\n",
    "\n",
    "\\widehat{\\nabla_\\theta J(\\theta)}\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{i=1}^N\n",
    "\\sum_{t=0}^{T_i-1}\n",
    "\\nabla_\\theta\\log\\pi_\\theta(A_{t,i}\\mid S_{t,i})\\;G_{t,i}\n",
    "\n",
    "这里：\n",
    "\t•\t\\frac{1}{N} 就是 Monte Carlo 平均\n",
    "\t•\t单条轨迹时 N=1，因此很多教材省略\n",
    "\n",
    "OpenAI Spinning Up 明确写出“collect trajectories then form a sample estimate”。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 9. REINFORCE Parameter Update（最终更新）\n",
    "\n",
    "REINFORCE 使用 stochastic gradient ascent：\n",
    "\n",
    "\\theta \\leftarrow \\theta+\\alpha\\widehat{\\nabla_\\theta J(\\theta)}\n",
    "\n",
    "即：\n",
    "\n",
    "\\theta \\leftarrow \\theta\n",
    "+\\alpha\n",
    "\\sum_{t=0}^{T-1}\n",
    "\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\;G_t\n",
    "\n",
    "这就是 REINFORCE Algorithm。\n",
    "\n",
    "原始论文：Williams 1992 明确称其为 REINFORCE algorithms。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 10. REINFORCE Algorithm（伪代码）\n",
    "\n",
    "⸻\n",
    "\n",
    "Algorithm: REINFORCE (Monte Carlo Policy Gradient)\n",
    "\n",
    "Repeat for each episode:\n",
    "\t1.\tGenerate one trajectory:\n",
    "\n",
    "\\tau=(S_0,A_0,R_1,\\dots,S_T)\n",
    "\t2.\tCompute returns for all time steps:\n",
    "\n",
    "G_t=\\sum_{k\\ge 0}\\gamma^kR_{t+1+k}\n",
    "\t3.\tUpdate policy parameters:\n",
    "\n",
    "\\theta\\leftarrow\\theta+\\alpha\n",
    "\\sum_{t=0}^{T-1}\n",
    "\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\;G_t\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 11. 最重要的直觉总结\n",
    "\n",
    "REINFORCE 更新可以读成一句话：\n",
    "\n",
    "如果某一步动作之后获得的回报 G_t 很大，\n",
    "就增加该动作在该状态下的概率。\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 12. REINFORCE 的主要特点\n",
    "\n",
    "特点\t说明\n",
    "On-policy\t必须用当前策略采样 trajectory\n",
    "Monte Carlo\t依赖完整 episode return\n",
    "无偏估计\t梯度估计是 unbiased\n",
    "方差很大\t实践中必须用 baseline/advantage 改进\n",
    "\n",
    "OpenAI 也强调 VPG/REINFORCE 的方差问题，因此实践用 advantage。\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 13. 三个最权威引用\n",
    "\t1.\t原始 REINFORCE 论文（必引用）\n",
    "\n",
    "Williams, 1992\n",
    "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning\n",
    "￼\n",
    "\t2.\tRL 教科书标准定义\n",
    "\n",
    "Sutton & Barto, Reinforcement Learning: An Introduction (2nd Ed.)\n",
    "￼\n",
    "\t3.\t现代实现与工程视角\n",
    "\n",
    "OpenAI Spinning Up: Vanilla Policy Gradient / REINFORCE\n",
    "￼\n",
    "\n",
    "⸻\n",
    "\n",
    "✅ 下一步（通向 PPO）\n",
    "\n",
    "你现在拿到了完整严谨的 REINFORCE。\n",
    "\n",
    "接下来现代算法（Actor-Critic, PPO）只做两件事：\n",
    "\t1.\t用 baseline 降方差：\n",
    "\n",
    "A_t = G_t - V(S_t)\n",
    "\t2.\t用 clipping 稳定更新（PPO）\n",
    "\n",
    "如果你愿意，我下一步可以同样严谨地给出：\n",
    "\n",
    "✅ Advantage REINFORCE → Actor-Critic → PPO 的完整数学链条。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
