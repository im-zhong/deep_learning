{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b0dccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/2/2\n",
    "# zhangzhong\n",
    "# my q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10b9f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先就是环境和目标\n",
    "# 我们得有一个环境，而且有一个目标，我们才能为了这个在这个环境里面达成这个目标去训练一个agent\n",
    "# 这个环境会被建模为一个状态机\n",
    "# 这里我们假设一个一维的数轴，我们最开始在原点，我们的目标是移动到x（如5）的位置，一次移动一格\n",
    "# 那么环境就会有状态，这里可以用一个整数来表示 state = 0, 1, 2 ...\n",
    "# 同时，在不同的状态下，可以采取相应的动作，会导致状态的转变，同时获得一定的reward\n",
    "# 因为目前的环境相对比较简单，这里的reward是静态的，和环境绑定的，所以直接在step函数里面返回reward就行了\n",
    "from typing import Literal\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, goal: int = 5) -> None:\n",
    "        self.state: int = 0\n",
    "        self.goal: int = goal\n",
    "\n",
    "    def init(self) -> None:\n",
    "        self.state = 0\n",
    "\n",
    "    def act(self, action: Literal[\"LEFT\",\"RIGHT\"]) -> float:\n",
    "        if action == \"LEFT\":\n",
    "            self.state = max(0, self.state - 1)\n",
    "        elif action == \"RIGHT\":\n",
    "            self.state = min(self.goal, self.state + 1)\n",
    "\n",
    "        if self.state  == self.goal:\n",
    "            return 10\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def hit_goal(self) -> bool:\n",
    "        return self.state == self.goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54b0dd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 0\n",
      "10 5\n"
     ]
    }
   ],
   "source": [
    "goal = 5\n",
    "myenv = Environment(goal=goal)\n",
    "\n",
    "r = myenv.act(\"LEFT\")\n",
    "print(r, myenv.state)\n",
    "\n",
    "myenv.state = 4\n",
    "r = myenv.act(\"RIGHT\")\n",
    "print(r, myenv.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90ae020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来就是定义一个Agent，这个Agent可以操作环境，并拿到环境给出的反馈（reward）\n",
    "# 他可以选择某种策略，来决定自己在某种状态的下一步动作\n",
    "# 这里可以先实现epsilon-greedy策略，\n",
    "\n",
    "import random\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env: Environment, Q: dict[int, dict[str, float]] = {}, epsilon: float = 1.0) -> None:\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "\n",
    "    def step(self) -> str:\n",
    "        # choice: Literal[\"LEFT\",\"RIGHT\"] = random.choice([\"LEFT\",\"RIGHT\"])\n",
    "        # self.env.act(choice)\n",
    "        state: int = self.env.state\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice([\"LEFT\",\"RIGHT\"])\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "\n",
    "    \n",
    "    def hit_goal(self) -> bool:\n",
    "        return self.env.state == self.env.goal\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab18bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step: 0, current state: 0\n",
      "current step: 1, current state: 0\n",
      "current step: 2, current state: 0\n",
      "current step: 3, current state: 0\n",
      "current step: 4, current state: 0\n",
      "current step: 5, current state: 1\n",
      "current step: 6, current state: 0\n",
      "current step: 7, current state: 1\n",
      "current step: 8, current state: 2\n",
      "current step: 9, current state: 3\n",
      "current step: 10, current state: 4\n",
      "current step: 11, current state: 5\n"
     ]
    }
   ],
   "source": [
    "myenv.init()\n",
    "myagent = Agent(env=myenv)\n",
    "\n",
    "for s in range(100):\n",
    "    action = myagent.step()\n",
    "    myenv.act(action)\n",
    "    print(f\"current step: {s}, current state: {myenv.state}\")\n",
    "    if myagent.hit_goal():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可以看到，即便是随机的策略，我们也能在比较少的步数内完成这个任务，但是有的时候，步数会多至数十步\n",
    "# 现在引入 Q-Learning\n",
    "\n",
    "# Q-learning的目的就是学一个Q函数，他可以给出在特定状态下，执行某个动作的期望reward（相对），那么我们就可以根据\n",
    "# 某个动作的期望reward，来决定选择那个动作\n",
    "\n",
    "Q:dict[int, dict[Literal[\"LEFT\",\"RIGHT\"], float]] = {}\n",
    "\n",
    "# Q = Q + lr*(Target - Q)\n",
    "# Target = reward + df*V\n",
    "\n",
    "# Value(s): 在某个状态下，可能获得的最大的期望reward\n",
    "# 在Q-Learning算法里面，Values(s) = max{a}( Q(s, a) ) = max{Q(1, LEFT), Q(1, RIGHT)}\n",
    "\n",
    "## step 1: initialize Q table\n",
    "for i in range(goal + 1):\n",
    "    Q[i] = {\"LEFT\": 0, \"RIGHT\": 0}\n",
    "\n",
    "def Value(Q: dict, state: int) -> float:\n",
    "    return max(Q[state].values())\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "\n",
    "def train_iter(env: Environment, agent: Agent):\n",
    "    env.init()\n",
    "\n",
    "    while not env.hit_goal(): # 或者每个iter都应该设置一个最大步数？\n",
    "\n",
    "        old_state = env.state\n",
    "\n",
    "        action = agent.step()\n",
    "        reward = env.act(action=action)\n",
    "\n",
    "        new_state = env.state\n",
    "\n",
    "        # question: 怎么处理state == goal的情况？\n",
    "        value = Value(Q, new_state)\n",
    "        target = reward + discount_factor*value\n",
    "\n",
    "        TD = target - Q[old_state][action]\n",
    "        Q[old_state][action] += learning_rate*TD\n",
    "\n",
    "        if env.hit_goal():\n",
    "            break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "427cb18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 59459.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_epochs = 100\n",
    "myenv = Environment()\n",
    "myagent = Agent(env=myenv, Q=Q, epsilon=0.2)\n",
    "for epoch in tqdm(range(max_epochs)):\n",
    "    train_iter(myenv, agent=myagent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4390c3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'LEFT': 1.3121877582039159, 'RIGHT': 5.547534122599602}, 1: {'LEFT': 1.324880720475138, 'RIGHT': 6.693199985677159}, 2: {'LEFT': 2.1869727898967604, 'RIGHT': 7.800410397656253}, 3: {'LEFT': 4.490920320800705, 'RIGHT': 8.897459175890468}, 4: {'LEFT': 3.0416152490515262, 'RIGHT': 9.999734386011124}, 5: {'LEFT': 0, 'RIGHT': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39223f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current step: 0, current state: 1\n",
      "current step: 1, current state: 2\n",
      "current step: 2, current state: 3\n",
      "current step: 3, current state: 4\n",
      "current step: 4, current state: 5\n"
     ]
    }
   ],
   "source": [
    "# 然后我们用现在的Q来在环境中执行\n",
    "# ok！可以看到执行的结果非常的稳定，一般都是10步以内就可以到达goal\n",
    "myenv.init()\n",
    "\n",
    "for s in range(100):\n",
    "    action = myagent.step()\n",
    "    myenv.act(action)\n",
    "    print(f\"current step: {s}, current state: {myenv.state}\")\n",
    "    if myagent.hit_goal():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# congratulations！我们写的代码基本上是正确的\n",
    "# TODO：·现在还是有几个问题需要深入的研究\n",
    "# 1. state == goal的情况如何处理，对照参考实现，看看我们的事情有哪些地方做的不好？\n",
    "# 2. 深入的剖析训练的过程，分析一下Q是如何变化的？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
