{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "643fcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/2/6\n",
    "# zhangzhong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "da190760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the env\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, goal: int = 4):\n",
    "        self.goal: int = goal\n",
    "        self.state: int = 0\n",
    "        self.actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "\n",
    "    def step(self, action: str) -> tuple[float, int]:\n",
    "        if action == \"LEFT\":\n",
    "            self.state = max(0, self.state-1)\n",
    "        elif action == \"RIGHT\":\n",
    "            self.state = min(self.goal, self.state+1)\n",
    "        return (10 if self.is_done() else -1, self.state)\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.state == self.goal\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "948639f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_env = Environment()\n",
    "test_env.reset()\n",
    "print(test_env.state)\n",
    "test_env.step(\"LEFT\")\n",
    "assert test_env.state == 0\n",
    "\n",
    "test_env.step(\"RIGHT\")\n",
    "assert test_env.state == 1\n",
    "\n",
    "test_env.step(\"RIGHT\")\n",
    "test_env.step(\"RIGHT\")\n",
    "test_env.step(\"RIGHT\")\n",
    "test_env.step(\"RIGHT\")\n",
    "test_env.step(\"RIGHT\")\n",
    "test_env.step(\"RIGHT\")\n",
    "assert test_env.state == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79396af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## agent\n",
    "\n",
    "import random\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env: Environment, epsilon: float = 0.1):\n",
    "        self.Q: dict[int, dict[str, float]] = {}\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        ## init Q\n",
    "        self.goal = env.goal\n",
    "        self.actions = env.actions\n",
    "\n",
    "        ##!!! BUG, should be range(env.goal + 1) instead of range(env.goal)\n",
    "        # cause Q[next_state] may access env.goal+1\n",
    "        for i in range(env.goal + 1):\n",
    "            self.Q[i] = {action: 0 for action in env.actions}\n",
    "        \n",
    "    def sample_action(self, state: int) -> str:\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.env.actions)\n",
    "        else:\n",
    "            return max(self.Q[state], key=self.Q[state].get)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e3cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's start training\n",
    "\n",
    "goal = 4\n",
    "env = Environment(goal=goal)\n",
    "agent = Agent(env=env)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "reward = 0\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "##BUG this should in each episode\n",
    "# curr_state: int = 0\n",
    "# curr_action: str = agent.sample_action(state=curr_state)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    env.reset()\n",
    "    ##BUG fix, should at here\n",
    "    curr_state: int = 0\n",
    "    curr_action: str = agent.sample_action(state=curr_state)\n",
    "\n",
    "    for t in range(100):\n",
    "        # curr_state = env.state\n",
    "        # # sample action\n",
    "\n",
    "        # curr_action = agent.sample_action(state=curr_state)\n",
    "        if env.is_done():\n",
    "            ##BUG chatgpt told me to update the end state\n",
    "            # but I found, to update it or not, does not affect the policy\n",
    "            # terminal update\n",
    "            TD = reward - agent.Q[curr_state][curr_action]\n",
    "            agent.Q[curr_state][curr_action] += learning_rate * TD\n",
    "            break\n",
    "\n",
    "        # take this action in env\n",
    "        reward, next_state = env.step(action=curr_action)\n",
    "        # we also sample next action\n",
    "        next_action = agent.sample_action(state=next_state)\n",
    "\n",
    "        # go to key point of SARSA\n",
    "        # Q(curr_state, curr_action) = Q(cs, ca) + lr*TD\n",
    "        # TD = reward + df*Q(next_state, next_action) - Q(cs, ca)\n",
    "        TD = reward + discount_factor*agent.Q[next_state][next_action] - agent.Q[curr_state][curr_action]\n",
    "        agent.Q[curr_state][curr_action] =  agent.Q[curr_state][curr_action] + learning_rate*TD\n",
    "\n",
    "        # update state\n",
    "        curr_state = next_state\n",
    "        curr_action = next_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "916cb067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'LEFT': 0.9212066922477286, 'RIGHT': 10.005826889511352}, 1: {'LEFT': 0.8400398308320598, 'RIGHT': 12.206447263500301}, 2: {'LEFT': 2.867200788698947, 'RIGHT': 15.767298766757618}, 3: {'LEFT': 0.7321647886244969, 'RIGHT': 18.829251681612565}, 4: {'LEFT': 9.999635646105794, 'RIGHT': 2.71}}\n"
     ]
    }
   ],
   "source": [
    "print(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ac10961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# simulate in the env\n",
    "env.reset()\n",
    "\n",
    "while not env.is_done():\n",
    "    action = agent.sample_action(env.state)\n",
    "    env.step(action=action)\n",
    "    print(env.state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
