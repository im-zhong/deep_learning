{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da16c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2026/1/30\n",
    "# zhangzhong\n",
    "# Reinforcement Learning with Simple Code\n",
    "# tutorial from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93de06b",
   "metadata": {},
   "source": [
    "\n",
    "# roblem: A Robot in a 1D World\n",
    "\n",
    "Imagine a world like this:\n",
    "[ Start ] --- --- --- --- [ Goal ]\n",
    "   0       1   2   3   4\n",
    "\n",
    "Robot starts at position 0\n",
    "\n",
    "Goal is at position 4\n",
    "\n",
    "Robot can do 2 actions:\n",
    "\t•\tLEFT\n",
    "\t•\tRIGHT\n",
    "\n",
    "Rewards:\n",
    "\t•\t+10 if reach the goal\n",
    "\t•\t-1 for every step (to encourage faster solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29aa14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 — Environment Code\n",
    "# # Now we have an environment where the agent can interact.    \n",
    "\n",
    "class SimpleWorld:\n",
    "    def __init__(self):\n",
    "        self.goal = 4\n",
    "        self.state = 0   # start position\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == \"RIGHT\":\n",
    "            self.state += 1\n",
    "        elif action == \"LEFT\":\n",
    "            self.state -= 1\n",
    "\n",
    "        # keep inside bounds\n",
    "        self.state = max(0, min(self.state, self.goal))\n",
    "\n",
    "        # reward\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 10, True   # done\n",
    "        else:\n",
    "            return self.state, -1, False  # not done\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3e42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: LEFT State: 0 Reward: -1\n",
      "Action: LEFT State: 0 Reward: -1\n",
      "Action: RIGHT State: 1 Reward: -1\n",
      "Action: RIGHT State: 2 Reward: -1\n",
      "Action: RIGHT State: 3 Reward: -1\n",
      "Action: LEFT State: 2 Reward: -1\n",
      "Action: LEFT State: 1 Reward: -1\n",
      "Action: RIGHT State: 2 Reward: -1\n",
      "Action: RIGHT State: 3 Reward: -1\n",
      "Action: RIGHT State: 4 Reward: 10\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "# This agent just moves randomly.\n",
    "# Sometimes it reaches the goal, often not efficiently.\n",
    "\n",
    "import random\n",
    "\n",
    "env = SimpleWorld()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = random.choice([\"LEFT\", \"RIGHT\"])\n",
    "    state, reward, done = env.step(action)\n",
    "\n",
    "    print(\"Action:\", action, \"State:\", state, \"Reward:\", reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae437cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: The Agent Needs Memory (Q-values)\n",
    "# The agent should learn:“Going RIGHT is usually good”\n",
    "# So we store values: Q[state][action] = how good it is\n",
    "\n",
    "# The Q-learning formula\n",
    "# Q(s,a) = Q(s,a) + \\alpha \\Big[r + \\gamma \\max Q(s') - Q(s,a)\\Big]\n",
    "# \\text{New estimate} = \\text{Old estimate} + \\text{Correction}\n",
    "# New estimate = Old estimate + Correction\n",
    "# Example: \n",
    "# - Old estimate: you have some experience\n",
    "# - CorrectionL you encouter some new events\n",
    "# - New estimate: you got new experience\n",
    "\n",
    "# Q(s,a): How good is it to take action a in state s?”\n",
    "# Example:\n",
    "# - State: position in maze\n",
    "# - Action: move right\n",
    "# - Q-value: expected future reward\n",
    "# Q-values measure [future usefulness], not just immediate reward.\n",
    "# Q-value means The expected total future reward if you take action a in state s.\n",
    "# Q-value is just a real nubmer:\n",
    "#   - positive means good action\n",
    "#   - negative means bad action\n",
    "\n",
    "## How do we calculate Q(s,a)?\n",
    "# 1. start with guess or 0s.\n",
    "# 2. then the agent interact with the env and get experience\n",
    "#   Then the agent interacts:\n",
    "#   Example experience:\n",
    "# \t•\tState: 0\n",
    "# \t•\tAction: RIGHT\n",
    "# \t•\tReward: -1\n",
    "# \t•\tNext state: 1\n",
    "#   Then update:\n",
    "#   Q(0,\\text{RIGHT}) = Q(0,\\text{RIGHT}) + \\alpha[\\dots]\n",
    "\n",
    "## Value Function, V(s)\n",
    "# How good is state s, assuming the agent acts optimally?\n",
    "# 在状态s下，假设执行所有可选动作a，能获得的最大的reward\n",
    "# V(s) = \\max_a Q(s,a)\n",
    "# Example:\n",
    "# - Q(s,\\text{LEFT}) = 3\n",
    "# - Q(s,\\text{RIGHT}) = 7\n",
    "# V(s) = \\max(3,7)=7\n",
    "\n",
    "## How do we represent Q-values?\n",
    "# 1. Small world -> Table, Q[s][a]\n",
    "# 2. Large world -> Neural Network (Deep Q Learning), Q(s, a) = Net(s, a)\n",
    "\n",
    "## let's explain the whole formula\n",
    "# so we start with the current(old) experience(estimation)\n",
    "\n",
    "# \\alpha is learning rate: Controls how fast we learn from new events\n",
    "\n",
    "# r: reward, the reward of take action a at state s. This is the agent’s feedback.\n",
    "#   - +10 if goal reached\n",
    "#   - -1 for step cost\n",
    "\n",
    "# \\gamma: discount factor\n",
    "#   Controls importance of [future rewards]. what is future reward?\n",
    "#   \t\\gamma = 0: only care about immediate reward\n",
    "#       \\gamma = 0.99: care a lot about future reward\n",
    "# example: Would you rather get $10 today or $10 next week?\n",
    "\n",
    "\n",
    "## Target！\n",
    "# Target = r+\\gamma\\max_{a'}Q(s',a')\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425baef",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "下面是一份把我们刚才围绕 Q-value / Q-learning 更新公式 的讨论整理成的详细笔记（包含你提出的所有疑问 + 对应回答），你可以直接复制到笔记软件里。\n",
    "\n",
    "⸻\n",
    "\n",
    "强化学习 Q-value / Q-learning 更新公式笔记（含疑问整理）\n",
    "\n",
    "0. 背景：我们在讨论什么？\n",
    "\n",
    "我们讨论的是 Q-learning 中最核心的更新规则（Temporal Difference / TD learning）：\n",
    "\n",
    "Q(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\Big[r_{t+1}+\\gamma\\max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)\\Big]\n",
    "\n",
    "这条规则告诉我们：执行一步交互后，如何更新 “在状态 s 采取动作 a” 的价值估计。\n",
    "\n",
    "⸻\n",
    "\n",
    "1) 疑问：公式里到底哪个是“下一个状态”？我看不出来状态转移\n",
    "\n",
    "你指出：“从一个状态经过一个 action 应该到达另一个状态，但公式里看不出来。”\n",
    "\n",
    "✅ 澄清：一次交互的完整形式\n",
    "\n",
    "一次交互产生四元组：\n",
    "\n",
    "(s_t,\\ a_t,\\ r_{t+1},\\ s_{t+1})\n",
    "\t•\t当前状态：s_t\n",
    "\t•\t当前动作：a_t\n",
    "\t•\t执行动作后得到的奖励：r_{t+1}\n",
    "\t•\t执行动作后到达的下一个状态：s_{t+1}\n",
    "\n",
    "✅ 正确、严格的 Q-learning 更新式\n",
    "\n",
    "你说得对：未来部分必须写出动作维度，否则会歧义：\n",
    "\n",
    "Q(s_t,a_t)\\leftarrow Q(s_t,a_t)\n",
    "+\\alpha\\Big[r_{t+1}+\\gamma\\max_{a'} Q(s_{t+1},a')-Q(s_t,a_t)\\Big]\n",
    "\n",
    "其中 \\max_{a'} Q(s_{t+1},a') 表示：在 下一个状态 s_{t+1} 下，尝试所有动作 a'，选 Q 值最大的那个。\n",
    "\n",
    "之前把它简写成 \\max Q(s') 的确不严谨，会让人看不出来在对哪个变量取 max。\n",
    "\n",
    "⸻\n",
    "\n",
    "2) 疑问：Q(s,a) 怎么计算？Q(s) 又是什么？Q 值是不是实数？\n",
    "\n",
    "✅ Q(s,a) 是什么？\n",
    "\n",
    "Q(s,a)\n",
    "含义：\n",
    "\n",
    "在状态 s 执行动作 a 后，未来能获得的 折扣累计回报 的期望（预测值）。\n",
    "\n",
    "✅ Q 值是不是实数？\n",
    "\n",
    "是的，Q-value 就是一个 实数（real number），可以是正、负或小数，比如 5.6、-3.2、0 等。\n",
    "\n",
    "✅ Q(s) 是什么？\n",
    "\n",
    "一般我们不用 Q(s)（因为 Q 通常需要 action），更标准的是 状态价值函数：\n",
    "\n",
    "V(s)\n",
    "\n",
    "定义：\n",
    "\n",
    "处于状态 s 的价值（假设之后采取最优动作或某个策略）。\n",
    "\n",
    "二者关系（最常见的一种）：\n",
    "\n",
    "V(s)=\\max_a Q(s,a)\n",
    "\n",
    "所以：\n",
    "\t•\tQ(s,a)：状态+动作 的价值\n",
    "\t•\tV(s)：状态本身的价值（取最优动作时）\n",
    "\n",
    "✅ Q(s,a) 到底怎么得到？\n",
    "\n",
    "不是直接算出来的，而是从经验中学出来的：\n",
    "\t•\t初始化：通常设为 0（或随机小值）\n",
    "\t•\t每走一步就用更新公式修正一次\n",
    "\t•\t经过很多次交互，Q 会慢慢接近真实的最优 Q 值\n",
    "\n",
    "⸻\n",
    "\n",
    "3) 疑问：公式里为什么要减去 Q(s,a)？为什么不每次执行动作就把 reward 加到 Q 里？\n",
    "\n",
    "你提出了一个非常核心的疑问：\n",
    "\n",
    "“每次得到 reward（可正可负）就加到 Q-value 里不行吗？为什么要减去旧的 Q(s,a)？”\n",
    "\n",
    "✅ 关键澄清：Q-value 不是“奖励累加器”\n",
    "\n",
    "Q(s,a) 是对 长期总回报 的预测，不是“累计奖励的总和”。\n",
    "\n",
    "如果你每次都做：\n",
    "\n",
    "Q(s,a)\\leftarrow Q(s,a)+r\n",
    "\n",
    "会出现两个问题：\n",
    "\t1.\tQ 会随着次数不断累加，可能越加越大或越小，不收敛\n",
    "\t2.\tQ 无法表达“这个动作长期是否值得”（例如每步 -1 但最终 +10 的路径）\n",
    "\n",
    "✅ 为什么要减去 Q(s,a)？\n",
    "\n",
    "看更新式的括号部分：\n",
    "\n",
    "r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)\n",
    "\n",
    "这是一个误差项，叫 TD error（时间差分误差）：\n",
    "\n",
    "新观察到的“更合理目标” 与 旧预测之间的差距。\n",
    "\n",
    "\t•\tQ(s,a)：旧预测（旧估计）\n",
    "\t•\tr+\\gamma\\max Q(s',a')：基于最新经验的“新目标”(Target)\n",
    "\n",
    "更新本质是：\n",
    "\n",
    "Q \\leftarrow Q + \\alpha(\\text{Target} - Q)\n",
    "\n",
    "这是一种非常通用的学习结构：预测 → 计算误差 → 修正预测。\n",
    "\n",
    "✅ 类比帮助理解\n",
    "\n",
    "你预测考试 80 分（旧 Q），结果你根据新信息觉得应该是 90（Target）：\n",
    "\t•\t你不会 80+90\n",
    "\t•\t你会把 80 朝 90 调整一点点\n",
    "\n",
    "⸻\n",
    "\n",
    "4) 疑问：Target 是不是我们期望的 Q-value？大约等于它应该变成的值？\n",
    "\n",
    "你总结得非常准确：\n",
    "\n",
    "“Target 就是我们期望的 Q-value，或者大约等于它应该变成的值。”\n",
    "\n",
    "✅ 更精确的表达\n",
    "\n",
    "Target 是对真实最优 Q 值 Q^\\*(s,a) 的一个 更好的估计：\n",
    "\n",
    "\\text{Target} = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a')\n",
    "\n",
    "并且学习目标是逐渐让：\n",
    "\n",
    "Q(s_t,a_t) \\approx \\text{Target}\n",
    "\n",
    "每次更新只迈一小步（乘上 \\alpha）来避免震荡、保证稳定。\n",
    "\n",
    "⸻\n",
    "\n",
    "5) 疑问：这是不是很像动态规划（DP）？\n",
    "\n",
    "你的直觉完全正确：\n",
    "\n",
    "Q-learning 非常像动态规划。\n",
    "\n",
    "✅ 为什么像？\n",
    "\n",
    "它们都基于 Bellman 递推思想：\n",
    "\n",
    "当前价值 = 当前收益 + 未来价值\n",
    "\n",
    "Q-learning 的 Target 形式其实就是 Bellman optimality 的采样版本。\n",
    "\n",
    "✅ RL 和 DP 的关键区别\n",
    "\t•\tDP（动态规划）：通常需要已知环境模型（转移概率 P(s'|s,a)），可以对所有可能 s' 做期望计算\n",
    "\t•\tRL（Q-learning）：不知道模型，只能通过采样经验 (s,a,r,s') 来近似更新\n",
    "\n",
    "一句话：\n",
    "\n",
    "RL = 动态规划思想 + 采样（model-free）学习\n",
    "\n",
    "⸻\n",
    "\n",
    "总结：我们讨论得到的核心认知\n",
    "\t1.\tQ(s,a) 是一个实数：表示从 s 做 a 的长期回报预测\n",
    "\t2.\t一步交互是：(s_t,a_t,r_{t+1},s_{t+1})\n",
    "\t3.\t更新式必须写清楚：\\max_{a'}Q(s_{t+1},a')，否则会歧义\n",
    "\t4.\t减去 Q(s,a) 是为了计算误差（TD error），让估计逐渐收敛\n",
    "\t5.\tTarget 是 “当前奖励 + 下一个状态的最优未来价值”，是对真实 Q 的更好估计\n",
    "\t6.\tQ-learning 的结构非常像 DP：都基于 Bellman 递推，但 RL 用采样不需要已知模型\n",
    "\n",
    "⸻\n",
    "\n",
    "如果你愿意，我还可以把这份笔记再补一个小节：\n",
    "“Q-learning vs SARSA（为什么一个用 max，一个用实际动作）”，这通常是下一个最自然的疑问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a6ba2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "{0: {'LEFT': 2.697266398169901, 'RIGHT': 4.579997849013276}, 1: {'LEFT': 2.7865865516327895, 'RIGHT': 6.19999957139955}, 2: {'LEFT': 3.7522975146988777, 'RIGHT': 7.999999941377253}, 3: {'LEFT': 5.301321426399191, 'RIGHT': 9.99999999294492}, 4: {'LEFT': 0, 'RIGHT': 0}}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "actions = [\"LEFT\", \"RIGHT\"]\n",
    "\n",
    "# initialize Q-table\n",
    "Q = {}\n",
    "for state in range(5):\n",
    "    Q[state] = {a: 0 for a in actions}\n",
    "\n",
    "# parameters\n",
    "alpha = 0.1      # learning rate\n",
    "gamma = 0.9      # future reward importance\n",
    "epsilon = 0.2    # exploration probability\n",
    "\n",
    "env = SimpleWorld()\n",
    "\n",
    "# training loop\n",
    "for episode in range(200):\n",
    "\n",
    "    env.state = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = env.state\n",
    "\n",
    "        # choose action (explore vs exploit)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            # 这里应该就是选择Q最大的action，不过我没太看明白这个写法\n",
    "            action = max(Q[state], key=Q[state].get)\n",
    "\n",
    "        # take step\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        # Q-learning update, 这个就是V\n",
    "        best_next = max(Q[next_state].values())\n",
    "\n",
    "        Q[state][action] += alpha * (\n",
    "            reward + gamma * best_next - Q[state][action]\n",
    "        )\n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
