{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c2f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/25\n",
    "# zhangzhong\n",
    "# https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53468875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it comes to saving and loading models, there are three core functions to be familiar with:\n",
    "\n",
    "# torch.save: Saves a serialized object to disk. This function uses Pythonâ€™s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "\n",
    "# torch.load: Uses pickleâ€™s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "\n",
    "# torch.nn.Module.load_state_dict: Loads a modelâ€™s parameter dictionary using a deserialized state_dict. For more information on state_dict, see What is a state_dict?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95839408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a state dict\n",
    "# torch.nn.Module, A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor.\n",
    "# Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizerâ€™s state, as well as the hyperparameters used.\n",
    "\n",
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77d4dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModelClass(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TheModelClass()\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8c5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "# print model's state dict\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb93d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Loading Model for Inference\n",
    "# When saving a model for inference, it is only necessary to save the trained modelâ€™s learned parameters. \n",
    "# Save\n",
    "# A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "# model_state = model.state_dict() returns a **reference** to the state and not its copy\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049b83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "# To load a model, you first need to instantiate the model class, then load the state_dict into it.\n",
    "# Note that the model class must match the one used to save the state_dict.\n",
    "# Loadæœ¬è´¨ä¸Šæ˜¯åŠ è½½å‚æ•°ï¼Œæ‰€ä»¥æ¨¡å‹çš„æ¶æ„å¿…é¡»æ˜¯ä¸€è‡´çš„ã€‚\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Entire Model\n",
    "# this way is strongly not recommended\n",
    "# This save/load process uses the most intuitive syntax and involves the least amount of code.\n",
    "# The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. \n",
    "# Because of this, your code can break in various ways when used in other projects or after refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving & Loading a General Checkpoint for Inference and Resuming Training\n",
    "# To save multiple components, organize them in a dictionary and use torch.save() to serialize the dictionary\n",
    "# A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
    "torch.save({\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            # ...\n",
    "            }, 'checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the items, first initialize the model and optimizer, then load the dictionary locally using torch.load()\n",
    "# From here, you can easily access the saved items by simply querying the dictionary as you would expect.\n",
    "model = TheModelClass()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load('checkpoint.tar')\n",
    "# è¿™é‡Œåªæ˜¯æ›´æ–°äº†æ¨¡å‹çš„å‚æ•°\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# è¿™æ ·å†™optimizerä»ç„¶æ­£ç¡®çš„ç»‘å®šåˆ°äº†modelçš„å‚æ•°ä¸Š\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™æ˜¯ä¸€ä¸ªéå¸¸å…³é”®çš„é—®é¢˜ï¼Œæ­£ç¡®é¡ºåºå¾ˆé‡è¦ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# ğŸ¯ ç®€çŸ­å›ç­”ï¼š\n",
    "\n",
    "# å¦‚æœä½ åœ¨åŠ è½½ optimizer state åï¼Œå†è°ƒç”¨ model.to(cuda)ï¼Œ**ä¼˜åŒ–å™¨å†…éƒ¨ç»‘å®šçš„å‚æ•° tensorï¼ˆå¦‚ momentum buffer ç­‰ï¼‰**ä»ç„¶åœ¨ CPU ä¸Šï¼Œè€Œæ¨¡å‹å‚æ•°æ­¤æ—¶åœ¨ CUDA ä¸Šï¼Œè¿™å°†å¯¼è‡´ optimizer çš„è¡Œä¸ºä¸æ­£ç¡®ï¼ˆæ¯”å¦‚æ¢¯åº¦æ›´æ–°å‡ºé”™æˆ–è®­ç»ƒå´©æºƒï¼‰ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# âœ… æ­£ç¡®é¡ºåºåº”å½“æ˜¯ï¼š\n",
    "\n",
    "# model = TheModelClass()\n",
    "# checkpoint = torch.load(\"checkpoint.pt\", map_location=\"cuda\")  # ä¿è¯åŠ è½½åˆ°å¯¹çš„è®¾å¤‡ä¸Š\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # æŠŠæ¨¡å‹æ”¾åˆ° CUDA ä¸Š\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])  # åŠ è½½çš„æ˜¯ CUDA ä¸Šçš„å‚æ•°çŠ¶æ€\n",
    "\n",
    "# \tâ€¢\tåœ¨ optimizer åŠ è½½ state_dict æ—¶ï¼Œå†…éƒ¨ state ä¼šæŒ‰å½“å‰ç»‘å®šçš„ model.parameters() çš„è®¾å¤‡è¿›è¡Œè¿ç§»ï¼ˆåªè¦æ˜¯åŒä¸€ä¸ªå‚æ•°å¯¹è±¡ï¼‰ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# âŒ é”™è¯¯é¡ºåºä¼šå¯¼è‡´ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "# model = TheModelClass()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])  # âš ï¸ æ­¤æ—¶ state åœ¨ CPU ä¸Š\n",
    "\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # æ¨¡å‹åˆ° CUDA ä¸Šäº†ï¼Œä½† optimizer è¿˜åœ¨ç®¡ç† CPU ä¸Šçš„çŠ¶æ€\n",
    "\n",
    "# \tâ€¢\tæ­¤æ—¶ optimizer çš„çŠ¶æ€ï¼ˆå¦‚åŠ¨é‡ï¼‰å’Œæ¨¡å‹å‚æ•°ä¸åŒ¹é…è®¾å¤‡ï¼Œä¼šå¯¼è‡´è®­ç»ƒå‡ºé”™ã€‚\n",
    "# \tâ€¢\tå¸¸è§æŠ¥é”™æœ‰ï¼šRuntimeError: expected scalar type Float but found CUDAFloat ç­‰ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# ğŸ”§ è¡¥æ•‘æ–¹æ³•ï¼ˆå¦‚æœä½ å¿…é¡»è¦ .to() åœ¨åé¢ï¼‰ï¼š\n",
    "\n",
    "# ä½ å¯ä»¥æ‰‹åŠ¨å°† optimizer çš„çŠ¶æ€è¿ç§»åˆ° GPUï¼š\n",
    "\n",
    "# # åœ¨ model.to(\"cuda\") ä¹‹åæ‰‹åŠ¨å¤„ç†\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             state[k] = v.cuda()\n",
    "\n",
    "# ä¸è¿‡è¿™æ›´ç¹çï¼Œå¼ºçƒˆå»ºè®®è¿˜æ˜¯åœ¨ .to(cuda) åå† load optimizerï¼Œè¿™æ ·æœ€è‡ªç„¶ã€æœ€å®‰å…¨ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# âœ… æ¨èé€šç”¨åŠ è½½é¡ºåºï¼š\n",
    "\n",
    "# # 1. Init model\n",
    "# model = TheModelClass()\n",
    "\n",
    "# # 2. Load checkpoint with correct device mapping\n",
    "# checkpoint = torch.load(\"checkpoint.pt\", map_location=\"cuda\")\n",
    "\n",
    "# # 3. Load model state\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # å¿…é¡»åœ¨ optimizer åˆå§‹åŒ–å‰\n",
    "\n",
    "# # 4. Init optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "# å¦‚æœä½ ç”¨ AMP æˆ– GradScalerï¼Œä¹Ÿä¸€æ ·è¦åœ¨æ¨¡å‹è½¬ cuda åå†æ¢å¤ scaler çŠ¶æ€ã€‚\n",
    "\n",
    "# â¸»\n",
    "\n",
    "# å¦‚ä½ å¸Œæœ›æˆ‘æä¾›å®Œæ•´çš„åŠ è½½ä»£ç æ¨¡æ¿ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘ä½ ä½¿ç”¨çš„ AMP / DDP ç­‰ç»„ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fff607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Saving & Loading Model Across Devices\n",
    "# éœ€è¦åœ¨load_state_dictæ—¶æŒ‡å®šmap_locationå‚æ•°\n",
    "\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ­¤æ—¶å‚æ•°å…¨éƒ¨åœ¨cpuä¸ŠÂ·\n",
    "model = TheModelClass()\n",
    "\n",
    "state_dict = torch.load('model_weights.pth', map_location=device)\n",
    "# æ­¤æ—¶å‚æ•°å…¨éƒ¨åœ¨cudaä¸Š\n",
    "print(next(iter(state_dict.values())).device)  # æ‰“å°å‚æ•°æ‰€åœ¨è®¾å¤‡\n",
    "\n",
    "# # è¿™ä¸€æ­¥åªæ˜¯æŠŠåŠ è½½çš„å‚æ•°å†…å®¹â€œå¤åˆ¶â€åˆ° model çš„å‚æ•°ä¸­ï¼Œä½† model çš„ param æœ¬èº«æ²¡å˜ device\n",
    "# æ‰€ä»¥å®é™…ä¸Šæ˜¯æŠŠå‚æ•°ä»cudaå¤åˆ¶åˆ°cpuä¸Š\n",
    "model.load_state_dict(state_dict)\n",
    "# è¿˜çœŸæ˜¯åœ¨cpuä¸Šï¼\n",
    "print(next(iter(model.parameters())).device)\n",
    "\n",
    "# ç„¶ååœ¨å§å‚æ•°ä»cpuå¤åˆ¶åˆ°gpuä¸Š\n",
    "model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š\n",
    "# è¿™é‡Œå°±æ˜¯åœ¨cudaé‡Œé¢äº†\n",
    "print(next(iter(model.parameters())).device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
