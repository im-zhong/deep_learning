{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c2f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/25\n",
    "# zhangzhong\n",
    "# https://docs.pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53468875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it comes to saving and loading models, there are three core functions to be familiar with:\n",
    "\n",
    "# torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "\n",
    "# torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "\n",
    "# torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a deserialized state_dict. For more information on state_dict, see What is a state_dict?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95839408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a state dict\n",
    "# torch.nn.Module, A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor.\n",
    "# Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state, as well as the hyperparameters used.\n",
    "\n",
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77d4dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModelClass(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TheModelClass()\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8c5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "# print model's state dict\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb93d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & Loading Model for Inference\n",
    "# When saving a model for inference, it is only necessary to save the trained model’s learned parameters. \n",
    "# Save\n",
    "# A common PyTorch convention is to save models using either a .pt or .pth file extension.\n",
    "# model_state = model.state_dict() returns a **reference** to the state and not its copy\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049b83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "# To load a model, you first need to instantiate the model class, then load the state_dict into it.\n",
    "# Note that the model class must match the one used to save the state_dict.\n",
    "# Load本质上是加载参数，所以模型的架构必须是一致的。\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load('model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ed20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Entire Model\n",
    "# this way is strongly not recommended\n",
    "# This save/load process uses the most intuitive syntax and involves the least amount of code.\n",
    "# The disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. \n",
    "# Because of this, your code can break in various ways when used in other projects or after refactors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving & Loading a General Checkpoint for Inference and Resuming Training\n",
    "# To save multiple components, organize them in a dictionary and use torch.save() to serialize the dictionary\n",
    "# A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
    "torch.save({\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "            # ...\n",
    "            }, 'checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load the items, first initialize the model and optimizer, then load the dictionary locally using torch.load()\n",
    "# From here, you can easily access the saved items by simply querying the dictionary as you would expect.\n",
    "model = TheModelClass()\n",
    "optimizer= optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load('checkpoint.tar')\n",
    "# 这里只是更新了模型的参数\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# 这样写optimizer仍然正确的绑定到了model的参数上\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个非常关键的问题，正确顺序很重要。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🎯 简短回答：\n",
    "\n",
    "# 如果你在加载 optimizer state 后，再调用 model.to(cuda)，**优化器内部绑定的参数 tensor（如 momentum buffer 等）**仍然在 CPU 上，而模型参数此时在 CUDA 上，这将导致 optimizer 的行为不正确（比如梯度更新出错或训练崩溃）。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 正确顺序应当是：\n",
    "\n",
    "# model = TheModelClass()\n",
    "# checkpoint = torch.load(\"checkpoint.pt\", map_location=\"cuda\")  # 保证加载到对的设备上\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # 把模型放到 CUDA 上\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])  # 加载的是 CUDA 上的参数状态\n",
    "\n",
    "# \t•\t在 optimizer 加载 state_dict 时，内部 state 会按当前绑定的 model.parameters() 的设备进行迁移（只要是同一个参数对象）。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ❌ 错误顺序会导致什么？\n",
    "\n",
    "# model = TheModelClass()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])  # ⚠️ 此时 state 在 CPU 上\n",
    "\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # 模型到 CUDA 上了，但 optimizer 还在管理 CPU 上的状态\n",
    "\n",
    "# \t•\t此时 optimizer 的状态（如动量）和模型参数不匹配设备，会导致训练出错。\n",
    "# \t•\t常见报错有：RuntimeError: expected scalar type Float but found CUDAFloat 等。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🔧 补救方法（如果你必须要 .to() 在后面）：\n",
    "\n",
    "# 你可以手动将 optimizer 的状态迁移到 GPU：\n",
    "\n",
    "# # 在 model.to(\"cuda\") 之后手动处理\n",
    "# for state in optimizer.state.values():\n",
    "#     for k, v in state.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             state[k] = v.cuda()\n",
    "\n",
    "# 不过这更繁琐，强烈建议还是在 .to(cuda) 后再 load optimizer，这样最自然、最安全。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 推荐通用加载顺序：\n",
    "\n",
    "# # 1. Init model\n",
    "# model = TheModelClass()\n",
    "\n",
    "# # 2. Load checkpoint with correct device mapping\n",
    "# checkpoint = torch.load(\"checkpoint.pt\", map_location=\"cuda\")\n",
    "\n",
    "# # 3. Load model state\n",
    "# model.load_state_dict(checkpoint[\"model\"])\n",
    "# model.to(\"cuda\")  # 必须在 optimizer 初始化前\n",
    "\n",
    "# # 4. Init optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "# 如果你用 AMP 或 GradScaler，也一样要在模型转 cuda 后再恢复 scaler 状态。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 如你希望我提供完整的加载代码模板，也可以告诉我你使用的 AMP / DDP 等组件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fff607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Saving & Loading Model Across Devices\n",
    "# 需要在load_state_dict时指定map_location参数\n",
    "\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 此时参数全部在cpu上·\n",
    "model = TheModelClass()\n",
    "\n",
    "state_dict = torch.load('model_weights.pth', map_location=device)\n",
    "# 此时参数全部在cuda上\n",
    "print(next(iter(state_dict.values())).device)  # 打印参数所在设备\n",
    "\n",
    "# # 这一步只是把加载的参数内容“复制”到 model 的参数中，但 model 的 param 本身没变 device\n",
    "# 所以实际上是把参数从cuda复制到cpu上\n",
    "model.load_state_dict(state_dict)\n",
    "# 还真是在cpu上！\n",
    "print(next(iter(model.parameters())).device)\n",
    "\n",
    "# 然后在吧参数从cpu复制到gpu上\n",
    "model.to(device)  # 确保模型在正确的设备上\n",
    "# 这里就是在cuda里面了\n",
    "print(next(iter(model.parameters())).device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
