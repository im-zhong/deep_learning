{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/25\n",
    "# zhangzhong\n",
    "# https://docs.pytorch.org/docs/stable/distributed.html\n",
    "# https://docs.pytorch.org/tutorials/intermediate/dist_tuto.html#collective-communication\n",
    "# https://docs.pytorch.org/docs/stable/distributed.html#torch.distributed.gather_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当然可以！我们用一个最简单的例子来直观理解 torch.distributed.all_gather_object。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧠 一句话理解\n",
    "\n",
    "# all_gather_object 可以让每个进程收集所有其他进程发来的 Python 对象，放在一个列表里。\n",
    "\n",
    "# 这个函数适合于非 Tensor 的 Python 对象，比如 dict、str、list、None、bool，你不需要手动做张量拼接或序列化，它会自动帮你完成。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧪 实验环境准备（假设 2 个进程）\n",
    "\n",
    "# 我们模拟 2 个进程（rank 0 和 rank 1），每个进程有自己的本地对象 local_data，我们想让两个进程都获得对方的内容。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 示例代码：2个进程收集彼此的数据\n",
    "\n",
    "# # dist_example.py\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# import os\n",
    "\n",
    "# def setup(rank, world_size):\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "# def run(rank, world_size):\n",
    "#     setup(rank, world_size)\n",
    "\n",
    "#     # 每个进程有一个本地对象（任何 Python 对象都行）\n",
    "#     local_data = {\"rank\": rank, \"value\": rank * 10}\n",
    "\n",
    "#     # 创建用于收集的空列表\n",
    "#     gathered_data = [None for _ in range(world_size)]\n",
    "\n",
    "#     # 所有进程都调用 all_gather_object，同步收集所有人的 local_data\n",
    "#     dist.all_gather_object(gathered_data, local_data)\n",
    "\n",
    "#     print(f\"[Rank {rank}] gathered_data: {gathered_data}\")\n",
    "\n",
    "#     cleanup()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import multiprocessing as mp\n",
    "\n",
    "#     world_size = 2\n",
    "#     mp.spawn(run, args=(world_size,), nprocs=world_size)\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧾 输出结果：\n",
    "\n",
    "# [Rank 0] gathered_data: [{'rank': 0, 'value': 0}, {'rank': 1, 'value': 10}]\n",
    "# [Rank 1] gathered_data: [{'rank': 0, 'value': 0}, {'rank': 1, 'value': 10}]\n",
    "\n",
    "# 也就是说：\n",
    "# \t•\t每个进程都收到了所有 rank 的对象。\n",
    "# \t•\t数据顺序和 rank 一致，gathered_data[0] 是 rank 0 的数据，gathered_data[1] 是 rank 1 的数据。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧠 原理图解\n",
    "\n",
    "# Rank\tlocal_data\tgathered_data after all_gather_object\n",
    "# 0\t{\"rank\": 0, \"value\": 0}\t[{\"rank\": 0, \"value\": 0}, {\"rank\": 1, \"value\": 10}]\n",
    "# 1\t{\"rank\": 1, \"value\": 10}\t[{\"rank\": 0, \"value\": 0}, {\"rank\": 1, \"value\": 10}]\n",
    "\n",
    "# 底层原理是：PyTorch 会自动将 Python 对象序列化（pickle），广播给所有进程，并写入 gathered_data 对应的列表位置。\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 📌 和 all_gather 的区别？\n",
    "\n",
    "# 函数\t用于对象类型\t例子\n",
    "# all_gather\tTensor\t所有 rank 上收集 tensor\n",
    "# all_gather_object\t任意 Python 对象\tdict、str、list、None 等\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 总结\n",
    "\n",
    "# 你有这个问题\t用这个函数\n",
    "# 想让所有进程收集彼此的非张量数据（如 dataloader.state_dict()）\tall_gather_object\n",
    "# 想收集 tensor 张量\tall_gather\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 如果你想我再举一个例子，比如收集字符串、布尔值、list 或其他对象，我也可以写。你想要哪种？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非常好的问题！\n",
    "\n",
    "# 你说的这个需求——“只让其他进程把自己的数据发给 rank 0”——其实不应该用 all_gather_object，而是用：\n",
    "\n",
    "# ✅ torch.distributed.gather_object()\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 正确用法：gather_object（只收集到 rank 0）\n",
    "\n",
    "# 📌 特性总结：\n",
    "\n",
    "# 特性\t值\n",
    "# 谁收集？\t只有 dst（一般为 rank 0） 会收到所有进程发来的对象\n",
    "# 谁发送？\t所有进程都发送自己的对象（包含 rank 0 自己）\n",
    "# 用途\t只集中收集一次，用于保存 checkpoint 或调试汇总\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧪 最小例子：只把所有数据传给 rank 0\n",
    "\n",
    "# import torch\n",
    "# import torch.distributed as dist\n",
    "# import os\n",
    "\n",
    "# def setup(rank, world_size):\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "#     dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "# def cleanup():\n",
    "#     dist.destroy_process_group()\n",
    "\n",
    "# def run(rank, world_size):\n",
    "#     setup(rank, world_size)\n",
    "\n",
    "#     local_data = {\"rank\": rank, \"value\": rank * 100}\n",
    "\n",
    "#     # rank 0 会收集所有人的数据，其它 rank 填 None\n",
    "#     gathered = None\n",
    "#     if rank == 0:\n",
    "#         gathered = [None for _ in range(world_size)]\n",
    "\n",
    "#     dist.gather_object(local_data, gathered, dst=0)\n",
    "\n",
    "#     if rank == 0:\n",
    "#         print(f\"[Rank 0] Gathered all data: {gathered}\")\n",
    "\n",
    "#     cleanup()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import multiprocessing as mp\n",
    "#     world_size = 4\n",
    "#     mp.spawn(run, args=(world_size,), nprocs=world_size)\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 🧾 输出（仅 rank 0 打印）：\n",
    "\n",
    "# [Rank 0] Gathered all data: [{'rank': 0, 'value': 0}, {'rank': 1, 'value': 100}, {'rank': 2, 'value': 200}, {'rank': 3, 'value': 300}]\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# ✅ 用在你的场景里（只让 rank 0 保存 dataloader 状态）\n",
    "\n",
    "# 你可以这样改你的代码：\n",
    "\n",
    "# local_loader_state = dataloader.state_dict()\n",
    "# gathered_loader_states = [None for _ in range(world_size)] if rank == 0 else None\n",
    "\n",
    "# dist.gather_object(local_loader_state, gathered_loader_states, dst=0)\n",
    "\n",
    "# if rank == 0:\n",
    "#     torch.save({\n",
    "#         \"model\": model.state_dict(),\n",
    "#         \"optimizer\": optimizer.state_dict(),\n",
    "#         \"scaler\": scaler.state_dict(),\n",
    "#         \"dataloader_states\": gathered_loader_states,\n",
    "#     }, \"checkpoint.pt\")\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# 📌 你要记住的区别：\n",
    "\n",
    "# 方法\t所有进程都得到？\t常用于\n",
    "# all_gather_object\t✅ 是\t所有进程都需要这些数据\n",
    "# gather_object\t❌ 只有 rank 0 得到\t集中保存、日志汇总、checkpoints\n",
    "\n",
    "\n",
    "# ⸻\n",
    "\n",
    "# # 如果你想我帮你封装一个 gather_and_save_checkpoint() 工具函数也可以说，我可以直接生成完整代码。是否继续？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
