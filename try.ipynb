{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023/9/28\n",
    "# zhangzhong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mytorch import training, losses, optim\n",
    "\n",
    "# TODO: \n",
    "# 训练的效果很差呀\n",
    "# 是不是哪里写错了？？\n",
    "mylm = torch.load('DeepBiGRU copy')\n",
    "mylm.predict(prompt='my name is ', max_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(a)\n",
    "# https://stackoverflow.com/questions/71723788/how-to-reverse-order-of-rows-in-a-tensor\n",
    "# https://pytorch.org/docs/stable/generated/torch.flip.html\n",
    "b = torch.flip(a, [0])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import training\n",
    "import data_module\n",
    "import losses\n",
    "import module\n",
    "import optim\n",
    "import torch\n",
    "\n",
    "def test_language_model():\n",
    "    time_machine = data_module.TimeMachineDataset(num_seq=16)\n",
    "    vocab = time_machine.get_vocabulary()\n",
    "    hidden_size = 64\n",
    "\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "    rnn = torch.nn.RNN(input_size=len(\n",
    "        vocab), hidden_size=hidden_size, nonlinearity='relu')\n",
    "    lm = module.LanguageModel(vocab=vocab, rnn=rnn, hidden_size=hidden_size)\n",
    "\n",
    "    batch_size = 32\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.01\n",
    "    gradient_clip = 1.0\n",
    "    train_dataloader, val_dataloader = time_machine.get_dataloader(\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    trainer = training.Trainer(\n",
    "        model=lm,\n",
    "        loss_fn=losses.CrossEntropyLoss(),\n",
    "        optimizer=optim.SGD(params=lm.parameters(),\n",
    "                            lr=learning_rate, gradient_clip=gradient_clip),\n",
    "        num_epochs=num_epochs,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader\n",
    "    )\n",
    "\n",
    "    trainer.train(tag='RNN_LM')\n",
    "    return lm\n",
    "    \n",
    "lm = test_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lm.predict('introduce yourself ', max_len=64)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lm, 'lm.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylm = torch.load('lm.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卧槽 是真的可以 牛逼啊 pytorch！！\n",
    "mylm.predict(prompt='my name is', max_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 0])\n",
    "F.one_hot(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2])\n",
    "F.one_hot(x, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html\n",
    "x = torch.tensor([[1, 2], [0, 3]])\n",
    "x_one_hot = F.one_hot(x, 4)\n",
    "\n",
    "# 其实就是之前对应位置的某一个数字 变成一个一个one_hot vector\n",
    "print(x[0, 0], x_one_hot[0, 0])\n",
    "print(x[0, 1], x_one_hot[0, 1])\n",
    "print(x[1, 0], x_one_hot[1, 0])\n",
    "print(x[1, 1], x_one_hot[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再尝试一下交换维度\n",
    "# batch_size = 2, num_seq = 3\n",
    "x = torch.tensor([[0,1,2], [3,4,5]])\n",
    "x_one_hot = F.one_hot(x, 6)\n",
    "print(x_one_hot)\n",
    "\n",
    "# 现在我们想遍历所有的样本 \n",
    "for x in x_one_hot:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完全符合我们的推导 太对了!!\n",
    "# https://pytorch.org/docs/stable/generated/torch.transpose.html\n",
    "# Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.\n",
    "x_t = torch.transpose(x_one_hot, 0, 1)\n",
    "for x in x_t:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yield from\n",
    "def funa():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3\n",
    "    \n",
    "def funb():\n",
    "    yield from funa()\n",
    "    yield 4\n",
    "    yield 5\n",
    "    \n",
    "for x in funb():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of Tensor can not convert to a whole tensor\n",
    "# outputs: list[Tensor] = []\n",
    "# outputs.append(torch.tensor([1,2,3]))\n",
    "# outputs.append(torch.tensor([1,2,3]))\n",
    "# outputs.append(torch.tensor([1,2,3]))\n",
    "# outputs.append(torch.tensor([1,2,3]))\n",
    "# output = torch.tensor(outputs)\n",
    "\n",
    "# so you can use torch.stack\n",
    "# https://pytorch.org/docs/stable/generated/torch.stack.html\n",
    "# Concatenates a sequence of tensors along a new dimension.\n",
    "# All tensors need to be of the same size.\n",
    "\n",
    "x = torch.tensor([1,2,3])\n",
    "xs = [x, x, x, x]\n",
    "torch.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,],[3,4,]])\n",
    "xs = [x, x, x, x]\n",
    "torch.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack(xs, dim=1)\n",
    "y.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量各分量的平方和 就是 l2范数\n",
    "# x = torch.tensor([1,2,3])\n",
    "# x.norm(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad clip\n",
    "# 二维矩阵的grad长什么样子\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor([1, 2], dtype=torch.float32, requires_grad=True)\n",
    "y = x + b\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training\n",
    "import data_module\n",
    "import losses\n",
    "import module\n",
    "import optim\n",
    "import torch\n",
    "\n",
    "def test_RNN_scratch():\n",
    "    \n",
    "    time_machine = data_module.TimeMachineDataset(num_seq=16)\n",
    "    vocab = time_machine.get_vocabulary()\n",
    "    \n",
    "    rnn = module.RNNScratch(vocab_size=len(vocab), num_hidden=64)\n",
    "    lm = module.LanguageModelScratch(vocab=vocab, rnn=rnn)\n",
    "    \n",
    "    batch_size = 2048\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.01\n",
    "    gradient_clip = 1.0\n",
    "    train_dataloader, val_dataloader = time_machine.get_dataloader(batch_size=batch_size)\n",
    "    \n",
    "    trainer = training.Trainer(\n",
    "        model=lm,\n",
    "        loss_fn=losses.CrossEntropyLoss(),\n",
    "        optimizer=optim.SGD(params=lm.parameters(), lr=learning_rate, gradient_clip=gradient_clip),\n",
    "        num_epochs=num_epochs,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader\n",
    "    )\n",
    "    \n",
    "    trainer.train(tag='RNN Scratch')\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = test_RNN_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = ''.join(lm.predict('introduce yourself ', num_seq=128))\n",
    "print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
