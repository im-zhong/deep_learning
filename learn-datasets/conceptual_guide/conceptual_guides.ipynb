{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee8156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/17\n",
    "# zhangzhong\n",
    "# https://huggingface.co/docs/datasets/about_arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee66c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrow\n",
    "# https://arrow.apache.org/\n",
    "# It is a specific data format that stores data in a columnar memory layout\n",
    "# \n",
    "# Arrow’s standard format allows zero-copy reads which removes virtually all serialization overhead.\n",
    "# Arrow is language-agnostic so it supports different programming languages.\n",
    "# Arrow is column-oriented so it is faster at querying and processing slices or columns of data.\n",
    "# Arrow allows for copy-free hand-offs to standard machine learning tools such as NumPy, Pandas, PyTorch, and TensorFlow.\n",
    "# Arrow supports many, possibly nested, column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1ea8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory mapping\n",
    "# 🤗 Datasets uses Arrow for its local caching system.\n",
    "# t allows datasets to be backed by an on-disk cache, which is memory-mapped for fast lookup. \n",
    "# This architecture allows for large datasets to be used on machines with relatively small device memory.\n",
    "# For example, loading the full English Wikipedia dataset only takes a few MB of RAM\n",
    "\n",
    "# import os\n",
    "# import psutil\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# mem_before = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "# wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "# mem_after = psutil.Process(os.getpid()).memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# # This is possible because the Arrow data is actually memory-mapped from disk, and not loaded in memory\n",
    "# # Memory-mapping allows access to data on disk, and leverages virtual memory capabilities for fast lookups.\n",
    "# print(f\"RAM memory used: {(mem_after - mem_before)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8b1f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "# s = \"\"\"batch_size = 1000\n",
    "# for batch in wiki.iter(batch_size):\n",
    "#     ...\n",
    "# \"\"\"\n",
    "\n",
    "# elapsed_time = timeit.timeit(stmt=s, number=1, globals=globals())\n",
    "# # wiki.dataset_size是数据集的大小，以字节为单位\n",
    "# # >>30 表示右移30位，相当于除以2的30次方，即转换为GB\n",
    "# # >>27 表示右移27位，相当于除以2的27次方，计算的是Gbit 因为GByte-> Gbit 需要乘以8\n",
    "# print(f\"Time to iterate over the {wiki.dataset_size >> 30} GB dataset: {elapsed_time:.1f} sec, \"\n",
    "#       f\"ie. {float(wiki.dataset_size >> 27)/elapsed_time:.1f} Gb/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5defb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cache\n",
    "# It stores previously downloaded and processed datasets so when you need to use them again, they are reloaded directly from the cache\n",
    "#  Even after you close and start another Python session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba8c1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde0e5ffc5374c0e96c29c90ecad09f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889f7244aaee43a3 7978cbad9f40e9bb\n"
     ]
    }
   ],
   "source": [
    "# Fingerprint\n",
    "# Datasets assigns a fingerprint to the cache file. \n",
    "# A fingerprint keeps track of the current state of a dataset.\n",
    "# The initial fingerprint is computed using a hash from the Arrow table, or a hash of the Arrow files if the dataset is on disk.\n",
    "# Subsequent fingerprints are computed by combining the fingerprint of the previous state, and a hash of the latest transform applied.\n",
    "\n",
    "# In order for a transform to be hashable, it needs to be picklable by dill or pickle.\n",
    "# https://dill.readthedocs.io/en/latest/\n",
    "# https://docs.python.org/3/library/pickle.html\n",
    "from datasets import Dataset, IterableDataset\n",
    "dataset1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
    "dataset2 = dataset1.map(lambda x: {\"a\": x[\"a\"] + 1})\n",
    "print(dataset1._fingerprint, dataset2._fingerprint)\n",
    "\n",
    "# If your transform is not hashable, Dataset will recompute the dataset every time.\n",
    "\n",
    "# When caching is disabled, use Dataset.save_to_disk() to save your transformed dataset or it will be deleted once the session ends.\n",
    "\n",
    "# The hash is computed by dumping the object using a dill pickler and hashing the dumped bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0d607f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences between Dataset and IterableDataset\n",
    "# https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable\n",
    "\n",
    "# IterableDataset is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages\n",
    "# while a Dataset is great for everything else\n",
    "\n",
    "# Dataset provides random access to the rows, but you must have the entire dataset stored on your disk or in memory,\n",
    "#\n",
    "# IterableDataset, you can access it using a for loop to load the data progressively as you iterate over the dataset. \n",
    "# This way, only a small fraction of examples is loaded in memory, and you don’t write anything on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d26a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'col_1': 0}\n",
      "{'col_1': 0}\n"
     ]
    }
   ],
   "source": [
    "# You can create a Dataset using lists or dictionaries, and the data is entirely converted to Arrow so you can easily access any row:\n",
    "my_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\n",
    "print(my_dataset[0])\n",
    "\n",
    "# Lazy\n",
    "# use generator to create a iterable dataset\n",
    "def my_generator(n):\n",
    "    for i in range(n):\n",
    "        yield {\"col_1\": i}\n",
    "\n",
    "my_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\n",
    "for example in my_iterable_dataset:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff17dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading local files entirely and progressively\n",
    "\n",
    "# To save disk space and skip the conversion step, you can define an IterableDataset by streaming from the local files directly.\n",
    "# the data is read progressively from the local files as you iterate over the dataset:\n",
    "\n",
    "# data_files = {\"train\": [\"path/to/data.csv\"]}\n",
    "# my_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\n",
    "# for example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\n",
    "#     print(example)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96181a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager data processing\n",
    "# When you process a Dataset object using Dataset.map(), the entire dataset is processed immediately and returned.\n",
    "# my_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\n",
    "# print(my_dataset[0])\n",
    "\n",
    "# Lazy data processing\n",
    "# On the other hand, due to the “lazy” nature of an IterableDataset,\n",
    "# calling IterableDataset.map() does not apply your map function over the full dataset. \n",
    "# Instead, your map function is applied on-the-fly.\n",
    "\n",
    "# you can chain multiple processing steps and they will all run at once when you start iterating over the dataset:\n",
    "# my_iterable_dataset = my_iterable_dataset.map(process_fn_1)\n",
    "# my_iterable_dataset = my_iterable_dataset.filter(filter_fn)\n",
    "# my_iterable_dataset = my_iterable_dataset.map(process_fn_2)\n",
    "\n",
    "# # process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\n",
    "# for example in my_iterable_dataset:  \n",
    "#     print(example)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd278f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact suffling\n",
    "# When you shuffle a Dataset using Dataset.shuffle(), you apply an exact shuffling of the dataset. \n",
    "# It works by taking a list of indices [0, 1, 2, ... len(my_dataset) - 1] and shuffling this list.\n",
    "# Then, accessing my_dataset[0] returns the row and index defined by the first element of the indices mapping that has been shuffled:\n",
    "\n",
    "# Approximate shuffling\n",
    "# It uses a shuffle buffer to sample random examples iteratively from the dataset\n",
    "# shuffles the dataset shards if your dataset is made of multiple files or sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed\n",
    "\n",
    "# However as soon as your Dataset has an indices mapping (via Dataset.shuffle() for example), the speed can become 10x slower.\n",
    "# you aren’t reading contiguous chunks of data anymore\n",
    "# To restore the speed, you’d need to rewrite the entire dataset on your disk again using Dataset.flatten_indices(), which removes the indices mapping.\n",
    "# my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
    "\n",
    "# IterableDataset.shuffle().\n",
    "#  It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b88700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume\n",
    "\n",
    "# To restart the iteration of a map-style dataset, you can simply skip the first examples:\n",
    "# my_dataset = my_dataset.select(range(start_index, len(dataset)))\n",
    "\n",
    "# On the other hand, iterable datasets don’t provide random access to a specific example index to resume from. \n",
    "# But you can use IterableDataset.state_dict() and IterableDataset.load_state_dict() to resume from a checkpoint instead, \n",
    "# similarly to what you can do for models and optimizers:\n",
    "# \n",
    "# iterable_dataset = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3)\n",
    "# # save in the middle of training\n",
    "# state_dict = iterable_dataset.state_dict()\n",
    "# # and resume later\n",
    "# iterable_dataset.load_state_dict(state_dict)\n",
    "# \n",
    "# Under the hood, the iterable dataset keeps track of the current shard being read and the example index in the current shard\n",
    "# and it stores this info in the state_dict.\n",
    "# To resume from a checkpoint, the dataset skips all the shards that were previously read to restart from the current shard. \n",
    "# Then it reads the shard and skips examples until it reaches the exact example from the checkpoint.\n",
    "# This can be used with the **StatefulDataLoader** from torchdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d9f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch from map-style to iterable-style\n",
    "# iterable + shards + torch.dataloader?\n",
    "# my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\n",
    "# my_iterable_dataset.num_shards  # 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b0628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Features\n",
    "# https://huggingface.co/docs/datasets/about_dataset_features\n",
    "\n",
    "# Features defines the internal structure of a dataset.\n",
    "# The Features format is simple: dict[column_name, column_type]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Load\n",
    "# https://huggingface.co/docs/datasets/about_dataset_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Mapping\n",
    "# It allows you to speed up processing, \n",
    "# and freely control the size of the generated dataset.\n",
    "\n",
    "# Input size != output size\n",
    "#  the mapped function does not have to return an output batch of the same size.\n",
    "# However, remember that all values in the output dictionary must contain the same number of elements as the other fields in the output dictionary. \n",
    "# To make it valid, you have to drop one of the columns:\n",
    "# dataset_with_duplicates = dataset.map(lambda batch: {\"b\": batch[\"a\"] * 2}, remove_columns=dataset.column_names, batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
