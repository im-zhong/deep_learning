{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0230bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/16\n",
    "# zhangzhong\n",
    "# https://huggingface.co/docs/datasets/process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f902d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "# This guide will show you how to:\n",
    "\n",
    "# - Reorder rows and split the dataset.\n",
    "# - Rename and remove columns, and other common column operations.\n",
    "# - Apply processing functions to each example in a dataset.\n",
    "# - Concatenate datasets.\n",
    "# - Apply a custom formatting transform.\n",
    "# - Save and export processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae102be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!\n",
    "# All processing methods in this guide return a **new** Dataset object\n",
    "# Modification is **not** done in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438ef6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from pprint import pprint\n",
    "\n",
    "dataset: Dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40bd821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reorder rows and split the dataset.\n",
    "# There are several functions for rearranging the structure of a dataset.\n",
    "# These functions are useful for selecting only the rows you want, creating train and test splits,\n",
    "#  and sharding very large datasets into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45aa5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sort\n",
    "# Use sort() to sort column values according to their numerical values\n",
    "# The provided column must be **NumPy** compatible.\n",
    "\n",
    "# !!! indicies mapping\n",
    "# ÂÖ∂ÂÆûÂπ∂Ê≤°Êúâ‰øÆÊîπÂéüÂßãÊï∞ÊçÆÔºåÁõ∏ÂΩì‰∫éÈó¥Êé•ÊéíÂ∫è\n",
    "# Under the hood, this creates a list of indices that is sorted according to values of the column\n",
    "# This indices mapping is then used to access the right rows in the underlying Arrow table.\n",
    "\n",
    "print(dataset[\"label\"][:10])\n",
    "\n",
    "sorted_dataset: Dataset = dataset.sort(column_names=\"label\")\n",
    "print(sorted_dataset[\"label\"][:10])\n",
    "print(sorted_dataset[\"label\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed516bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle\n",
    "# The shuffle() function randomly rearranges the column values. \n",
    "# Shuffling takes the list of indices [0:len(my_dataset)] and shuffles it to create an indices mapping\n",
    "\n",
    "# However as soon as your Dataset has an indices mapping, the speed can become 10x slower.\n",
    "# To restore the speed, you‚Äôd need to rewrite the entire dataset on your disk again using Dataset.flatten_indices(),\n",
    "#  which removes the indices mapping.\n",
    "\n",
    "# wocao! iterable‰πüÂèØ‰ª•ÂÅöshuffleÔºüNB\n",
    "# Alternatively, you can switch to an IterableDataset and leverage its fast approximate shuffling IterableDataset.shuffle():\n",
    "\n",
    "shuffled_dataset: Dataset = sorted_dataset.shuffle(seed=42)\n",
    "shuffled_dataset[\"label\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515d4d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "1834\n"
     ]
    }
   ],
   "source": [
    "# Select and filter\n",
    "# Unless the list of indices to keep is contiguous, those methods also create an indices mapping under the hood.\n",
    "\n",
    "# select() returns rows according to a list of indices:\n",
    "small_dataset: Dataset = shuffled_dataset.select([0, 10, 20, 30, 40, 50])\n",
    "print(len(small_dataset))\n",
    "\n",
    "# filter() returns rows that match a specified condition:\n",
    "start_with_ar: Dataset = dataset.filter(function=lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n",
    "print(len(start_with_ar))\n",
    "\n",
    "# filter() can also filter by indices if you set with_indices=True:\n",
    "even_dataset: Dataset = dataset.filter(function=lambda example, idx: idx %2==0, with_indices=True)\n",
    "print(len(even_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb0b82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3301\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 367\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "# The train_test_split() function creates train and test splits if your dataset doesn‚Äôt already have them. \n",
    "# test_size parameter to create a test split that is 10% of the original dataset:\n",
    "# The splits are shuffled by default, but you can set shuffle=False to prevent shuffling.\n",
    "\n",
    "print(dataset)\n",
    "train_test_split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "print(train_test_split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f5b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 6250\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Shard\n",
    "# ü§ó Datasets supports sharding to divide a very large dataset into a predefined number of chunks.\n",
    "#  \n",
    "\n",
    "dataset: Dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "# Specify the **num_shards** parameter in shard() to determine the number of shards to split the dataset into\n",
    "# rovide the shard you want to return with the **index** parameter.\n",
    "# After sharding the dataset into four chunks, the first shard will only have 6250 examples:\n",
    "sharded_dataset: Dataset = dataset.shard(num_shards=4, index=0)\n",
    "print(sharded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e15f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentenceA', 'sentenceB', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Rename, remove, cast, flatten\n",
    "\n",
    "#  Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place.\n",
    "# Rename\n",
    "dataset: Dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.rename_column(original_column_name=\"sentence1\", new_column_name=\"sentenceA\")\n",
    "dataset = dataset.rename_column(original_column_name=\"sentence2\", new_column_name=\"sentenceB\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637e58e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove\n",
    "\n",
    "dataset = dataset.remove_columns(column_names=[\"sentenceA\", \"sentenceB\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c54ec7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label'],\n",
      "    num_rows: 3668\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Conversely, select_columns() selects one or more columns to keep and removes the rest\n",
    "\n",
    "dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.select_columns(column_names=[\"sentence1\", \"sentence2\", \"label\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e375af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': Value('int32'),\n",
      " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
      " 'sentence1': Value('string'),\n",
      " 'sentence2': Value('string')}\n",
      "{'idx': Value('int64'),\n",
      " 'label': ClassLabel(names=['negative', 'positive']),\n",
      " 'sentence1': Value('string'),\n",
      " 'sentence2': Value('string')}\n",
      "{'idx': Value('int32'),\n",
      " 'label': ClassLabel(names=['negative', 'positive']),\n",
      " 'sentence1': Value('string'),\n",
      " 'sentence2': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# Cast\n",
    "# Casting only works if the original feature type and new feature type are compatible\n",
    "\n",
    "dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")\n",
    "pprint(dataset.features)\n",
    "\n",
    "from datasets import ClassLabel, Value\n",
    "\n",
    "new_features = dataset.features.copy()\n",
    "new_features[\"label\"] = ClassLabel(names=['negative', 'positive'])\n",
    "new_features[\"idx\"] = Value(dtype=\"int64\")\n",
    "dataset = dataset.cast(features=new_features)\n",
    "pprint(dataset.features)\n",
    "\n",
    "# Use the cast_column() function to change the feature type of a single column.\n",
    "dataset = dataset.cast_column(column=\"idx\", feature=Value(dtype=\"int32\"))\n",
    "pprint(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d720d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': List(Value('int32')),\n",
      "             'text': List(Value('string'))},\n",
      " 'context': Value('string'),\n",
      " 'id': Value('string'),\n",
      " 'question': Value('string'),\n",
      " 'title': Value('string')}\n",
      "{'answers.answer_start': List(Value('int32')),\n",
      " 'answers.text': List(Value('string')),\n",
      " 'context': Value('string'),\n",
      " 'id': Value('string'),\n",
      " 'question': Value('string'),\n",
      " 'title': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# Flatten\n",
    "# Sometimes a column can be a nested structure of several types\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\", split=\"train\")\n",
    "pprint(dataset.features)\n",
    "\n",
    "# The answers field contains two subfields: text and answer_start.\n",
    "# Use the flatten() function to extract the subfields into their own separate columns:\n",
    "flat_dataset = dataset.flatten()\n",
    "pprint(flat_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd475e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My sentence: Amrozi accused his brother , whom he called \" the witness \" , '\n",
      " 'of deliberately distorting his evidence .',\n",
      " \"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway \"\n",
      " 'in 1998 for $ 2.5 billion .']\n",
      "['sentence2', 'label', 'idx', 'new_sentence']\n",
      "['0: Referring to him as only \" the witness \" , Amrozi accused his brother of '\n",
      " 'deliberately distorting his evidence .',\n",
      " \"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to \"\n",
      " 'Safeway for $ 1.8 billion in 1998 .',\n",
      " \"2: On June 10 , the ship 's owners had published an advertisement on the \"\n",
      " 'Internet , offering the explosives for sale .',\n",
      " '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A '\n",
      " '$ 4.57 .',\n",
      " '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York '\n",
      " 'Stock Exchange on Friday .']\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "# The primary purpose of map() is to speed up processing functions\n",
    "# It allows you to apply a processing function to each example in a dataset, independently or in batches\n",
    "# This function can even create new rows and columns.\n",
    "\n",
    "dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")\n",
    "\n",
    "# !!! Ëøô‰∏™ÂáΩÊï∞Âè™ËÉΩÁî®Êù•Â§ÑÁêÜÂçï‰∏™example\n",
    "# Â¶ÇÊûúÊòØbatchÔºåÈÇ£‰πà examples[\"sentence1\"] Â∞±ÊòØ‰∏Ä‰∏™list\n",
    "def add_prefix(example):\n",
    "    example[\"sentence1\"] = \"My sentence: \" + example[\"sentence1\"]\n",
    "    return example\n",
    "\n",
    "def add_prefix_batch(examples):\n",
    "    examples[\"sentence1\"] = [\"My sentence: \" + sentence for sentence in examples[\"sentence1\"]]\n",
    "    return examples\n",
    "\n",
    "updated_dataset = dataset.map(function=add_prefix_batch, batched=True)\n",
    "pprint(updated_dataset[\"sentence1\"][:2], compact=True)\n",
    "\n",
    "# remove a column, actually rename a column\n",
    "# Datasets also has a remove_columns() function which is faster because it doesn‚Äôt copy the data of the remaining columns.\n",
    "updated_dataset = dataset.map(function=lambda example: {\"new_sentence\": example[\"sentence1\"]}, remove_columns=[\"sentence1\"])\n",
    "print(updated_dataset.column_names)\n",
    "\n",
    "# You can also use map() with indices if you set with_indices=True\n",
    "updated_dataset = dataset.map(function=lambda example, idx: {\"sentence2\": f\"{idx}: {example['sentence2']}\"}, with_indices=True)\n",
    "pprint(updated_dataset[\"sentence2\"][:5], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fced0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing Map\n",
    "# Multiprocessing significantly speeds up processing by parallelizing processes on the CPU\n",
    "# Set the num_proc parameter in map() to set the number of processes to use:\n",
    "\n",
    "updated_dataset: Dataset = dataset.map(\n",
    "    function=lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]},\n",
    "    with_indices=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# ÊàëÊáÇ‰∫ÜÔºåwith rankÂ∞±Áü•ËØÜÊèê‰æõ‰∏Ä‰∏™ËøõÁ®ãÁöÑÁºñÂè∑\n",
    "# Âú®preprocessÁöÑÂèÇÊï∞ÈáåÈù¢\n",
    "# Êàë‰ª¨ÂèØ‰ª•Áî®Ëøô‰∏™rankÊù•ÂÅö‰∏Ä‰∫õÊò†Â∞ÑÔºåÊØîÂ¶ÇÊò†Â∞ÑÂà∞gpu‰∏ä\n",
    "# ËøôÈáåÁöÑ rank ÊòØ Hugging Face datasets.map(..., with_rank=True) Êèê‰æõÁöÑ ËøõÁ®ãÁºñÂè∑ÔºåÁî®‰∫éÂú®Â§ö‰∏™ËøõÁ®ã‰∏≠ËØÜÂà´ÂΩìÂâçÂ§ÑÁêÜÁöÑÊòØÁ¨¨Âá†‰∏™ËøõÁ®ãÔºå‰ªéËÄåÈÄâÊã©‰∏çÂêåÁöÑ GPU\n",
    "#\n",
    "# def gpu_computation(batch, rank): # ÔºÅÔºÅÔºÅÊ≥®ÊÑèËøôÈáåÁöÑrankÂèÇÊï∞ÔºåËøô‰∏™ÂáΩÊï∞Â∞±ÊòØ‰º†ÈÄíÁªômapÁöÑÂáΩÊï∞\n",
    "#     # Move the model on the right GPU if it's not there already\n",
    "#     device = f\"cuda:{(rank or 0) % torch.cuda.device_count()}\"\n",
    "#     model.to(device)\n",
    "# \n",
    "#     # Your big GPU call goes here, for example:\n",
    "#     chats = [[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ] for prompt in batch[\"prompt\"]]\n",
    "#     texts = [tokenizer.apply_chat_template(\n",
    "#         chat,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     ) for chat in chats]\n",
    "#     model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**model_inputs, max_new_tokens=512)\n",
    "#     batch[\"output\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "#     return batch\n",
    "#\n",
    "# updated_dataset= dataset.map(function=gpu_computation, with_rank=True, num_proc=torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# TODOÔºö‰ªäÊôöÂÜôËÆ≠ÁªÉ‰ª£Á†ÅÁöÑÊó∂ÂÄô‰ªîÁªÜÁ†îÁ©∂\n",
    "# https://huggingface.co/docs/datasets/process#multiprocessing\n",
    "# ËøôÈáåÁöÑÂ§ÑÁêÜÂ•ΩÂÉèÈùûÂ∏∏ÂÖ≥ÈîÆÂïäÔºåÊàëÁ™ÅÁÑ∂ÊÑèËØÜÂà∞Êàë‰ª¨Ê†πÊú¨Â∞±‰∏çÈúÄË¶ÅstreamingÂïäÔºåÂõ†‰∏∫Êú¨Âú∞Âä†ËΩΩÊòØÁî®mmapÂÅöÁöÑ\n",
    "# Â∞±ÁÆóÊòØË∂ÖÂ§ßÁöÑÊï∞ÊçÆ‰πü‰∏ç‰ºöÂç†Áî®Â§ßÈáèÁöÑÂÜÖÂ≠òÔºåËøôÊÆµ‰ª£Á†ÅÂ∫îËØ•Â∞±ÊòØÊàë‰ª¨ÂÅöDDPÁöÑÊ†∏ÂøÉ‰ª£Á†Å‰∫ÜÔºÅ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da959c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3668\n",
      "10470\n",
      "['Amrozi accused his brother , whom he called \" the ', 'witness \" , of deliberately distorting his evidenc', 'e .']\n"
     ]
    }
   ],
   "source": [
    "# Batch processing\n",
    "# The map() function supports working with batches of examples.\n",
    "# Operate on batches by setting batched=True\n",
    "# The default batch size is 1000, but you can adjust it with the batch_size parameter.\n",
    "\n",
    "\n",
    "dataset = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\", split=\"train\")\n",
    "\n",
    "# split long examples\n",
    "# 1. splits the sentence1 field into chunks of 50 characters\n",
    "# 2. stack all the chunks together to create the new dataset\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"sentence1\"]:\n",
    "        chunks.extend([sentence[i : i + 50] for i in range(0, len(sentence), 50)])\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "\n",
    "chunked_dataset = dataset.map(\n",
    "    function=chunk_examples,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    # ‰∏çÂä†Ëøô‰∏ÄË°åÂ∞±‰ºöÊä•ÈîôÔºåColumn 4 named chunks expected length 1000 but got length 2847\n",
    "    # Ë¶ÅÂíåËøô‰∏™‰∏úË•øÊê≠ÈÖçÔºåÂõ†‰∏∫Êàë‰ª¨ÁöÑchunk_exampleËøîÂõûÁöÑË°åÊï∞ÊØîÂéüÂßãÁöÑÊï∞ÊçÆÈõÜË¶ÅÂ§ö\n",
    "    # Âõ†‰∏∫chunk_examplesÂáΩÊï∞ËøîÂõûÁöÑÈïøÂ∫¶Â∫îËØ•Âíå‰º†ÂÖ•ÁöÑexamplesÔºå‰πüÂ∞±ÊòØbatch_size‰∏ÄÊ†∑\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "print(len(dataset))\n",
    "print(len(chunked_dataset))\n",
    "print(chunked_dataset[\"chunks\"][:3])\n",
    "\n",
    "# ÈÇ£ËøôÊ†∑ÊÉ≥Ë±°Á©∫Èó¥Â∞±ÂæàÂ§ß‰∫Ü\n",
    "# hugging face‰∏äËøòÊúâ‰∏Ä‰∏™data augmentaitonÁöÑ‰æãÂ≠ê\n",
    "# Create a function to randomly select a word to mask in the sentence.\n",
    "# def augment_data(examples):\n",
    "#     outputs = []\n",
    "#     for sentence in examples[\"sentence1\"]:\n",
    "#         words = sentence.split(' ')\n",
    "#         K = randint(1, len(words)-1)\n",
    "#         masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n",
    "#         predictions = fillmask(masked_sentence)\n",
    "#         augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)]\n",
    "#         outputs += [sentence] + augmented_sequences\n",
    "#     return {\"data\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/process#asynchronous-processing\n",
    "# TODO: read this if needed, ÁÆÄÂçïÁöÑËØ¥Â∞±ÊòØmap‰πüÊîØÊåÅasyncÂáΩÊï∞ÔºåÊîØÊåÅÂπ∂ÂèëÔºåÈªòËÆ§ÁöÑÂπ∂ÂèëÊòØ1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48f52847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1],\n",
      " 'idx': 0,\n",
      " 'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170,\n",
      "               1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436,\n",
      "               2010, 3350, 1012, 102],\n",
      " 'label': 1,\n",
      " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , '\n",
      "              'of deliberately distorting his evidence .',\n",
      " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his '\n",
      "              'brother of deliberately distorting his evidence .',\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/process#process-multiple-splits\n",
    "# Many datasets have splits that can be processed simultaneously with DatasetDict.map().\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")\n",
    "\n",
    "dataset_dict: DatasetDict = load_dataset(path=\"nyu-mll/glue\", name=\"mrpc\")\n",
    "encoded_dataset = dataset_dict.map(lambda examples: tokenizer(examples[\"sentence1\"]), batched=True)\n",
    "pprint(encoded_dataset[\"train\"][0], compact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/process#distributed-usage\n",
    "# ÂàÜÂ∏ÉÂºèËÆ≠ÁªÉÁöÑÊó∂ÂÄôÔºåÈúÄË¶ÅÊ≥®ÊÑè\n",
    "# This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work.\n",
    "# ÔºüÔºüÔºü\n",
    "# ‰∏çÂèØ‰ª•‰∏çÂêåÁöÑËøõÁ®ãÁã¨Á´ãÁöÑÂ§ÑÁêÜÊï∞ÊçÆÔºåÂàÜÂà´ÂñÇÁªôÂ§ßÊ®°ÂûãÂêóÔºü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8ec59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405eba976b0b44f2b6f0aadd64aeed2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [[1, 1, 1, 1], [1, 1, 1, 1]],\n",
      " 'text': [['the rock is destined to be the 21st century\\'s new \" conan \" and '\n",
      "           \"that he's going to make a splash even greater than arnold \"\n",
      "           'schwarzenegger , jean-claud van damme or steven segal .',\n",
      "           'the gorgeously elaborate continuation of \" the lord of the rings \" '\n",
      "           'trilogy is so huge that a column of words cannot adequately '\n",
      "           \"describe co-writer/director peter jackson's expanded vision of j . \"\n",
      "           \"r . r . tolkien's middle-earth .\",\n",
      "           'effective but too-tepid biopic',\n",
      "           'if you sometimes like to go to the movies to have fun , wasabi is '\n",
      "           'a good place to start .'],\n",
      "          [\"emerges as something rare , an issue movie that's so honest and \"\n",
      "           \"keenly observed that it doesn't feel like one .\",\n",
      "           'the film provides some great insight into the neurotic mindset of '\n",
      "           'all comics -- even those who have reached the absolute top of the '\n",
      "           'game .',\n",
      "           'offers that rare combination of entertainment and education .',\n",
      "           'perhaps no picture ever made has more literally showed that the '\n",
      "           'road to hell is paved with good intentions .']]}\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "# The batch() method allows you to group samples from the dataset into batches.\n",
    "# Note that Dataset.batch() returns a new Dataset where each item is a batch of multiple samples from the original datase\n",
    "\n",
    "dataset: Dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "# The batch() method accepts the following parameters:\n",
    "# \n",
    "# batch_size (int): The number of samples in each batch.\n",
    "# drop_last_batch (bool, defaults to False): Whether to drop the last incomplete batch if the dataset size is not divisible by the batch size.\n",
    "# num_proc (int, optional, defaults to None): The number of processes to use for multiprocessing. If None, no multiprocessing is used. This can significantly speed up batching for large datasets.\n",
    "batched_dataset = dataset.batch(batch_size=4)\n",
    "pprint(batched_dataset[0:2], compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd92e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "# Separate datasets can be concatenated if they share the same column types.\n",
    "\n",
    "# from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "# stories = load_dataset(\"ajibawa-2023/General-Stories-Collection\", split=\"train\")\n",
    "# stories = stories.remove_columns([col for col in stories.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "# wiki = load_dataset(\"wikimedia/wikipedia\", \"20220301.en\", split=\"train\")\n",
    "# wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "# assert stories.features.type == wiki.features.type\n",
    "# bert_dataset = concatenate_datasets([stories, wiki])\n",
    "\n",
    "# # You can also concatenate two datasets horizontally by setting axis=1 as long as the datasets have the same number of rows:\n",
    "# # Áõ∏ÂΩì‰∫éÊ∑ªÂä†Êñ∞ÁöÑÂàóÂïä\n",
    "# from datasets import Dataset\n",
    "# stories_ids = Dataset.from_dict({\"ids\": list(range(len(stories)))})\n",
    "# stories_with_ids = concatenate_datasets([stories, stories_ids], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a411d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column([10, 11, 20, 12, 0])\n",
      "Column([10, 11, 20, 12, 0])\n"
     ]
    }
   ],
   "source": [
    "# Interleave\n",
    "# You can also mix several datasets together by taking alternating examples from each one to create a new dataset. \n",
    "# This is known as interleaving, which is enabled by the interleave_datasets() function\n",
    "\n",
    "# stopping_strategy\n",
    "# The default strategy, first_exhausted, is a subsampling strategy, \n",
    "# i.e the dataset construction is stopped as soon one of the dataset runs out of samples.\n",
    "#\n",
    "# stopping_strategy=all_exhausted to execute an oversampling strategy\n",
    "# In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once\n",
    "# In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached\n",
    "# ! epoch=2\n",
    "\n",
    "\n",
    "from datasets import Dataset, interleave_datasets\n",
    "seed = 42\n",
    "probabilities = [0.3, 0.5, 0.2] # wocao,ÈááÊ†∑ÁéáÔºÅ\n",
    "d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
    "d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
    "d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
    "dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed, stopping_strategy=\"first_exhausted\")\n",
    "print(dataset[\"a\"])\n",
    "\n",
    "# ÊúâbugÔºüÊÄé‰πàÁªìÊûúÊòØ‰∏ÄÊ†∑ÁöÑÔºü\n",
    "dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed, stopping_strategy=\"all_exhausted\")\n",
    "print(dataset[\"a\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a687b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([0, 10, 20, 1, 11])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Á°ÆÂÆûÊúâbugÔºÅ\n",
    "# TODOÔºöÂèØ‰ª•Ë∞ÉËØï‰∏Ä‰∏ã\n",
    "d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
    "d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
    "d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
    "dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n",
    "dataset[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # format\n",
    "# # The with_format() function changes the format of a column to be compatible with some common data formats\n",
    "# # Specify the output you‚Äôd like in the type parameter. such as type=torch\n",
    "# # You can also choose which the columns you want to format using columns=. Formatting is applied on-the-fly.\n",
    "# dataset = dataset.with_format(type=\"torch\", columns=[\"a\"])\n",
    "\n",
    "# # danger! The set_format() function also changes the format of a column, except it runs in-place:\n",
    "# dataset.set_format(type=\"torch\")\n",
    "\n",
    "# # If you need to reset the dataset to its original format, set the format to None (or use reset_format()):\n",
    "# dataset.format\n",
    "# dataset = dataset.with_format(None)\n",
    "# dataset.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bc757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['foo', 'bar'],\n",
       " 'tokens': tensor([[0, 1, 2],\n",
       "         [3, 4, 5]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor format: numpy, torch, tensorflow, jax\n",
    "# When a dataset is formatted in a tensor or array format, \n",
    "# all the data are formatted as tensors or arrays (except unsupported types like strings for example for PyTorch):\n",
    "ds = Dataset.from_dict({\"text\": [\"foo\", \"bar\"], \"tokens\": [[0, 1, 2], [3, 4, 5]]})\n",
    "ds = ds.with_format(\"torch\")\n",
    "ds[0]\n",
    "ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular format: pandas, polars, arrow\n",
    "# custom format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ae188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# Once your dataset is ready, you can save it as a Hugging Face Dataset in **Parquet** format and reuse it later with load_dataset().\n",
    "# use push_to_hub\n",
    "# You can use multiple processes to upload it in paralle\n",
    "# dataset.push_to_hub(\"username/my_dataset\", num_proc=8)\n",
    "\n",
    "# Alternatively, you can save it locally in **Arrow** format on disk.\n",
    "# encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\")\n",
    "# reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "# datasets supports exporting as well so you can work with your dataset in other applications\n",
    "# dataset.to_csv(\"path/of/my/dataset.csv\")\n",
    "# dataset.to_json(\"path/of/my/dataset.json\")\n",
    "# dataset.to_parquet(\"path/of/my/dataset.parquet\")\n",
    "# dataset.to_sql(\"sqlite:///path/of/my/dataset.db\", table_name=\"my_table\")\n",
    "# dataset.to_pandas()\n",
    "# dataset.to_dict()\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
