{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389a41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025/7/16\n",
    "# zhangzhong\n",
    "# https://huggingface.co/docs/datasets/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4048fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='',\n",
      "            citation='',\n",
      "            homepage='',\n",
      "            license='',\n",
      "            features={'label': ClassLabel(names=['neg', 'pos']),\n",
      "                      'text': Value('string')},\n",
      "            post_processed=None,\n",
      "            supervised_keys=None,\n",
      "            builder_name='parquet',\n",
      "            dataset_name='rotten_tomatoes',\n",
      "            config_name='default',\n",
      "            version=0.0.0,\n",
      "            splits={'test': SplitInfo(name='test',\n",
      "                                      num_bytes=136102,\n",
      "                                      num_examples=1066,\n",
      "                                      shard_lengths=None,\n",
      "                                      dataset_name='rotten_tomatoes'),\n",
      "                    'train': SplitInfo(name='train',\n",
      "                                       num_bytes=1075873,\n",
      "                                       num_examples=8530,\n",
      "                                       shard_lengths=None,\n",
      "                                       dataset_name='rotten_tomatoes'),\n",
      "                    'validation': SplitInfo(name='validation',\n",
      "                                            num_bytes=134809,\n",
      "                                            num_examples=1066,\n",
      "                                            shard_lengths=None,\n",
      "                                            dataset_name='rotten_tomatoes')},\n",
      "            download_checksums={'hf://datasets/cornell-movie-review-data/rotten_tomatoes@aa13bc287fa6fcab6daf52f0dfb9994269ffea28/test.parquet': {'checksum': None,\n",
      "                                                                                                                                                  'num_bytes': 92206},\n",
      "                                'hf://datasets/cornell-movie-review-data/rotten_tomatoes@aa13bc287fa6fcab6daf52f0dfb9994269ffea28/train.parquet': {'checksum': None,\n",
      "                                                                                                                                                   'num_bytes': 698845},\n",
      "                                'hf://datasets/cornell-movie-review-data/rotten_tomatoes@aa13bc287fa6fcab6daf52f0dfb9994269ffea28/validation.parquet': {'checksum': None,\n",
      "                                                                                                                                                        'num_bytes': 90001}},\n",
      "            download_size=881052,\n",
      "            post_processing_size=None,\n",
      "            dataset_size=1346784,\n",
      "            size_in_bytes=2227836)\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset\n",
    "# Before you take the time to download a dataset, it’s often helpful to quickly get some general information about a dataset\n",
    "# A dataset’s information is stored inside DatasetInfo and can include information such as the dataset description, features, and dataset size.\n",
    "# https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.DatasetInfo\n",
    "\n",
    "from datasets import load_dataset_builder\n",
    "from datasets.builder import DatasetBuilder\n",
    "from pprint import pprint\n",
    "\n",
    "ds_builder: DatasetBuilder = load_dataset_builder(path=\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "pprint(ds_builder.info, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2adfa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 8530\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "pprint(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6b7835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'validation', 'test']\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 8530\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/datasets/load_hub#splits\n",
    "# Splits: A split is a specific subset of a dataset like train and test. \n",
    "# List a dataset’s split names with the get_dataset_split_names() function:\n",
    "\n",
    "from datasets import get_dataset_split_names\n",
    "split_names = get_dataset_split_names(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "pprint(split_names)\n",
    "\n",
    "# Then you can load a specific split with the split parameter. Loading a dataset split returns a Dataset object:\n",
    "# https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.Dataset\n",
    "from datasets import Dataset \n",
    "\n",
    "# Load a specific split\n",
    "dataset: Dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "pprint(dataset)\n",
    "\n",
    "# If you don’t specify a split, 🤗 Datasets returns a DatasetDict object instead:\n",
    "from datasets import DatasetDict\n",
    "dataset_dict: DatasetDict = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")\n",
    "pprint(dataset_dict)\n",
    "\n",
    "# 如果设置了stream=True, 就会返回iterable\n",
    "# If set to True, don't download the data files. Instead, it streams the data progressively while iterating on the dataset. An [IterableDataset] or [IterableDatasetDict] is returned instead in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df7b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['submissions', 'successful_submissions', 'leaderboards']\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['submission_id', 'leaderboard_id', 'user_id', 'submission_time', 'file_name', 'code', 'code_id', 'run_id', 'run_start_time', 'run_end_time', 'run_mode', 'run_score', 'run_passed', 'run_result', 'run_compilation', 'run_meta', 'run_system_info'],\n",
      "        num_rows: 40095\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Configurations\n",
    "# Some datasets contain several sub-datasets.\n",
    "# These sub-datasets are known as configurations or subsets, and you **must** explicitly select one when loading the dataset.\n",
    "\n",
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs = get_dataset_config_names(\"GPUMODE/kernelbot-data\")\n",
    "pprint(configs)\n",
    "\n",
    "# then load the configuartion you want\n",
    "dataset = load_dataset(path=\"GPUMODE/kernelbot-data\", name=\"submissions\")\n",
    "pprint(dataset)\n",
    "\n",
    "# 就算是dataset builder，也必须传入name，不同的name就是看作不同的数据集的！\n",
    "# 直接像这样获取ds builder会报错\n",
    "# ds_builder = load_dataset_builder(path=\"GPUMODE/kernelbot-data\")\n",
    "# pprint(ds_builder.info, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7480639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Know your dataset\n",
    "# There are two types of dataset objects, a regular Dataset and then an ✨ IterableDataset ✨\n",
    "#\n",
    "# https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.Dataset\n",
    "# A Dataset provides fast random access to the rows, and memory-mapping so that loading even large datasets only uses a relatively small amount of device memory.\n",
    "# \n",
    "# https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.IterableDataset\n",
    "# But for really, really big datasets that won’t even fit on disk or in memory, an IterableDataset allows you to access and use the dataset without waiting for it to download completely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2273a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "{'text': 'things really get weird , though not particularly scary : the movie is all portent and no content .', 'label': 0}\n",
      "Column(['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic', 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"])\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'effective but too-tepid biopic'], 'label': [1, 1, 1]}\n",
      "{'text': ['if you sometimes like to go to the movies to have fun , wasabi is a good place to start .', \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\", 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .'], 'label': [1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "# When you load a dataset split, you’ll get a Dataset object.\n",
    "\n",
    "dataset: Dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "\n",
    "# Indexing\n",
    "# A Dataset contains columns of data, and each column can be a different type of data. \n",
    "print(dataset[0]) # # Get the first row in the dataset\n",
    "print(dataset[-1]) # Get the last row in the dataset\n",
    "print(dataset['text']) # Indexing by the column name returns a list of all the values in the column:\n",
    "print(dataset[0][\"text\"]) # Get the value of the \"text\" column in the first row\n",
    "\n",
    "# Slicing\n",
    "print(dataset[:3]) # Get the first three rows\n",
    "print(dataset[3:6]) # Get rows between three and six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d660159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.Image.Image image mode=RGB size=384x512 at 0x7F252A06F5C0>, 'label': 6}\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
      "[{'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}, {'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'label': 1}, {'text': 'effective but too-tepid biopic', 'label': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Iterable Dataset\n",
    "# An IterableDataset is loaded when you set the streaming parameter to True in load_dataset():\n",
    "# An IterableDataset progressively iterates over a dataset one example at a time, \n",
    "# so you don’t have to wait for the whole dataset to download before you can use it\n",
    "\n",
    "from datasets import IterableDataset\n",
    "\n",
    "iterable_dataset: IterableDataset = load_dataset(path=\"ethz/food101\", split=\"train\", streaming=True)\n",
    "for example in iterable_dataset:\n",
    "    print(example)\n",
    "    break  # Just print the first example to avoid flooding the output\n",
    "\n",
    "# You can also create an IterableDataset from an existing Dataset, but it is faster than streaming mode because the dataset is streamed from local files:\n",
    "dataset: Dataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")\n",
    "iterable_dataset: IterableDataset = dataset.to_iterable_dataset()\n",
    "\n",
    "# Indexing\n",
    "# !!!  You don’t get random access to examples in an IterableDataset. Instead, you should iterate over its elements,\n",
    "# by calling next(iter()) or with a for loop to return the next item from the IterableDataset:\n",
    "print(next(iter(iterable_dataset)))  # Get the first example\n",
    "\n",
    "for example in iterable_dataset:\n",
    "    print(example)\n",
    "    break  # Just print the first example to avoid flooding the output\n",
    "\n",
    "# But an IterableDataset supports column indexing that returns an iterable for the column values:\n",
    "print(next(iter(iterable_dataset['text'])))\n",
    "\n",
    "# Subset\n",
    "# IterableDataset.take() creates a new IterableDataset.\n",
    "print(list(iterable_dataset.take(3)))  # Get the first three examples as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d95875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "# TODO: read the https://huggingface.co/docs/datasets/use_dataset#resample-audio-signals if needed.\n",
    "# TODO: read the https://huggingface.co/docs/datasets/use_dataset#apply-data-augmentations if needed.\n",
    "\n",
    "# Tokenize\n",
    "# Models cannot process raw text, so you’ll need to convert the text into numbers\n",
    "# Tokenization provides a way to do this by dividing text into individual words called tokens.\n",
    "# Tokens are finally converted to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03081e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a tokenizer\n",
    "# Using the **same** tokenizer as the pretrained model is important because you want to make sure the text is split in the same way.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")\n",
    "dataset: Dataset = load_dataset(path=\"cornell-movie-review-data/rotten_tomatoes\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5738ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005,\n",
      "               1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055,\n",
      "               2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058,\n",
      "               8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168,\n",
      "               2030, 7112, 16562, 2140, 1012, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0]}\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005,\n",
      "               1055, 2047, 1000, 16608, 1000, 1998, 2008, 2002, 1005, 1055,\n",
      "               2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058,\n",
      "               8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168,\n",
      "               2030, 7112, 16562, 2140, 1012, 102, 1996, 9882, 2135, 9603,\n",
      "               13633, 1997, 1000, 1996, 2935, 1997, 1996, 7635, 1000, 11544,\n",
      "               2003, 2061, 4121, 2008, 1037, 5930, 1997, 2616, 3685, 23613,\n",
      "               6235, 2522, 1011, 3213, 1013, 2472, 2848, 4027, 1005, 1055, 4423,\n",
      "               4432, 1997, 1046, 1012, 1054, 1012, 1054, 1012, 23602, 1005,\n",
      "               1055, 2690, 1011, 3011, 1012, 102],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "                    0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 2. Call your tokenizer on the first row of text in the dataset:\n",
    "\n",
    "# The tokenizer returns a dictionary with three items:\n",
    "# - input_ids: the numbers representing the tokens in the text.\n",
    "# - token_type_ids: indicates which sequence a token belongs to if there is more than one sequence.\n",
    "# - attention_mask: indicates whether a token should be masked or not.\n",
    "# These values are actually the model inputs.\n",
    "\n",
    "pprint(tokenizer(dataset[0][\"text\"]), compact=True)\n",
    "pprint(tokenizer(dataset[0][\"text\"], dataset[1]['text']), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a946a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c98315f7d041c387b34e1290df0e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. The fastest way to tokenize your entire dataset is to use the **map()** function\n",
    "# This function speeds up tokenization by applying the tokenizer to **batches** of examples instead of individual examples\n",
    "# Set the batched parameter to True:\n",
    "\n",
    "def tokenization(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "dataset = dataset.map(function=tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30db1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\n"
     ]
    }
   ],
   "source": [
    "# 4. Set the format of your dataset to be compatible with your machine learning framework:\n",
    "# Use the set_format() function to set the dataset format to be compatible with PyTorch:\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"], output_all_columns=False)\n",
    "print(dataset.format['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/create_dataset\n",
    "# fast loading and processing,\n",
    "# stream enormous datasets, \n",
    "# memory-mapping\n",
    "# TODO: read this when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/datasets/upload_dataset\n",
    "# TODO: read this when needed.\n",
    "# https://huggingface.co/docs/datasets/share 这里面有更多的上传数据集相关的资料\n",
    "# https://huggingface.co/docs/datasets/dataset_card\n",
    "# https://huggingface.co/docs/datasets/repository_structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
